{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YYk8NG3yOIT9"
   },
   "source": [
    "### A MNIST-like fashion product database\n",
    "\n",
    "In this, we classify the images into respective classes given in the dataset. We use a Neural Net and a Deep Neural Net in Keras to solve this and check the accuracy scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tFO6PuxzOIT_"
   },
   "source": [
    "### Load tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "efNjNImfOIUC"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4207,
     "status": "ok",
     "timestamp": 1569734332240,
     "user": {
      "displayName": "Raghav Praneeth Amaravadi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAMhpxsJE7d0g9jOsWi8V_E3rjg4Ut9Yz-RLZWsQw=s64",
      "userId": "05628056859539857657"
     },
     "user_tz": -330
    },
    "id": "l9C4aAIGOIUH",
    "outputId": "4b681e22-e1db-4e1f-80d2-e43fbd24a317"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.14.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HcoZBStrOIUQ"
   },
   "source": [
    "### Collect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4180,
     "status": "ok",
     "timestamp": 1569734332245,
     "user": {
      "displayName": "Raghav Praneeth Amaravadi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAMhpxsJE7d0g9jOsWi8V_E3rjg4Ut9Yz-RLZWsQw=s64",
      "userId": "05628056859539857657"
     },
     "user_tz": -330
    },
    "id": "XA1WsFSeOIUS",
    "outputId": "d927554a-1613-47c3-fdbe-1f9796a4032a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6126,
     "status": "ok",
     "timestamp": 1569734334227,
     "user": {
      "displayName": "Raghav Praneeth Amaravadi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAMhpxsJE7d0g9jOsWi8V_E3rjg4Ut9Yz-RLZWsQw=s64",
      "userId": "05628056859539857657"
     },
     "user_tz": -330
    },
    "id": "qnbx7TyQOIUY",
    "outputId": "1ae92935-704e-4de4-f688-c8f6be4284fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "32768/29515 [=================================] - 0s 0us/step\n",
      "40960/29515 [=========================================] - 0s 0us/step\n",
      "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "26427392/26421880 [==============================] - 0s 0us/step\n",
      "26435584/26421880 [==============================] - 0s 0us/step\n",
      "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "16384/5148 [===============================================================================================] - 0s 0us/step\n",
      "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "4423680/4422102 [==============================] - 0s 0us/step\n",
      "4431872/4422102 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(trainX, trainY), (testX, testY) = keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6080,
     "status": "ok",
     "timestamp": 1569734334229,
     "user": {
      "displayName": "Raghav Praneeth Amaravadi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAMhpxsJE7d0g9jOsWi8V_E3rjg4Ut9Yz-RLZWsQw=s64",
      "userId": "05628056859539857657"
     },
     "user_tz": -330
    },
    "id": "8Jkp4AzoU_B5",
    "outputId": "94894936-90fe-47af-9884-60ef77df6ac3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ukFbnPeHU_IL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6041,
     "status": "ok",
     "timestamp": 1569734334233,
     "user": {
      "displayName": "Raghav Praneeth Amaravadi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAMhpxsJE7d0g9jOsWi8V_E3rjg4Ut9Yz-RLZWsQw=s64",
      "userId": "05628056859539857657"
     },
     "user_tz": -330
    },
    "id": "UbiHj5YPOIUc",
    "outputId": "73204b70-9e92-4673-c531-d05a4e426091"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9 2 1 1 6]\n"
     ]
    }
   ],
   "source": [
    "print(testY[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lDAYzkwyOIUj"
   },
   "source": [
    "### Convert both training and testing labels into one-hot vectors.\n",
    "\n",
    "**Hint:** check **tf.keras.utils.to_categorical()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vBlfYlANOIUk"
   },
   "outputs": [],
   "source": [
    "trainY = tf.keras.utils.to_categorical(trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6018,
     "status": "ok",
     "timestamp": 1569734334236,
     "user": {
      "displayName": "Raghav Praneeth Amaravadi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAMhpxsJE7d0g9jOsWi8V_E3rjg4Ut9Yz-RLZWsQw=s64",
      "userId": "05628056859539857657"
     },
     "user_tz": -330
    },
    "id": "RHV3b9mzOIUq",
    "outputId": "65048ea1-d804-4b45-ce84-5fe70d0b03ce",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10)\n",
      "('First 5 examples now are: ', array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "print(trainY.shape)\n",
    "print('First 5 examples now are: ', trainY[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7dUyrgKJN4Hf"
   },
   "outputs": [],
   "source": [
    "testY = tf.keras.utils.to_categorical(testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5997,
     "status": "ok",
     "timestamp": 1569734334237,
     "user": {
      "displayName": "Raghav Praneeth Amaravadi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAMhpxsJE7d0g9jOsWi8V_E3rjg4Ut9Yz-RLZWsQw=s64",
      "userId": "05628056859539857657"
     },
     "user_tz": -330
    },
    "id": "RmJmr60YN4Kx",
    "outputId": "8e91817f-85e7-4937-f075-d36fa4a8df6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 10)\n",
      "('First 5 examples now are: ', array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "print(testY.shape)\n",
    "print('First 5 examples now are: ', testY[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FwhQ8e7VOIUw"
   },
   "source": [
    "### Visualize the data\n",
    "\n",
    "Plot first 10 images in the triaining set and their labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AvDML2OoOIUx"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7054,
     "status": "ok",
     "timestamp": 1569734335326,
     "user": {
      "displayName": "Raghav Praneeth Amaravadi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAMhpxsJE7d0g9jOsWi8V_E3rjg4Ut9Yz-RLZWsQw=s64",
      "userId": "05628056859539857657"
     },
     "user_tz": -330
    },
    "id": "H6TjWcnvPJ1B",
    "outputId": "e15cd064-4fff-40f4-d213-dc7ec4c2dee1"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nREEx/ERBMfxEQf0/R6M+dMkjgqgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCorhJwqK4ScKiuEnCorhJwqK4ScK6v8BWgo7tbbP0/8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngmL4iYJi+ImC+n+aIsW4YMNh/wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUVAMP1FQfwfkfkYDoZ69DQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi40LCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcv7US4rQAAEPRJREFUeJzt3X+MVfWZx/HPIyDyQwWcWUBEKZX4\nI+pacyGblGiN20aJUUmMlsSGjVpqLLpNalzi/iGJEH9ktSERq3QlxababmyNPyBuLZIAsWmcQQTU\nZUEypSAwg4hDBXQGn/1jDt1R53zP5Z5z77nj9/1KyNx7n3vueeY4H8+993vO+Zq7C0B8Tiq7AQDl\nIPxApAg/ECnCD0SK8AORIvxApAg/ECnCD0SK8AORGtrIlbW0tPiUKVMauUogKh0dHdq/f79V89xc\n4TezqyUtkTRE0n+6+0Oh50+ZMkVtbW15VgkgoFKpVP3cmt/2m9kQSUslXSPpQklzzOzCWl8PQGPl\n+cw/Q9J2d9/h7p9J+o2k64tpC0C95Qn/JEl/7Xd/V/LYF5jZPDNrM7O2rq6uHKsDUKS6f9vv7svc\nveLuldbW1nqvDkCV8oR/t6TJ/e6flTwGYBDIE/43JU0zs2+Y2cmSvi/ppWLaAlBvNQ/1uXuvmc2X\n9N/qG+pb7u7vFNYZgLrKNc7v7qskrSqoFwANxOG9QKQIPxApwg9EivADkSL8QKQIPxCphp7Pj8bL\nmpEpq24WPjU8q55n3VnyrDuvHTt2BOtTp04N1kPnubS0tASXLer3Zs8PRIrwA5Ei/ECkCD8QKcIP\nRIrwA5FiqA9B9RxOK3OobuvWrcH6li1bgvV33gmfvf7WW28F66Fhzueffz647NChxcSWPT8QKcIP\nRIrwA5Ei/ECkCD8QKcIPRIrwA5FinL8J5D2tNs+y9R5rX7NmTWrt4osvDi7b3t4erC9cuDBYD51W\nu379+uCyl112WbA+Y8aMYP3xxx8P1idN+srMdg3Hnh+IFOEHIkX4gUgRfiBShB+IFOEHIkX4gUjl\nGuc3sw5JhyQdk9Tr7pUimkLz6OzsDNaPHTsWrK9alT6J87p164LLfvjhh8H63XffHazPnDkztXbW\nWWcFl925c2ew/vbbbwfrWefcHzhwILU2bty44LJFKeIgnyvdfX8BrwOggXjbD0Qqb/hd0h/MrN3M\n5hXREIDGyPu2f6a77zazf5D0mpn9j7uv7f+E5H8K8yTp7LPPzrk6AEXJted3993Jz05JL0j6ytkO\n7r7M3SvuXmltbc2zOgAFqjn8ZjbKzE49flvS9ySFL3kKoGnkeds/XtILySmhQyU96+6vFtIVgLqr\nOfzuvkPSPxbYS7TqeU59T09PsL5t27ZgPWvMeeTIkcH6HXfckVp7+OGHg8tmjcXPmTMnWP/kk09S\na1nbfMKECcH65s2bg/W1a9cG6yNGjEitXXvttcFli8JQHxApwg9EivADkSL8QKQIPxApwg9Eikt3\nN4HPP/88WM9z+e3PPvssuOywYcOC9e3btwfrr74aPrRj8eLFqbVNmzYFl826tHeWUaNG1bxsaJhQ\nks4444xgPeuU4CVLlqTWKpXwmfFZw5DVYs8PRIrwA5Ei/ECkCD8QKcIPRIrwA5Ei/ECkGOdvAvWc\nRjvrEtJZVq5cGazfeuutwfqiRYtyrb8shw4dCta7u7uD9awpvufPn59ayzoN+8iRI6m1rGNG+mPP\nD0SK8AORIvxApAg/ECnCD0SK8AORIvxApBjnbwL1vHT38OHDg/Vp06YF6w8++GCu9YfGrLOOQci7\nXdy95tfu6uoK1seMGROsZ11L4IorrkithabvlqT9+9Mnxe7t7Q0u2x97fiBShB+IFOEHIkX4gUgR\nfiBShB+IFOEHIpU5zm9myyVdK6nT3S9KHhsn6beSpkjqkHSTu39UvzZRL3nnDAiNpWfJWraexz9k\nCY2lS9Lo0aOD9azfLbTdDx8+HFz21FNPDdarVc2e/5eSrv7SYwskrXb3aZJWJ/cBDCKZ4Xf3tZK+\nfMjR9ZJWJLdXSLqh4L4A1Fmtn/nHu/ue5PZeSeML6gdAg+T+ws/7PtykfsAxs3lm1mZmbVnHSwNo\nnFrDv8/MJkpS8rMz7YnuvszdK+5eaW1trXF1AIpWa/hfkjQ3uT1X0ovFtAOgUTLDb2bPSfqTpPPM\nbJeZ3SbpIUnfNbNtkv45uQ9gEMkc53f3OSmlqwruBSU46aR8X/tkjcWHXv9ErjE/kHoeJ5B13f6l\nS5cG6zfeeGOwPnv27NTayJEjg8uefPLJqbUT+Z05wg+IFOEHIkX4gUgRfiBShB+IFOEHIsWluwvQ\nzKemNrOsYca8Q4F5tvv48eHTVaZPnx6sv/HGG8H6zTffnFrbuXNncNlLLrkktcZQH4BMhB+IFOEH\nIkX4gUgRfiBShB+IFOEHIsU4fwEYx6+PvKcbh+zevTtYz5q6fObMmcH6hg0bgvX29vbU2qeffhpc\n9vTTT0+tZU173h97fiBShB+IFOEHIkX4gUgRfiBShB+IFOEHIsU4P0qTdb5+3nH+FStWpNaypo67\n/fbbg/U1a9YE62PHjg3WK5VKau3AgS/Pi/tFQ4YMCdarxZ4fiBThByJF+IFIEX4gUoQfiBThByJF\n+IFIZY7zm9lySddK6nT3i5LHFkr6oaTjg6X3ufuqejWJr6escfyDBw8G6w888ECw3tvbm1qbMGFC\ncNmVK1cG6+edd16w3tPTE6x3d3en1k7knPw8qtnz/1LS1QM8/jN3vzT5R/CBQSYz/O6+VlL4kCMA\ng06ez/zzzWyTmS03s/CxjACaTq3h/7mkb0q6VNIeSY+mPdHM5plZm5m1ZR1PDaBxagq/u+9z92Pu\n/rmkX0iaEXjuMnevuHultbW11j4BFKym8JvZxH53Z0vaUkw7ABqlmqG+5yR9R1KLme2SdL+k75jZ\npZJcUoekH9WxRwB1kBl+d58zwMNP16GXuqr3ueODVdZ2yXLs2LFgfdiwYam1zs7O4LL33HNPsH7B\nBRcE6x0dHam1BQsWBJfNOxdD1rwAH3zwQWot6xiCosT5Fw+A8AOxIvxApAg/ECnCD0SK8AORiubS\n3XmH8ty95mWbeQrvrO2SNRQYGsqTpEOHDqXWHnvsseCy11xzTbC+bt26YP2pp54K1usp6795aLtm\nbdOisOcHIkX4gUgRfiBShB+IFOEHIkX4gUgRfiBS0Yzz59XMY/UhWccnZP1eeY+PWLx4cWpt8uTJ\nwWXb2tqC9SeeeKKmnhoha7uGpuEuagruLOz5gUgRfiBShB+IFOEHIkX4gUgRfiBShB+IVDTj/HnH\nu48ePZpaC52zLoXHdKX6Xqq53scnLF26NFgPnZu+YcOG4LJPP12/K8TnvWR53usg7N27N9f6i8Ce\nH4gU4QciRfiBSBF+IFKEH4gU4QciRfiBSGWO85vZZEnPSBovySUtc/clZjZO0m8lTZHUIekmd/+o\nfq3mk3e8OzSl8p49e4LLjh49Oljv6ekJ1ht1HfeBZB3DsHbt2mD98OHDqbWXX365pp4aIe/fS9by\n77//fq7XL0I1e/5eST919wsl/ZOkH5vZhZIWSFrt7tMkrU7uAxgkMsPv7nvcfUNy+5Ck9yRNknS9\npBXJ01ZIuqFeTQIo3gl95jezKZK+JenPksa7+/H3u3vV97EAwCBRdfjNbLSk30n6ibt3969534Hz\nAx48b2bzzKzNzNq6urpyNQugOFWF38yGqS/4v3b33ycP7zOziUl9oqTOgZZ192XuXnH3SmtraxE9\nAyhAZvit72vLpyW95+79p1V9SdLc5PZcSS8W3x6AeqnmlN5vS/qBpM1mtjF57D5JD0n6LzO7TdJf\nJN1UzQpDp9bW8/TTvKf0Tp06tabaYHfnnXcG61mX116/fn2R7RQm65TcPFOyS9l/T++++26u1y9C\nZvjdfb2ktN/kqmLbAdAoHOEHRIrwA5Ei/ECkCD8QKcIPRIrwA5Fq+KW7y5rqOu96Q+O+t9xyS3DZ\nrNM3H3nkkWD98ssvD9bzePLJJ4P1Z599NlhftGhRsD5x4sQT7unrIOvS3Z2dAx4Q21Ds+YFIEX4g\nUoQfiBThByJF+IFIEX4gUoQfiFRDx/mPHj2qrVu3ptazLlE9YsSI1FrW5bGzXnvIkCHB+tCh6Zvq\nlFNOCS67ZcuWYP3+++8P1l955ZVgfdSoUXVb9+zZs4P1e++9N1gfrPIeF5I1zj9y5Mhcr18E9vxA\npAg/ECnCD0SK8AORIvxApAg/ECnCD0SqoeP8PT092rVrV2o9dAyAJO3bty+19vHHHweXzRrnb2lp\nCdZD13k/55xzgsveddddwfr06dOD9fb29mD99ddfT61t3LgxtSZJs2bNCtYfffTRYD3r+Ije3t7U\nWujYicFu+PDhwfp1113XoE7SsecHIkX4gUgRfiBShB+IFOEHIkX4gUgRfiBSVsW89ZMlPSNpvCSX\ntMzdl5jZQkk/lNSVPPU+d18Veq1KpeJZ87nXy5EjR4L1jz76KFjv7u5Ore3duze4bNY23rZtW7C+\nefPmYD10jMNVV4VnUc86X/+0004L1jGwgwcPBuvnnntuam3//v01r7dSqaitra2qixFUc5RFr6Sf\nuvsGMztVUruZvZbUfubu/1FrowDKkxl+d98jaU9y+5CZvSdpUr0bA1BfJ/SZ38ymSPqWpD8nD803\ns01mttzMxqYsM8/M2sysraura6CnAChB1eE3s9GSfifpJ+7eLennkr4p6VL1vTMY8CBwd1/m7hV3\nr7S2thbQMoAiVBV+MxumvuD/2t1/L0nuvs/dj7n755J+IWlG/doEULTM8FvfZUyflvSeuz/W7/H+\n06/OlhS+TCyAplLNt/3flvQDSZvN7Pj5ofdJmmNml6pv+K9D0o/q0mFBQpf9rqZ+5plnptbOP//8\nmno67sorr8y1PJrPmDFjgvWsS6Y3QjXf9q+XNNC4YXBMH0Bz4wg/IFKEH4gU4QciRfiBSBF+IFKE\nH4jU1/fayUATy7qceyOw5wciRfiBSBF+IFKEH4gU4QciRfiBSBF+IFKZl+4udGVmXZL+0u+hFkm1\nX6e4vpq1t2btS6K3WhXZ2znuXtX18hoa/q+s3KzN3SulNRDQrL01a18SvdWqrN542w9EivADkSo7\n/MtKXn9Is/bWrH1J9FarUnor9TM/gPKUvecHUJJSwm9mV5vZVjPbbmYLyughjZl1mNlmM9toZuVM\nKfz/vSw3s04z29LvsXFm9pqZbUt+DjhNWkm9LTSz3cm222hms0rqbbKZrTGzd83sHTP71+TxUrdd\noK9StlvD3/ab2RBJ/yvpu5J2SXpT0hx3f7ehjaQwsw5JFXcvfUzYzC6X9DdJz7j7Rcljj0g64O4P\nJf/jHOvu/9YkvS2U9LeyZ25OJpSZ2H9maUk3SPoXlbjtAn3dpBK2Wxl7/hmStrv7Dnf/TNJvJF1f\nQh9Nz93XSjrwpYevl7Qiub1CfX88DZfSW1Nw9z3uviG5fUjS8ZmlS912gb5KUUb4J0n6a7/7u9Rc\nU367pD+YWbuZzSu7mQGMT6ZNl6S9ksaX2cwAMmdubqQvzSzdNNuulhmvi8YXfl81090vk3SNpB8n\nb2+bkvd9Zmum4ZqqZm5ulAFmlv67MrddrTNeF62M8O+WNLnf/bOSx5qCu+9OfnZKekHNN/vwvuOT\npCY/O0vu5++aaebmgWaWVhNsu2aa8bqM8L8paZqZfcPMTpb0fUkvldDHV5jZqOSLGJnZKEnfU/PN\nPvySpLnJ7bmSXiyxly9olpmb02aWVsnbrulmvHb3hv+TNEt93/i/L+nfy+ghpa+pkt5O/r1Tdm+S\nnlPf28Ae9X03cpukMyStlrRN0h8ljWui3n4labOkTeoL2sSSepupvrf0myRtTP7NKnvbBfoqZbtx\nhB8QKb7wAyJF+IFIEX4gUoQfiBThByJF+IFIEX4gUoQfiNT/AYjLTiT7+4DNAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nA/PYiG1SKFWvAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(0,10):\n",
    "  plt.imshow(trainX[i], cmap='Greys')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l4TbJGeSOIU4"
   },
   "source": [
    "### Build a neural Network with a cross entropy loss function and sgd optimizer in Keras. The output layer with 10 neurons as we have 10 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7034,
     "status": "ok",
     "timestamp": 1569734335328,
     "user": {
      "displayName": "Raghav Praneeth Amaravadi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAMhpxsJE7d0g9jOsWi8V_E3rjg4Ut9Yz-RLZWsQw=s64",
      "userId": "05628056859539857657"
     },
     "user_tz": -330
    },
    "id": "Ac06XZZTOIU6",
    "outputId": "99db6302-22e8-455c-8e28-4c32cb27a5da"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0929 05:18:56.247464 139937936250752 deprecation.py:506] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling __init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "#Initialize Sequential model\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "#Reshape data from 2D to 1D -> 28x28 to 784\n",
    "model.add(tf.keras.layers.Reshape((784,),input_shape=(28,28,)))\n",
    "\n",
    "#Add Dense Layer which provides 10 Outputs after applying softmax\n",
    "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "#Comile the model\n",
    "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3hQpLv3aOIU_"
   },
   "source": [
    "### Execute the model using model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 583670,
     "status": "ok",
     "timestamp": 1569734912007,
     "user": {
      "displayName": "Raghav Praneeth Amaravadi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAMhpxsJE7d0g9jOsWi8V_E3rjg4Ut9Yz-RLZWsQw=s64",
      "userId": "05628056859539857657"
     },
     "user_tz": -330
    },
    "id": "O59C_-IgOIVB",
    "outputId": "fcc24cb8-9c4e-4275-d679-06d84dbcdd3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "60000/60000 [==============================] - 6s 107us/sample - loss: 2090.7693 - acc: 0.7407 - val_loss: 1573.7273 - val_acc: 0.7904\n",
      "Epoch 2/100\n",
      "60000/60000 [==============================] - 6s 97us/sample - loss: 1630.7235 - acc: 0.7782 - val_loss: 2817.6766 - val_acc: 0.6670\n",
      "Epoch 3/100\n",
      "60000/60000 [==============================] - 6s 96us/sample - loss: 1563.9093 - acc: 0.7843 - val_loss: 1603.4943 - val_acc: 0.7958\n",
      "Epoch 4/100\n",
      "60000/60000 [==============================] - 6s 94us/sample - loss: 1483.6755 - acc: 0.7904 - val_loss: 2085.5669 - val_acc: 0.7576\n",
      "Epoch 5/100\n",
      "60000/60000 [==============================] - 6s 94us/sample - loss: 1502.0720 - acc: 0.7928 - val_loss: 1157.8106 - val_acc: 0.8080\n",
      "Epoch 6/100\n",
      "60000/60000 [==============================] - 6s 98us/sample - loss: 1481.5293 - acc: 0.7955 - val_loss: 1831.5234 - val_acc: 0.7779\n",
      "Epoch 7/100\n",
      "60000/60000 [==============================] - 6s 102us/sample - loss: 1476.7170 - acc: 0.7950 - val_loss: 1610.8739 - val_acc: 0.7933\n",
      "Epoch 8/100\n",
      "60000/60000 [==============================] - 6s 97us/sample - loss: 1442.5526 - acc: 0.7972 - val_loss: 2262.2816 - val_acc: 0.7547\n",
      "Epoch 9/100\n",
      "60000/60000 [==============================] - 6s 97us/sample - loss: 1490.1138 - acc: 0.7988 - val_loss: 1055.1433 - val_acc: 0.8034\n",
      "Epoch 10/100\n",
      "60000/60000 [==============================] - 6s 97us/sample - loss: 1406.1418 - acc: 0.8023 - val_loss: 1329.8616 - val_acc: 0.8114\n",
      "Epoch 11/100\n",
      "60000/60000 [==============================] - 6s 96us/sample - loss: 1418.7416 - acc: 0.8000 - val_loss: 1270.3481 - val_acc: 0.7972\n",
      "Epoch 12/100\n",
      "60000/60000 [==============================] - 6s 94us/sample - loss: 1455.5431 - acc: 0.7989 - val_loss: 1179.6298 - val_acc: 0.8078\n",
      "Epoch 13/100\n",
      "60000/60000 [==============================] - 6s 95us/sample - loss: 1397.5763 - acc: 0.8030 - val_loss: 1919.9551 - val_acc: 0.7592\n",
      "Epoch 14/100\n",
      "60000/60000 [==============================] - 6s 94us/sample - loss: 1452.1672 - acc: 0.8011 - val_loss: 1351.8366 - val_acc: 0.8127\n",
      "Epoch 15/100\n",
      "60000/60000 [==============================] - 6s 94us/sample - loss: 1414.5170 - acc: 0.8033 - val_loss: 2317.8670 - val_acc: 0.7468\n",
      "Epoch 16/100\n",
      "60000/60000 [==============================] - 6s 93us/sample - loss: 1436.6111 - acc: 0.8005 - val_loss: 1081.3598 - val_acc: 0.8275\n",
      "Epoch 17/100\n",
      "60000/60000 [==============================] - 6s 94us/sample - loss: 1388.8257 - acc: 0.8019 - val_loss: 1077.3791 - val_acc: 0.8181\n",
      "Epoch 18/100\n",
      "60000/60000 [==============================] - 6s 93us/sample - loss: 1372.9378 - acc: 0.8051 - val_loss: 1207.0351 - val_acc: 0.7890\n",
      "Epoch 19/100\n",
      "60000/60000 [==============================] - 6s 99us/sample - loss: 1414.2692 - acc: 0.8047 - val_loss: 2823.4486 - val_acc: 0.7475\n",
      "Epoch 20/100\n",
      "60000/60000 [==============================] - 6s 99us/sample - loss: 1397.7990 - acc: 0.8056 - val_loss: 1196.6451 - val_acc: 0.8126\n",
      "Epoch 21/100\n",
      "60000/60000 [==============================] - 6s 94us/sample - loss: 1369.5723 - acc: 0.8056 - val_loss: 1509.6375 - val_acc: 0.7836\n",
      "Epoch 22/100\n",
      "60000/60000 [==============================] - 6s 94us/sample - loss: 1382.2367 - acc: 0.8063 - val_loss: 1408.6091 - val_acc: 0.7909\n",
      "Epoch 23/100\n",
      "60000/60000 [==============================] - 6s 94us/sample - loss: 1408.8011 - acc: 0.8055 - val_loss: 968.1612 - val_acc: 0.8287\n",
      "Epoch 24/100\n",
      "60000/60000 [==============================] - 6s 94us/sample - loss: 1354.3262 - acc: 0.8063 - val_loss: 1162.8309 - val_acc: 0.8052\n",
      "Epoch 25/100\n",
      "60000/60000 [==============================] - 6s 93us/sample - loss: 1355.5278 - acc: 0.8071 - val_loss: 970.7881 - val_acc: 0.8156\n",
      "Epoch 26/100\n",
      "60000/60000 [==============================] - 6s 94us/sample - loss: 1420.7839 - acc: 0.8064 - val_loss: 1615.8276 - val_acc: 0.7664\n",
      "Epoch 27/100\n",
      "60000/60000 [==============================] - 6s 95us/sample - loss: 1407.4628 - acc: 0.8063 - val_loss: 1051.3337 - val_acc: 0.8106\n",
      "Epoch 28/100\n",
      "60000/60000 [==============================] - 6s 95us/sample - loss: 1361.3689 - acc: 0.8065 - val_loss: 1624.6181 - val_acc: 0.7614\n",
      "Epoch 29/100\n",
      "60000/60000 [==============================] - 6s 94us/sample - loss: 1365.1826 - acc: 0.8090 - val_loss: 1230.3996 - val_acc: 0.8116\n",
      "Epoch 30/100\n",
      "60000/60000 [==============================] - 6s 94us/sample - loss: 1370.7451 - acc: 0.8079 - val_loss: 1505.1498 - val_acc: 0.7800\n",
      "Epoch 31/100\n",
      "60000/60000 [==============================] - 6s 95us/sample - loss: 1379.5213 - acc: 0.8064 - val_loss: 1051.8343 - val_acc: 0.8198\n",
      "Epoch 32/100\n",
      "60000/60000 [==============================] - 6s 95us/sample - loss: 1375.1736 - acc: 0.8091 - val_loss: 1091.4139 - val_acc: 0.8166\n",
      "Epoch 33/100\n",
      "60000/60000 [==============================] - 6s 94us/sample - loss: 1411.3930 - acc: 0.8056 - val_loss: 1879.3624 - val_acc: 0.7145\n",
      "Epoch 34/100\n",
      "60000/60000 [==============================] - 6s 94us/sample - loss: 1388.4164 - acc: 0.8079 - val_loss: 1555.5153 - val_acc: 0.7990\n",
      "Epoch 35/100\n",
      "60000/60000 [==============================] - 6s 94us/sample - loss: 1386.5824 - acc: 0.8092 - val_loss: 1199.3426 - val_acc: 0.8095\n",
      "Epoch 36/100\n",
      "60000/60000 [==============================] - 6s 95us/sample - loss: 1401.2416 - acc: 0.8080 - val_loss: 1070.2699 - val_acc: 0.8298\n",
      "Epoch 37/100\n",
      "60000/60000 [==============================] - 6s 94us/sample - loss: 1375.2860 - acc: 0.8078 - val_loss: 1078.5624 - val_acc: 0.8124\n",
      "Epoch 38/100\n",
      "60000/60000 [==============================] - 6s 95us/sample - loss: 1415.8760 - acc: 0.8073 - val_loss: 1122.6098 - val_acc: 0.8160\n",
      "Epoch 39/100\n",
      "60000/60000 [==============================] - 6s 96us/sample - loss: 1413.3480 - acc: 0.8085 - val_loss: 1081.0191 - val_acc: 0.8171\n",
      "Epoch 40/100\n",
      "60000/60000 [==============================] - 6s 94us/sample - loss: 1316.4512 - acc: 0.8099 - val_loss: 1098.8352 - val_acc: 0.7959\n",
      "Epoch 41/100\n",
      "60000/60000 [==============================] - 6s 94us/sample - loss: 1377.0350 - acc: 0.8108 - val_loss: 1672.1315 - val_acc: 0.7732\n",
      "Epoch 42/100\n",
      "60000/60000 [==============================] - 6s 95us/sample - loss: 1338.8254 - acc: 0.8108 - val_loss: 2830.5862 - val_acc: 0.7370\n",
      "Epoch 43/100\n",
      "60000/60000 [==============================] - 6s 94us/sample - loss: 1377.0037 - acc: 0.8076 - val_loss: 1202.0289 - val_acc: 0.8051\n",
      "Epoch 44/100\n",
      "60000/60000 [==============================] - 6s 95us/sample - loss: 1355.2828 - acc: 0.8109 - val_loss: 1668.5576 - val_acc: 0.7794\n",
      "Epoch 45/100\n",
      "60000/60000 [==============================] - 6s 96us/sample - loss: 1364.7860 - acc: 0.8096 - val_loss: 2507.9470 - val_acc: 0.7143\n",
      "Epoch 46/100\n",
      "60000/60000 [==============================] - 6s 95us/sample - loss: 1367.5180 - acc: 0.8108 - val_loss: 1494.1947 - val_acc: 0.7823\n",
      "Epoch 47/100\n",
      "60000/60000 [==============================] - 6s 95us/sample - loss: 1322.8993 - acc: 0.8134 - val_loss: 1261.5121 - val_acc: 0.8090\n",
      "Epoch 48/100\n",
      "60000/60000 [==============================] - 6s 94us/sample - loss: 1353.9219 - acc: 0.8106 - val_loss: 1340.3933 - val_acc: 0.8021\n",
      "Epoch 49/100\n",
      "60000/60000 [==============================] - 6s 95us/sample - loss: 1354.6912 - acc: 0.8098 - val_loss: 1619.1105 - val_acc: 0.7775\n",
      "Epoch 50/100\n",
      "60000/60000 [==============================] - 6s 95us/sample - loss: 1345.0374 - acc: 0.8103 - val_loss: 1374.4019 - val_acc: 0.7873\n",
      "Epoch 51/100\n",
      "60000/60000 [==============================] - 6s 101us/sample - loss: 1334.7782 - acc: 0.8109 - val_loss: 2241.2802 - val_acc: 0.7856\n",
      "Epoch 52/100\n",
      "60000/60000 [==============================] - 6s 95us/sample - loss: 1361.6490 - acc: 0.8100 - val_loss: 2186.8204 - val_acc: 0.7934\n",
      "Epoch 53/100\n",
      "60000/60000 [==============================] - 6s 96us/sample - loss: 1366.1334 - acc: 0.8112 - val_loss: 1104.8606 - val_acc: 0.8201\n",
      "Epoch 54/100\n",
      "60000/60000 [==============================] - 6s 95us/sample - loss: 1356.1221 - acc: 0.8118 - val_loss: 1307.0362 - val_acc: 0.7897\n",
      "Epoch 55/100\n",
      "60000/60000 [==============================] - 6s 97us/sample - loss: 1335.1237 - acc: 0.8133 - val_loss: 1725.2331 - val_acc: 0.7888\n",
      "Epoch 56/100\n",
      "60000/60000 [==============================] - 6s 94us/sample - loss: 1356.9004 - acc: 0.8105 - val_loss: 1474.8156 - val_acc: 0.7731\n",
      "Epoch 57/100\n",
      "60000/60000 [==============================] - 6s 95us/sample - loss: 1367.9047 - acc: 0.8086 - val_loss: 1500.1021 - val_acc: 0.7717\n",
      "Epoch 58/100\n",
      "60000/60000 [==============================] - 6s 94us/sample - loss: 1374.9381 - acc: 0.8112 - val_loss: 1138.3443 - val_acc: 0.8106\n",
      "Epoch 59/100\n",
      "60000/60000 [==============================] - 6s 96us/sample - loss: 1329.2536 - acc: 0.8139 - val_loss: 1253.6433 - val_acc: 0.8077\n",
      "Epoch 60/100\n",
      "60000/60000 [==============================] - 6s 96us/sample - loss: 1350.5830 - acc: 0.8114 - val_loss: 1233.6303 - val_acc: 0.8145\n",
      "Epoch 61/100\n",
      "60000/60000 [==============================] - 6s 95us/sample - loss: 1357.7676 - acc: 0.8123 - val_loss: 1579.3059 - val_acc: 0.7667\n",
      "Epoch 62/100\n",
      "60000/60000 [==============================] - 6s 95us/sample - loss: 1359.0684 - acc: 0.8126 - val_loss: 2134.2663 - val_acc: 0.7737\n",
      "Epoch 63/100\n",
      "60000/60000 [==============================] - 6s 96us/sample - loss: 1340.3643 - acc: 0.8133 - val_loss: 1423.8875 - val_acc: 0.7733\n",
      "Epoch 64/100\n",
      "60000/60000 [==============================] - 6s 95us/sample - loss: 1308.7155 - acc: 0.8138 - val_loss: 1877.5905 - val_acc: 0.7460\n",
      "Epoch 65/100\n",
      "60000/60000 [==============================] - 6s 95us/sample - loss: 1318.4394 - acc: 0.8126 - val_loss: 4267.9490 - val_acc: 0.6492\n",
      "Epoch 66/100\n",
      "60000/60000 [==============================] - 6s 96us/sample - loss: 1368.3267 - acc: 0.8110 - val_loss: 1375.3559 - val_acc: 0.7871\n",
      "Epoch 67/100\n",
      "60000/60000 [==============================] - 6s 95us/sample - loss: 1348.0814 - acc: 0.8107 - val_loss: 1400.9963 - val_acc: 0.7971\n",
      "Epoch 68/100\n",
      "60000/60000 [==============================] - 6s 95us/sample - loss: 1341.6286 - acc: 0.8118 - val_loss: 1889.3579 - val_acc: 0.7492\n",
      "Epoch 69/100\n",
      "60000/60000 [==============================] - 6s 95us/sample - loss: 1372.1860 - acc: 0.8111 - val_loss: 2203.0469 - val_acc: 0.7357\n",
      "Epoch 70/100\n",
      "60000/60000 [==============================] - 6s 96us/sample - loss: 1334.4361 - acc: 0.8108 - val_loss: 2054.2700 - val_acc: 0.7644\n",
      "Epoch 71/100\n",
      "60000/60000 [==============================] - 6s 97us/sample - loss: 1324.5643 - acc: 0.8146 - val_loss: 1346.9067 - val_acc: 0.8059\n",
      "Epoch 72/100\n",
      "60000/60000 [==============================] - 6s 95us/sample - loss: 1337.0955 - acc: 0.8115 - val_loss: 1101.5237 - val_acc: 0.8120\n",
      "Epoch 73/100\n",
      "60000/60000 [==============================] - 6s 99us/sample - loss: 1346.4189 - acc: 0.8113 - val_loss: 1069.7023 - val_acc: 0.8204\n",
      "Epoch 74/100\n",
      "60000/60000 [==============================] - 6s 101us/sample - loss: 1374.6018 - acc: 0.8133 - val_loss: 1685.6536 - val_acc: 0.7577\n",
      "Epoch 75/100\n",
      "60000/60000 [==============================] - 6s 98us/sample - loss: 1327.6131 - acc: 0.8142 - val_loss: 1928.0439 - val_acc: 0.7658\n",
      "Epoch 76/100\n",
      "60000/60000 [==============================] - 6s 93us/sample - loss: 1319.7217 - acc: 0.8133 - val_loss: 1373.0189 - val_acc: 0.8120\n",
      "Epoch 77/100\n",
      "60000/60000 [==============================] - 6s 94us/sample - loss: 1311.7652 - acc: 0.8134 - val_loss: 1344.2328 - val_acc: 0.7978\n",
      "Epoch 78/100\n",
      "60000/60000 [==============================] - 6s 94us/sample - loss: 1314.3893 - acc: 0.8153 - val_loss: 1268.7139 - val_acc: 0.8067\n",
      "Epoch 79/100\n",
      "60000/60000 [==============================] - 6s 95us/sample - loss: 1339.8327 - acc: 0.8135 - val_loss: 1628.6687 - val_acc: 0.7865\n",
      "Epoch 80/100\n",
      "60000/60000 [==============================] - 6s 95us/sample - loss: 1313.6086 - acc: 0.8141 - val_loss: 1138.3936 - val_acc: 0.8206\n",
      "Epoch 81/100\n",
      "60000/60000 [==============================] - 6s 95us/sample - loss: 1406.8607 - acc: 0.8102 - val_loss: 1937.0286 - val_acc: 0.7699\n",
      "Epoch 82/100\n",
      "60000/60000 [==============================] - 6s 96us/sample - loss: 1320.1365 - acc: 0.8133 - val_loss: 2831.1608 - val_acc: 0.7416\n",
      "Epoch 83/100\n",
      "60000/60000 [==============================] - 6s 95us/sample - loss: 1328.6493 - acc: 0.8132 - val_loss: 1499.7887 - val_acc: 0.7770\n",
      "Epoch 84/100\n",
      "60000/60000 [==============================] - 6s 95us/sample - loss: 1337.2611 - acc: 0.8136 - val_loss: 1704.7019 - val_acc: 0.7818\n",
      "Epoch 85/100\n",
      "60000/60000 [==============================] - 6s 96us/sample - loss: 1345.1773 - acc: 0.8125 - val_loss: 1423.5432 - val_acc: 0.8040\n",
      "Epoch 86/100\n",
      "60000/60000 [==============================] - 6s 96us/sample - loss: 1340.4846 - acc: 0.8146 - val_loss: 2462.3734 - val_acc: 0.7636\n",
      "Epoch 87/100\n",
      "60000/60000 [==============================] - 6s 95us/sample - loss: 1332.6116 - acc: 0.8122 - val_loss: 3306.6121 - val_acc: 0.7078\n",
      "Epoch 88/100\n",
      "60000/60000 [==============================] - 6s 96us/sample - loss: 1317.0669 - acc: 0.8127 - val_loss: 1163.6679 - val_acc: 0.8236\n",
      "Epoch 89/100\n",
      "60000/60000 [==============================] - 6s 95us/sample - loss: 1303.9550 - acc: 0.8130 - val_loss: 1757.8797 - val_acc: 0.7907\n",
      "Epoch 90/100\n",
      "60000/60000 [==============================] - 6s 95us/sample - loss: 1319.4275 - acc: 0.8155 - val_loss: 2129.8226 - val_acc: 0.7322\n",
      "Epoch 91/100\n",
      "60000/60000 [==============================] - 6s 95us/sample - loss: 1332.7957 - acc: 0.8148 - val_loss: 1277.9544 - val_acc: 0.7974\n",
      "Epoch 92/100\n",
      "60000/60000 [==============================] - 6s 93us/sample - loss: 1326.2472 - acc: 0.8146 - val_loss: 3216.3025 - val_acc: 0.7144\n",
      "Epoch 93/100\n",
      "60000/60000 [==============================] - 6s 94us/sample - loss: 1314.8995 - acc: 0.8143 - val_loss: 5928.2794 - val_acc: 0.5884\n",
      "Epoch 94/100\n",
      "60000/60000 [==============================] - 6s 95us/sample - loss: 1366.0630 - acc: 0.8120 - val_loss: 2043.9687 - val_acc: 0.7762\n",
      "Epoch 95/100\n",
      "60000/60000 [==============================] - 6s 94us/sample - loss: 1295.4285 - acc: 0.8138 - val_loss: 1721.8017 - val_acc: 0.7745\n",
      "Epoch 96/100\n",
      "60000/60000 [==============================] - 6s 95us/sample - loss: 1348.4099 - acc: 0.8130 - val_loss: 1906.0772 - val_acc: 0.7499\n",
      "Epoch 97/100\n",
      "60000/60000 [==============================] - 6s 96us/sample - loss: 1315.7957 - acc: 0.8139 - val_loss: 2276.4592 - val_acc: 0.7560\n",
      "Epoch 98/100\n",
      "60000/60000 [==============================] - 6s 94us/sample - loss: 1327.7099 - acc: 0.8136 - val_loss: 1379.5575 - val_acc: 0.8111\n",
      "Epoch 99/100\n",
      "60000/60000 [==============================] - 6s 94us/sample - loss: 1297.9874 - acc: 0.8135 - val_loss: 1224.3149 - val_acc: 0.8221\n",
      "Epoch 100/100\n",
      "60000/60000 [==============================] - 6s 94us/sample - loss: 1330.5334 - acc: 0.8147 - val_loss: 1473.0018 - val_acc: 0.7763\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f457b806d50>"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trainX, trainY, validation_data=(testX, testY), epochs=100, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JdzDtGwDOIVF"
   },
   "source": [
    "### In the above Neural Network model add Batch Normalization layer after the input layer and repeat the steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kndfpdidOIVI"
   },
   "outputs": [],
   "source": [
    "#Initialize Sequential model\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "#Reshape data from 2D to 1D -> 28x28 to 784\n",
    "model.add(tf.keras.layers.Reshape((784,),input_shape=(28,28,)))\n",
    "\n",
    "#Normalize the data\n",
    "model.add(tf.keras.layers.BatchNormalization(input_shape=(4,)))\n",
    "\n",
    "#Add Dense Layer which provides 10 Outputs after applying softmax\n",
    "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "#Comile the model\n",
    "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mwk3T5LJOIVN"
   },
   "source": [
    "### Execute the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2559318,
     "status": "ok",
     "timestamp": 1569736887718,
     "user": {
      "displayName": "Raghav Praneeth Amaravadi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAMhpxsJE7d0g9jOsWi8V_E3rjg4Ut9Yz-RLZWsQw=s64",
      "userId": "05628056859539857657"
     },
     "user_tz": -330
    },
    "id": "JNLR8tcBOIVP",
    "outputId": "afe66f0b-caae-479d-b09b-0bfab2bc0ab9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/300\n",
      "60000/60000 [==============================] - 7s 111us/sample - loss: 0.5922 - acc: 0.7980 - val_loss: 0.5178 - val_acc: 0.8265\n",
      "Epoch 2/300\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 0.4898 - acc: 0.8310 - val_loss: 0.4970 - val_acc: 0.8321\n",
      "Epoch 3/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.4660 - acc: 0.8389 - val_loss: 0.4821 - val_acc: 0.8374\n",
      "Epoch 4/300\n",
      "60000/60000 [==============================] - 7s 114us/sample - loss: 0.4564 - acc: 0.8423 - val_loss: 0.4825 - val_acc: 0.8366\n",
      "Epoch 5/300\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 0.4486 - acc: 0.8443 - val_loss: 0.4784 - val_acc: 0.8363\n",
      "Epoch 6/300\n",
      "60000/60000 [==============================] - 6s 107us/sample - loss: 0.4426 - acc: 0.8467 - val_loss: 0.4692 - val_acc: 0.8393\n",
      "Epoch 7/300\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 0.4371 - acc: 0.8486 - val_loss: 0.4646 - val_acc: 0.8406\n",
      "Epoch 8/300\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 0.4362 - acc: 0.8485 - val_loss: 0.4813 - val_acc: 0.8347\n",
      "Epoch 9/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.4342 - acc: 0.8495 - val_loss: 0.4619 - val_acc: 0.8419\n",
      "Epoch 10/300\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 0.4312 - acc: 0.8505 - val_loss: 0.4620 - val_acc: 0.8436\n",
      "Epoch 11/300\n",
      "60000/60000 [==============================] - 7s 111us/sample - loss: 0.4257 - acc: 0.8515 - val_loss: 0.4630 - val_acc: 0.8398\n",
      "Epoch 12/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.4259 - acc: 0.8515 - val_loss: 0.4591 - val_acc: 0.8414\n",
      "Epoch 13/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.4225 - acc: 0.8528 - val_loss: 0.4690 - val_acc: 0.8410\n",
      "Epoch 14/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.4252 - acc: 0.8506 - val_loss: 0.4690 - val_acc: 0.8434\n",
      "Epoch 15/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.4209 - acc: 0.8527 - val_loss: 0.4736 - val_acc: 0.8421\n",
      "Epoch 16/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.4225 - acc: 0.8524 - val_loss: 0.4615 - val_acc: 0.8436\n",
      "Epoch 17/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.4200 - acc: 0.8532 - val_loss: 0.4616 - val_acc: 0.8440\n",
      "Epoch 18/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.4213 - acc: 0.8513 - val_loss: 0.4547 - val_acc: 0.8446\n",
      "Epoch 19/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.4173 - acc: 0.8532 - val_loss: 0.4679 - val_acc: 0.8386\n",
      "Epoch 20/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.4159 - acc: 0.8549 - val_loss: 0.4810 - val_acc: 0.8430\n",
      "Epoch 21/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.4166 - acc: 0.8544 - val_loss: 0.4626 - val_acc: 0.8392\n",
      "Epoch 22/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.4158 - acc: 0.8541 - val_loss: 0.4770 - val_acc: 0.8402\n",
      "Epoch 23/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.4141 - acc: 0.8551 - val_loss: 0.4636 - val_acc: 0.8412\n",
      "Epoch 24/300\n",
      "60000/60000 [==============================] - 7s 114us/sample - loss: 0.4138 - acc: 0.8533 - val_loss: 0.4649 - val_acc: 0.8414\n",
      "Epoch 25/300\n",
      "60000/60000 [==============================] - 7s 115us/sample - loss: 0.4130 - acc: 0.8557 - val_loss: 0.4609 - val_acc: 0.8430\n",
      "Epoch 26/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.4145 - acc: 0.8540 - val_loss: 0.4728 - val_acc: 0.8397\n",
      "Epoch 27/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.4127 - acc: 0.8545 - val_loss: 0.4670 - val_acc: 0.8408\n",
      "Epoch 28/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.4122 - acc: 0.8560 - val_loss: 0.4646 - val_acc: 0.8393\n",
      "Epoch 29/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.4110 - acc: 0.8572 - val_loss: 0.4740 - val_acc: 0.8388\n",
      "Epoch 30/300\n",
      "60000/60000 [==============================] - 7s 111us/sample - loss: 0.4095 - acc: 0.8568 - val_loss: 0.4818 - val_acc: 0.8403\n",
      "Epoch 31/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.4094 - acc: 0.8561 - val_loss: 0.4889 - val_acc: 0.8413\n",
      "Epoch 32/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.4090 - acc: 0.8565 - val_loss: 0.4830 - val_acc: 0.8422\n",
      "Epoch 33/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.4082 - acc: 0.8569 - val_loss: 0.4668 - val_acc: 0.8414\n",
      "Epoch 34/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.4097 - acc: 0.8565 - val_loss: 0.4728 - val_acc: 0.8431\n",
      "Epoch 35/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.4093 - acc: 0.8569 - val_loss: 0.4739 - val_acc: 0.8419\n",
      "Epoch 36/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.4092 - acc: 0.8563 - val_loss: 0.4686 - val_acc: 0.8413\n",
      "Epoch 37/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.4086 - acc: 0.8572 - val_loss: 0.4826 - val_acc: 0.8425\n",
      "Epoch 38/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.4077 - acc: 0.8568 - val_loss: 0.4858 - val_acc: 0.8425\n",
      "Epoch 39/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.4067 - acc: 0.8570 - val_loss: 0.4918 - val_acc: 0.8395\n",
      "Epoch 40/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.4073 - acc: 0.8572 - val_loss: 0.4749 - val_acc: 0.8448\n",
      "Epoch 41/300\n",
      "60000/60000 [==============================] - 6s 107us/sample - loss: 0.4067 - acc: 0.8561 - val_loss: 0.4697 - val_acc: 0.8418\n",
      "Epoch 42/300\n",
      "60000/60000 [==============================] - 7s 111us/sample - loss: 0.4062 - acc: 0.8565 - val_loss: 0.4695 - val_acc: 0.8395\n",
      "Epoch 43/300\n",
      "60000/60000 [==============================] - 7s 111us/sample - loss: 0.4050 - acc: 0.8561 - val_loss: 0.4636 - val_acc: 0.8408\n",
      "Epoch 44/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.4083 - acc: 0.8558 - val_loss: 0.4769 - val_acc: 0.8416\n",
      "Epoch 45/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.4058 - acc: 0.8567 - val_loss: 0.4845 - val_acc: 0.8407\n",
      "Epoch 46/300\n",
      "60000/60000 [==============================] - 6s 107us/sample - loss: 0.4041 - acc: 0.8574 - val_loss: 0.4778 - val_acc: 0.8421\n",
      "Epoch 47/300\n",
      "60000/60000 [==============================] - 7s 111us/sample - loss: 0.4060 - acc: 0.8572 - val_loss: 0.4718 - val_acc: 0.8447\n",
      "Epoch 48/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.4039 - acc: 0.8586 - val_loss: 0.4738 - val_acc: 0.8419\n",
      "Epoch 49/300\n",
      "60000/60000 [==============================] - 7s 108us/sample - loss: 0.4056 - acc: 0.8584 - val_loss: 0.4802 - val_acc: 0.8410\n",
      "Epoch 50/300\n",
      "60000/60000 [==============================] - 7s 115us/sample - loss: 0.4059 - acc: 0.8571 - val_loss: 0.4672 - val_acc: 0.8415\n",
      "Epoch 51/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.4036 - acc: 0.8571 - val_loss: 0.4758 - val_acc: 0.8396\n",
      "Epoch 52/300\n",
      "60000/60000 [==============================] - 7s 108us/sample - loss: 0.4045 - acc: 0.8575 - val_loss: 0.4692 - val_acc: 0.8396\n",
      "Epoch 53/300\n",
      "60000/60000 [==============================] - 6s 107us/sample - loss: 0.4071 - acc: 0.8556 - val_loss: 0.5006 - val_acc: 0.8396\n",
      "Epoch 54/300\n",
      "60000/60000 [==============================] - 6s 107us/sample - loss: 0.4058 - acc: 0.8566 - val_loss: 0.4989 - val_acc: 0.8422\n",
      "Epoch 55/300\n",
      "60000/60000 [==============================] - 6s 107us/sample - loss: 0.4042 - acc: 0.8575 - val_loss: 0.4825 - val_acc: 0.8433\n",
      "Epoch 56/300\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 0.4062 - acc: 0.8570 - val_loss: 0.5045 - val_acc: 0.8360\n",
      "Epoch 57/300\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 0.4037 - acc: 0.8577 - val_loss: 0.4836 - val_acc: 0.8405\n",
      "Epoch 58/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.4031 - acc: 0.8571 - val_loss: 0.4752 - val_acc: 0.8397\n",
      "Epoch 59/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.4041 - acc: 0.8590 - val_loss: 0.4989 - val_acc: 0.8379\n",
      "Epoch 60/300\n",
      "60000/60000 [==============================] - 7s 108us/sample - loss: 0.4054 - acc: 0.8559 - val_loss: 0.4834 - val_acc: 0.8364\n",
      "Epoch 61/300\n",
      "60000/60000 [==============================] - 6s 107us/sample - loss: 0.4008 - acc: 0.8576 - val_loss: 0.4766 - val_acc: 0.8400\n",
      "Epoch 62/300\n",
      "60000/60000 [==============================] - 6s 107us/sample - loss: 0.4031 - acc: 0.8563 - val_loss: 0.4940 - val_acc: 0.8342\n",
      "Epoch 63/300\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 0.4027 - acc: 0.8580 - val_loss: 0.4924 - val_acc: 0.8360\n",
      "Epoch 64/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.4028 - acc: 0.8560 - val_loss: 0.4863 - val_acc: 0.8364\n",
      "Epoch 65/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.4037 - acc: 0.8569 - val_loss: 0.4677 - val_acc: 0.8427\n",
      "Epoch 66/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.4016 - acc: 0.8580 - val_loss: 0.4711 - val_acc: 0.8393\n",
      "Epoch 67/300\n",
      "60000/60000 [==============================] - 7s 111us/sample - loss: 0.4031 - acc: 0.8571 - val_loss: 0.5090 - val_acc: 0.8400\n",
      "Epoch 68/300\n",
      "60000/60000 [==============================] - 7s 112us/sample - loss: 0.4022 - acc: 0.8573 - val_loss: 0.4689 - val_acc: 0.8388\n",
      "Epoch 69/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.4027 - acc: 0.8590 - val_loss: 0.4724 - val_acc: 0.8387\n",
      "Epoch 70/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.4013 - acc: 0.8585 - val_loss: 0.4757 - val_acc: 0.8405\n",
      "Epoch 71/300\n",
      "60000/60000 [==============================] - 7s 111us/sample - loss: 0.4009 - acc: 0.8573 - val_loss: 0.4876 - val_acc: 0.8404\n",
      "Epoch 72/300\n",
      "60000/60000 [==============================] - 7s 116us/sample - loss: 0.4005 - acc: 0.8587 - val_loss: 0.4767 - val_acc: 0.8427\n",
      "Epoch 73/300\n",
      "60000/60000 [==============================] - 7s 111us/sample - loss: 0.4023 - acc: 0.8562 - val_loss: 0.4905 - val_acc: 0.8392\n",
      "Epoch 74/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.4032 - acc: 0.8568 - val_loss: 0.4790 - val_acc: 0.8404\n",
      "Epoch 75/300\n",
      "60000/60000 [==============================] - 6s 107us/sample - loss: 0.4029 - acc: 0.8583 - val_loss: 0.4907 - val_acc: 0.8405\n",
      "Epoch 76/300\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 0.4029 - acc: 0.8579 - val_loss: 0.4843 - val_acc: 0.8349\n",
      "Epoch 77/300\n",
      "60000/60000 [==============================] - 6s 107us/sample - loss: 0.3990 - acc: 0.8587 - val_loss: 0.4889 - val_acc: 0.8378\n",
      "Epoch 78/300\n",
      "60000/60000 [==============================] - 6s 107us/sample - loss: 0.4028 - acc: 0.8587 - val_loss: 0.4820 - val_acc: 0.8400\n",
      "Epoch 79/300\n",
      "60000/60000 [==============================] - 6s 107us/sample - loss: 0.3997 - acc: 0.8587 - val_loss: 0.4737 - val_acc: 0.8382\n",
      "Epoch 80/300\n",
      "60000/60000 [==============================] - 6s 107us/sample - loss: 0.4000 - acc: 0.8563 - val_loss: 0.4792 - val_acc: 0.8408\n",
      "Epoch 81/300\n",
      "60000/60000 [==============================] - 6s 107us/sample - loss: 0.4006 - acc: 0.8574 - val_loss: 0.4831 - val_acc: 0.8362\n",
      "Epoch 82/300\n",
      "60000/60000 [==============================] - 6s 107us/sample - loss: 0.4022 - acc: 0.8584 - val_loss: 0.4773 - val_acc: 0.8407\n",
      "Epoch 83/300\n",
      "60000/60000 [==============================] - 6s 107us/sample - loss: 0.4022 - acc: 0.8587 - val_loss: 0.4717 - val_acc: 0.8413\n",
      "Epoch 84/300\n",
      "60000/60000 [==============================] - 6s 107us/sample - loss: 0.4012 - acc: 0.8576 - val_loss: 0.4901 - val_acc: 0.8410\n",
      "Epoch 85/300\n",
      "60000/60000 [==============================] - 6s 106us/sample - loss: 0.3989 - acc: 0.8588 - val_loss: 0.4766 - val_acc: 0.8427\n",
      "Epoch 86/300\n",
      "60000/60000 [==============================] - 6s 106us/sample - loss: 0.4007 - acc: 0.8588 - val_loss: 0.4794 - val_acc: 0.8387\n",
      "Epoch 87/300\n",
      "60000/60000 [==============================] - 6s 107us/sample - loss: 0.3988 - acc: 0.8578 - val_loss: 0.4906 - val_acc: 0.8411\n",
      "Epoch 88/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.4006 - acc: 0.8576 - val_loss: 0.4880 - val_acc: 0.8397\n",
      "Epoch 89/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.4022 - acc: 0.8571 - val_loss: 0.4820 - val_acc: 0.8388\n",
      "Epoch 90/300\n",
      "60000/60000 [==============================] - 6s 107us/sample - loss: 0.3986 - acc: 0.8594 - val_loss: 0.4916 - val_acc: 0.8398\n",
      "Epoch 91/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.3985 - acc: 0.8605 - val_loss: 0.4830 - val_acc: 0.8404\n",
      "Epoch 92/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.4016 - acc: 0.8575 - val_loss: 0.4772 - val_acc: 0.8415\n",
      "Epoch 93/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.3987 - acc: 0.8587 - val_loss: 0.5008 - val_acc: 0.8401\n",
      "Epoch 94/300\n",
      "60000/60000 [==============================] - 7s 111us/sample - loss: 0.3974 - acc: 0.8592 - val_loss: 0.4827 - val_acc: 0.8416\n",
      "Epoch 95/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.3991 - acc: 0.8582 - val_loss: 0.5117 - val_acc: 0.8405\n",
      "Epoch 96/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.3990 - acc: 0.8598 - val_loss: 0.4880 - val_acc: 0.8384\n",
      "Epoch 97/300\n",
      "60000/60000 [==============================] - 7s 113us/sample - loss: 0.4001 - acc: 0.8581 - val_loss: 0.4906 - val_acc: 0.8362\n",
      "Epoch 98/300\n",
      "60000/60000 [==============================] - 6s 106us/sample - loss: 0.3960 - acc: 0.8610 - val_loss: 0.4856 - val_acc: 0.8394\n",
      "Epoch 99/300\n",
      "60000/60000 [==============================] - 6s 107us/sample - loss: 0.3982 - acc: 0.8592 - val_loss: 0.5053 - val_acc: 0.8400\n",
      "Epoch 100/300\n",
      "60000/60000 [==============================] - 6s 107us/sample - loss: 0.4007 - acc: 0.8588 - val_loss: 0.4964 - val_acc: 0.8370\n",
      "Epoch 101/300\n",
      "60000/60000 [==============================] - 6s 106us/sample - loss: 0.4006 - acc: 0.8592 - val_loss: 0.4789 - val_acc: 0.8434\n",
      "Epoch 102/300\n",
      "60000/60000 [==============================] - 6s 106us/sample - loss: 0.3986 - acc: 0.8583 - val_loss: 0.4777 - val_acc: 0.8380\n",
      "Epoch 103/300\n",
      "60000/60000 [==============================] - 6s 107us/sample - loss: 0.3991 - acc: 0.8592 - val_loss: 0.4867 - val_acc: 0.8385\n",
      "Epoch 104/300\n",
      "60000/60000 [==============================] - 6s 107us/sample - loss: 0.3956 - acc: 0.8598 - val_loss: 0.5251 - val_acc: 0.8374\n",
      "Epoch 105/300\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 0.3993 - acc: 0.8589 - val_loss: 0.4730 - val_acc: 0.8385\n",
      "Epoch 106/300\n",
      "60000/60000 [==============================] - 6s 107us/sample - loss: 0.3980 - acc: 0.8591 - val_loss: 0.4951 - val_acc: 0.8360\n",
      "Epoch 107/300\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 0.4007 - acc: 0.8579 - val_loss: 0.4860 - val_acc: 0.8382\n",
      "Epoch 108/300\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 0.3983 - acc: 0.8600 - val_loss: 0.4831 - val_acc: 0.8413\n",
      "Epoch 109/300\n",
      "60000/60000 [==============================] - 6s 107us/sample - loss: 0.4003 - acc: 0.8580 - val_loss: 0.4665 - val_acc: 0.8387\n",
      "Epoch 110/300\n",
      "60000/60000 [==============================] - 6s 107us/sample - loss: 0.4004 - acc: 0.8570 - val_loss: 0.4909 - val_acc: 0.8393\n",
      "Epoch 111/300\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 0.3975 - acc: 0.8596 - val_loss: 0.4906 - val_acc: 0.8392\n",
      "Epoch 112/300\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 0.3982 - acc: 0.8593 - val_loss: 0.4744 - val_acc: 0.8417\n",
      "Epoch 113/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.4025 - acc: 0.8584 - val_loss: 0.4829 - val_acc: 0.8415\n",
      "Epoch 114/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.3966 - acc: 0.8595 - val_loss: 0.4907 - val_acc: 0.8424\n",
      "Epoch 115/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.3983 - acc: 0.8585 - val_loss: 0.4784 - val_acc: 0.8384\n",
      "Epoch 116/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.3969 - acc: 0.8596 - val_loss: 0.4807 - val_acc: 0.8384\n",
      "Epoch 117/300\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 0.3972 - acc: 0.8603 - val_loss: 0.4701 - val_acc: 0.8388\n",
      "Epoch 118/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.3955 - acc: 0.8592 - val_loss: 0.4782 - val_acc: 0.8402\n",
      "Epoch 119/300\n",
      "60000/60000 [==============================] - 7s 112us/sample - loss: 0.4002 - acc: 0.8586 - val_loss: 0.5009 - val_acc: 0.8388\n",
      "Epoch 120/300\n",
      "60000/60000 [==============================] - 7s 117us/sample - loss: 0.3988 - acc: 0.8574 - val_loss: 0.4949 - val_acc: 0.8351\n",
      "Epoch 121/300\n",
      "60000/60000 [==============================] - 7s 111us/sample - loss: 0.3976 - acc: 0.8589 - val_loss: 0.4922 - val_acc: 0.8374\n",
      "Epoch 122/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.3950 - acc: 0.8600 - val_loss: 0.5049 - val_acc: 0.8338\n",
      "Epoch 123/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.3970 - acc: 0.8585 - val_loss: 0.4685 - val_acc: 0.8399\n",
      "Epoch 124/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.3978 - acc: 0.8593 - val_loss: 0.4791 - val_acc: 0.8399\n",
      "Epoch 125/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.3978 - acc: 0.8606 - val_loss: 0.4776 - val_acc: 0.8406\n",
      "Epoch 126/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.3971 - acc: 0.8582 - val_loss: 0.4937 - val_acc: 0.8386\n",
      "Epoch 127/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.4003 - acc: 0.8584 - val_loss: 0.4801 - val_acc: 0.8397\n",
      "Epoch 128/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.3975 - acc: 0.8590 - val_loss: 0.5004 - val_acc: 0.8380\n",
      "Epoch 129/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.3960 - acc: 0.8589 - val_loss: 0.4933 - val_acc: 0.8393\n",
      "Epoch 130/300\n",
      "60000/60000 [==============================] - 7s 108us/sample - loss: 0.3988 - acc: 0.8588 - val_loss: 0.4830 - val_acc: 0.8366\n",
      "Epoch 131/300\n",
      "60000/60000 [==============================] - 7s 108us/sample - loss: 0.3986 - acc: 0.8587 - val_loss: 0.4965 - val_acc: 0.8416\n",
      "Epoch 132/300\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 0.3985 - acc: 0.8591 - val_loss: 0.4920 - val_acc: 0.8389\n",
      "Epoch 133/300\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 0.3976 - acc: 0.8607 - val_loss: 0.4704 - val_acc: 0.8375\n",
      "Epoch 134/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.3984 - acc: 0.8571 - val_loss: 0.4971 - val_acc: 0.8401\n",
      "Epoch 135/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.3976 - acc: 0.8589 - val_loss: 0.4922 - val_acc: 0.8393\n",
      "Epoch 136/300\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 0.3970 - acc: 0.8595 - val_loss: 0.5139 - val_acc: 0.8404\n",
      "Epoch 137/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.3996 - acc: 0.8580 - val_loss: 0.5173 - val_acc: 0.8384\n",
      "Epoch 138/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.3987 - acc: 0.8598 - val_loss: 0.4708 - val_acc: 0.8398\n",
      "Epoch 139/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.3973 - acc: 0.8594 - val_loss: 0.4929 - val_acc: 0.8399\n",
      "Epoch 140/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.3979 - acc: 0.8581 - val_loss: 0.4863 - val_acc: 0.8372\n",
      "Epoch 141/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.3979 - acc: 0.8575 - val_loss: 0.5049 - val_acc: 0.8384\n",
      "Epoch 142/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.3985 - acc: 0.8597 - val_loss: 0.5093 - val_acc: 0.8390\n",
      "Epoch 143/300\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 0.3962 - acc: 0.8590 - val_loss: 0.4935 - val_acc: 0.8400\n",
      "Epoch 144/300\n",
      "60000/60000 [==============================] - 7s 115us/sample - loss: 0.3990 - acc: 0.8580 - val_loss: 0.4856 - val_acc: 0.8384\n",
      "Epoch 145/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.3978 - acc: 0.8586 - val_loss: 0.4874 - val_acc: 0.8409\n",
      "Epoch 146/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.3952 - acc: 0.8598 - val_loss: 0.4970 - val_acc: 0.8369\n",
      "Epoch 147/300\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 0.3981 - acc: 0.8595 - val_loss: 0.4873 - val_acc: 0.8391\n",
      "Epoch 148/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.3977 - acc: 0.8568 - val_loss: 0.5036 - val_acc: 0.8374\n",
      "Epoch 149/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.3970 - acc: 0.8605 - val_loss: 0.4936 - val_acc: 0.8410\n",
      "Epoch 150/300\n",
      "60000/60000 [==============================] - 7s 108us/sample - loss: 0.3958 - acc: 0.8600 - val_loss: 0.4846 - val_acc: 0.8374\n",
      "Epoch 151/300\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 0.3974 - acc: 0.8587 - val_loss: 0.4814 - val_acc: 0.8404\n",
      "Epoch 152/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.3961 - acc: 0.8598 - val_loss: 0.4842 - val_acc: 0.8389\n",
      "Epoch 153/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.3974 - acc: 0.8595 - val_loss: 0.5166 - val_acc: 0.8386\n",
      "Epoch 154/300\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 0.3958 - acc: 0.8609 - val_loss: 0.4937 - val_acc: 0.8378\n",
      "Epoch 155/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.3955 - acc: 0.8585 - val_loss: 0.5067 - val_acc: 0.8356\n",
      "Epoch 156/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.3962 - acc: 0.8600 - val_loss: 0.4793 - val_acc: 0.8396\n",
      "Epoch 157/300\n",
      "60000/60000 [==============================] - 7s 108us/sample - loss: 0.3984 - acc: 0.8605 - val_loss: 0.4849 - val_acc: 0.8384\n",
      "Epoch 158/300\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 0.3983 - acc: 0.8601 - val_loss: 0.4900 - val_acc: 0.8412\n",
      "Epoch 159/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.3938 - acc: 0.8605 - val_loss: 0.4916 - val_acc: 0.8437\n",
      "Epoch 160/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.3984 - acc: 0.8586 - val_loss: 0.4832 - val_acc: 0.8379\n",
      "Epoch 161/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.3977 - acc: 0.8582 - val_loss: 0.4872 - val_acc: 0.8408\n",
      "Epoch 162/300\n",
      "60000/60000 [==============================] - 7s 108us/sample - loss: 0.3966 - acc: 0.8591 - val_loss: 0.5169 - val_acc: 0.8384\n",
      "Epoch 163/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.3971 - acc: 0.8599 - val_loss: 0.4973 - val_acc: 0.8399\n",
      "Epoch 164/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.3961 - acc: 0.8585 - val_loss: 0.4774 - val_acc: 0.8401\n",
      "Epoch 165/300\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 0.3980 - acc: 0.8580 - val_loss: 0.4839 - val_acc: 0.8397\n",
      "Epoch 166/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.3970 - acc: 0.8586 - val_loss: 0.4700 - val_acc: 0.8448\n",
      "Epoch 167/300\n",
      "60000/60000 [==============================] - 7s 117us/sample - loss: 0.3957 - acc: 0.8591 - val_loss: 0.4870 - val_acc: 0.8420\n",
      "Epoch 168/300\n",
      "60000/60000 [==============================] - 7s 113us/sample - loss: 0.3963 - acc: 0.8590 - val_loss: 0.5299 - val_acc: 0.8390\n",
      "Epoch 169/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.3957 - acc: 0.8591 - val_loss: 0.4992 - val_acc: 0.8401\n",
      "Epoch 170/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.3942 - acc: 0.8608 - val_loss: 0.4871 - val_acc: 0.8376\n",
      "Epoch 171/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.3956 - acc: 0.8597 - val_loss: 0.4880 - val_acc: 0.8366\n",
      "Epoch 172/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.3946 - acc: 0.8602 - val_loss: 0.4931 - val_acc: 0.8338\n",
      "Epoch 173/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.3984 - acc: 0.8607 - val_loss: 0.5531 - val_acc: 0.8369\n",
      "Epoch 174/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.3939 - acc: 0.8602 - val_loss: 0.4722 - val_acc: 0.8409\n",
      "Epoch 175/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.3941 - acc: 0.8599 - val_loss: 0.4975 - val_acc: 0.8391\n",
      "Epoch 176/300\n",
      "60000/60000 [==============================] - 7s 111us/sample - loss: 0.3937 - acc: 0.8598 - val_loss: 0.5015 - val_acc: 0.8380\n",
      "Epoch 177/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.3963 - acc: 0.8590 - val_loss: 0.4982 - val_acc: 0.8416\n",
      "Epoch 178/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.3968 - acc: 0.8593 - val_loss: 0.4889 - val_acc: 0.8425\n",
      "Epoch 179/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.3950 - acc: 0.8604 - val_loss: 0.4946 - val_acc: 0.8370\n",
      "Epoch 180/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.3958 - acc: 0.8595 - val_loss: 0.4900 - val_acc: 0.8399\n",
      "Epoch 181/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.3958 - acc: 0.8589 - val_loss: 0.5265 - val_acc: 0.8390\n",
      "Epoch 182/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.3966 - acc: 0.8573 - val_loss: 0.5027 - val_acc: 0.8381\n",
      "Epoch 183/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.3973 - acc: 0.8585 - val_loss: 0.4842 - val_acc: 0.8384\n",
      "Epoch 184/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.3957 - acc: 0.8595 - val_loss: 0.4802 - val_acc: 0.8418\n",
      "Epoch 185/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.3948 - acc: 0.8594 - val_loss: 0.5187 - val_acc: 0.8383\n",
      "Epoch 186/300\n",
      "60000/60000 [==============================] - 7s 108us/sample - loss: 0.3959 - acc: 0.8602 - val_loss: 0.4909 - val_acc: 0.8390\n",
      "Epoch 187/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.3945 - acc: 0.8600 - val_loss: 0.4738 - val_acc: 0.8372\n",
      "Epoch 188/300\n",
      "60000/60000 [==============================] - 7s 111us/sample - loss: 0.3965 - acc: 0.8594 - val_loss: 0.4905 - val_acc: 0.8357\n",
      "Epoch 189/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.3961 - acc: 0.8588 - val_loss: 0.4828 - val_acc: 0.8372\n",
      "Epoch 190/300\n",
      "60000/60000 [==============================] - 7s 114us/sample - loss: 0.3936 - acc: 0.8601 - val_loss: 0.4955 - val_acc: 0.8401\n",
      "Epoch 191/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.3966 - acc: 0.8587 - val_loss: 0.5059 - val_acc: 0.8372\n",
      "Epoch 192/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.3951 - acc: 0.8600 - val_loss: 0.5039 - val_acc: 0.8396\n",
      "Epoch 193/300\n",
      "60000/60000 [==============================] - 7s 108us/sample - loss: 0.3952 - acc: 0.8580 - val_loss: 0.4784 - val_acc: 0.8408\n",
      "Epoch 194/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.3946 - acc: 0.8587 - val_loss: 0.4916 - val_acc: 0.8389\n",
      "Epoch 195/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.3951 - acc: 0.8590 - val_loss: 0.5298 - val_acc: 0.8326\n",
      "Epoch 196/300\n",
      "60000/60000 [==============================] - 7s 111us/sample - loss: 0.3974 - acc: 0.8593 - val_loss: 0.4824 - val_acc: 0.8405\n",
      "Epoch 197/300\n",
      "60000/60000 [==============================] - 7s 111us/sample - loss: 0.3942 - acc: 0.8595 - val_loss: 0.4841 - val_acc: 0.8365\n",
      "Epoch 198/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.3973 - acc: 0.8588 - val_loss: 0.5042 - val_acc: 0.8400\n",
      "Epoch 199/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.3953 - acc: 0.8578 - val_loss: 0.5146 - val_acc: 0.8361\n",
      "Epoch 200/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.3963 - acc: 0.8600 - val_loss: 0.4837 - val_acc: 0.8339\n",
      "Epoch 201/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.3939 - acc: 0.8606 - val_loss: 0.4955 - val_acc: 0.8381\n",
      "Epoch 202/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.3956 - acc: 0.8589 - val_loss: 0.5078 - val_acc: 0.8413\n",
      "Epoch 203/300\n",
      "60000/60000 [==============================] - 7s 111us/sample - loss: 0.3951 - acc: 0.8601 - val_loss: 0.4860 - val_acc: 0.8374\n",
      "Epoch 204/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.3941 - acc: 0.8611 - val_loss: 0.4838 - val_acc: 0.8387\n",
      "Epoch 205/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.3938 - acc: 0.8605 - val_loss: 0.4891 - val_acc: 0.8400\n",
      "Epoch 206/300\n",
      "60000/60000 [==============================] - 7s 111us/sample - loss: 0.3951 - acc: 0.8611 - val_loss: 0.4891 - val_acc: 0.8377\n",
      "Epoch 207/300\n",
      "60000/60000 [==============================] - 7s 111us/sample - loss: 0.3954 - acc: 0.8595 - val_loss: 0.5072 - val_acc: 0.8364\n",
      "Epoch 208/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.3940 - acc: 0.8594 - val_loss: 0.5252 - val_acc: 0.8373\n",
      "Epoch 209/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.3968 - acc: 0.8595 - val_loss: 0.4965 - val_acc: 0.8355\n",
      "Epoch 210/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.3943 - acc: 0.8594 - val_loss: 0.5070 - val_acc: 0.8362\n",
      "Epoch 211/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.3956 - acc: 0.8581 - val_loss: 0.5012 - val_acc: 0.8383\n",
      "Epoch 212/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.3946 - acc: 0.8610 - val_loss: 0.4887 - val_acc: 0.8408\n",
      "Epoch 213/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.3947 - acc: 0.8594 - val_loss: 0.4842 - val_acc: 0.8406\n",
      "Epoch 214/300\n",
      "60000/60000 [==============================] - 7s 118us/sample - loss: 0.3937 - acc: 0.8600 - val_loss: 0.4837 - val_acc: 0.8365\n",
      "Epoch 215/300\n",
      "60000/60000 [==============================] - 7s 114us/sample - loss: 0.3954 - acc: 0.8595 - val_loss: 0.4908 - val_acc: 0.8379\n",
      "Epoch 216/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.3932 - acc: 0.8598 - val_loss: 0.5067 - val_acc: 0.8388\n",
      "Epoch 217/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.3945 - acc: 0.8596 - val_loss: 0.4970 - val_acc: 0.8364\n",
      "Epoch 218/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.3950 - acc: 0.8597 - val_loss: 0.4971 - val_acc: 0.8413\n",
      "Epoch 219/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.3927 - acc: 0.8602 - val_loss: 0.4984 - val_acc: 0.8393\n",
      "Epoch 220/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.3972 - acc: 0.8591 - val_loss: 0.5083 - val_acc: 0.8427\n",
      "Epoch 221/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.3928 - acc: 0.8607 - val_loss: 0.5586 - val_acc: 0.8382\n",
      "Epoch 222/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.3945 - acc: 0.8591 - val_loss: 0.4959 - val_acc: 0.8421\n",
      "Epoch 223/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.3923 - acc: 0.8604 - val_loss: 0.4863 - val_acc: 0.8386\n",
      "Epoch 224/300\n",
      "60000/60000 [==============================] - 7s 111us/sample - loss: 0.3961 - acc: 0.8589 - val_loss: 0.4868 - val_acc: 0.8352\n",
      "Epoch 225/300\n",
      "60000/60000 [==============================] - 7s 111us/sample - loss: 0.3933 - acc: 0.8597 - val_loss: 0.4943 - val_acc: 0.8366\n",
      "Epoch 226/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.3954 - acc: 0.8600 - val_loss: 0.4874 - val_acc: 0.8392\n",
      "Epoch 227/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.3930 - acc: 0.8609 - val_loss: 0.4987 - val_acc: 0.8401\n",
      "Epoch 228/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.3931 - acc: 0.8598 - val_loss: 0.5028 - val_acc: 0.8355\n",
      "Epoch 229/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.3952 - acc: 0.8593 - val_loss: 0.5046 - val_acc: 0.8419\n",
      "Epoch 230/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.3938 - acc: 0.8598 - val_loss: 0.5004 - val_acc: 0.8359\n",
      "Epoch 231/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.3927 - acc: 0.8608 - val_loss: 0.4835 - val_acc: 0.8392\n",
      "Epoch 232/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.3927 - acc: 0.8600 - val_loss: 0.5007 - val_acc: 0.8392\n",
      "Epoch 233/300\n",
      "60000/60000 [==============================] - 7s 111us/sample - loss: 0.3927 - acc: 0.8608 - val_loss: 0.5138 - val_acc: 0.8402\n",
      "Epoch 234/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.3986 - acc: 0.8579 - val_loss: 0.5175 - val_acc: 0.8387\n",
      "Epoch 235/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.3943 - acc: 0.8600 - val_loss: 0.4972 - val_acc: 0.8369\n",
      "Epoch 236/300\n",
      "60000/60000 [==============================] - 7s 112us/sample - loss: 0.3943 - acc: 0.8592 - val_loss: 0.5191 - val_acc: 0.8349\n",
      "Epoch 237/300\n",
      "60000/60000 [==============================] - 7s 113us/sample - loss: 0.3942 - acc: 0.8594 - val_loss: 0.4924 - val_acc: 0.8391\n",
      "Epoch 238/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.3953 - acc: 0.8589 - val_loss: 0.5489 - val_acc: 0.8377\n",
      "Epoch 239/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.3953 - acc: 0.8607 - val_loss: 0.4916 - val_acc: 0.8390\n",
      "Epoch 240/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.3949 - acc: 0.8599 - val_loss: 0.4859 - val_acc: 0.8401\n",
      "Epoch 241/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.3925 - acc: 0.8603 - val_loss: 0.5065 - val_acc: 0.8381\n",
      "Epoch 242/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.3955 - acc: 0.8595 - val_loss: 0.5523 - val_acc: 0.8360\n",
      "Epoch 243/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.3952 - acc: 0.8590 - val_loss: 0.4847 - val_acc: 0.8389\n",
      "Epoch 244/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.3945 - acc: 0.8585 - val_loss: 0.5223 - val_acc: 0.8372\n",
      "Epoch 245/300\n",
      "60000/60000 [==============================] - 7s 112us/sample - loss: 0.3930 - acc: 0.8607 - val_loss: 0.4824 - val_acc: 0.8367\n",
      "Epoch 246/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.3941 - acc: 0.8598 - val_loss: 0.4904 - val_acc: 0.8409\n",
      "Epoch 247/300\n",
      "60000/60000 [==============================] - 7s 108us/sample - loss: 0.3936 - acc: 0.8604 - val_loss: 0.5163 - val_acc: 0.8397\n",
      "Epoch 248/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.3940 - acc: 0.8594 - val_loss: 0.5232 - val_acc: 0.8386\n",
      "Epoch 249/300\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 0.3946 - acc: 0.8598 - val_loss: 0.5183 - val_acc: 0.8400\n",
      "Epoch 250/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.3964 - acc: 0.8591 - val_loss: 0.4810 - val_acc: 0.8401\n",
      "Epoch 251/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.3925 - acc: 0.8615 - val_loss: 0.4829 - val_acc: 0.8395\n",
      "Epoch 252/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.3950 - acc: 0.8600 - val_loss: 0.5420 - val_acc: 0.8375\n",
      "Epoch 253/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.3955 - acc: 0.8601 - val_loss: 0.4891 - val_acc: 0.8397\n",
      "Epoch 254/300\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 0.3919 - acc: 0.8606 - val_loss: 0.4873 - val_acc: 0.8399\n",
      "Epoch 255/300\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 0.3939 - acc: 0.8604 - val_loss: 0.4883 - val_acc: 0.8412\n",
      "Epoch 256/300\n",
      "60000/60000 [==============================] - 7s 108us/sample - loss: 0.3941 - acc: 0.8595 - val_loss: 0.4978 - val_acc: 0.8410\n",
      "Epoch 257/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.3934 - acc: 0.8593 - val_loss: 0.4928 - val_acc: 0.8418\n",
      "Epoch 258/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.3946 - acc: 0.8597 - val_loss: 0.5302 - val_acc: 0.8380\n",
      "Epoch 259/300\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 0.3930 - acc: 0.8608 - val_loss: 0.4922 - val_acc: 0.8368\n",
      "Epoch 260/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.3925 - acc: 0.8600 - val_loss: 0.5073 - val_acc: 0.8413\n",
      "Epoch 261/300\n",
      "60000/60000 [==============================] - 7s 114us/sample - loss: 0.3942 - acc: 0.8607 - val_loss: 0.5317 - val_acc: 0.8409\n",
      "Epoch 262/300\n",
      "60000/60000 [==============================] - 7s 113us/sample - loss: 0.3901 - acc: 0.8605 - val_loss: 0.5040 - val_acc: 0.8377\n",
      "Epoch 263/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.3945 - acc: 0.8590 - val_loss: 0.5162 - val_acc: 0.8411\n",
      "Epoch 264/300\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 0.3945 - acc: 0.8598 - val_loss: 0.4741 - val_acc: 0.8405\n",
      "Epoch 265/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.3939 - acc: 0.8596 - val_loss: 0.5443 - val_acc: 0.8348\n",
      "Epoch 266/300\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 0.3932 - acc: 0.8595 - val_loss: 0.5088 - val_acc: 0.8398\n",
      "Epoch 267/300\n",
      "60000/60000 [==============================] - 6s 107us/sample - loss: 0.3930 - acc: 0.8604 - val_loss: 0.4919 - val_acc: 0.8426\n",
      "Epoch 268/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.3934 - acc: 0.8608 - val_loss: 0.5071 - val_acc: 0.8426\n",
      "Epoch 269/300\n",
      "60000/60000 [==============================] - 7s 111us/sample - loss: 0.3899 - acc: 0.8615 - val_loss: 0.5742 - val_acc: 0.8377\n",
      "Epoch 270/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.3919 - acc: 0.8610 - val_loss: 0.4855 - val_acc: 0.8377\n",
      "Epoch 271/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.3922 - acc: 0.8594 - val_loss: 0.5122 - val_acc: 0.8388\n",
      "Epoch 272/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.3934 - acc: 0.8594 - val_loss: 0.5026 - val_acc: 0.8396\n",
      "Epoch 273/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.3932 - acc: 0.8594 - val_loss: 0.5042 - val_acc: 0.8383\n",
      "Epoch 274/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.3954 - acc: 0.8587 - val_loss: 0.4841 - val_acc: 0.8412\n",
      "Epoch 275/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.3937 - acc: 0.8593 - val_loss: 0.4865 - val_acc: 0.8376\n",
      "Epoch 276/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.3936 - acc: 0.8605 - val_loss: 0.4939 - val_acc: 0.8372\n",
      "Epoch 277/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.3940 - acc: 0.8595 - val_loss: 0.5228 - val_acc: 0.8389\n",
      "Epoch 278/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.3940 - acc: 0.8603 - val_loss: 0.4991 - val_acc: 0.8365\n",
      "Epoch 279/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.3953 - acc: 0.8598 - val_loss: 0.4935 - val_acc: 0.8371\n",
      "Epoch 280/300\n",
      "60000/60000 [==============================] - 7s 111us/sample - loss: 0.3920 - acc: 0.8606 - val_loss: 0.5441 - val_acc: 0.8365\n",
      "Epoch 281/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.3955 - acc: 0.8590 - val_loss: 0.5248 - val_acc: 0.8409\n",
      "Epoch 282/300\n",
      "60000/60000 [==============================] - 7s 111us/sample - loss: 0.3920 - acc: 0.8604 - val_loss: 0.5021 - val_acc: 0.8393\n",
      "Epoch 283/300\n",
      "60000/60000 [==============================] - 7s 115us/sample - loss: 0.3925 - acc: 0.8598 - val_loss: 0.4895 - val_acc: 0.8404\n",
      "Epoch 284/300\n",
      "60000/60000 [==============================] - 7s 111us/sample - loss: 0.3921 - acc: 0.8607 - val_loss: 0.5011 - val_acc: 0.8387\n",
      "Epoch 285/300\n",
      "60000/60000 [==============================] - 7s 113us/sample - loss: 0.3931 - acc: 0.8605 - val_loss: 0.5044 - val_acc: 0.8408\n",
      "Epoch 286/300\n",
      "60000/60000 [==============================] - 7s 113us/sample - loss: 0.3928 - acc: 0.8608 - val_loss: 0.5035 - val_acc: 0.8387\n",
      "Epoch 287/300\n",
      "60000/60000 [==============================] - 7s 113us/sample - loss: 0.3952 - acc: 0.8586 - val_loss: 0.5037 - val_acc: 0.8369\n",
      "Epoch 288/300\n",
      "60000/60000 [==============================] - 7s 113us/sample - loss: 0.3948 - acc: 0.8598 - val_loss: 0.5254 - val_acc: 0.8366\n",
      "Epoch 289/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.3916 - acc: 0.8611 - val_loss: 0.4979 - val_acc: 0.8352\n",
      "Epoch 290/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.3945 - acc: 0.8589 - val_loss: 0.5470 - val_acc: 0.8401\n",
      "Epoch 291/300\n",
      "60000/60000 [==============================] - 7s 111us/sample - loss: 0.3931 - acc: 0.8609 - val_loss: 0.5257 - val_acc: 0.8383\n",
      "Epoch 292/300\n",
      "60000/60000 [==============================] - 7s 114us/sample - loss: 0.3910 - acc: 0.8608 - val_loss: 0.5027 - val_acc: 0.8360\n",
      "Epoch 293/300\n",
      "60000/60000 [==============================] - 7s 113us/sample - loss: 0.3939 - acc: 0.8598 - val_loss: 0.4924 - val_acc: 0.8384\n",
      "Epoch 294/300\n",
      "60000/60000 [==============================] - 7s 113us/sample - loss: 0.3933 - acc: 0.8613 - val_loss: 0.4955 - val_acc: 0.8403\n",
      "Epoch 295/300\n",
      "60000/60000 [==============================] - 7s 114us/sample - loss: 0.3931 - acc: 0.8601 - val_loss: 0.5020 - val_acc: 0.8372\n",
      "Epoch 296/300\n",
      "60000/60000 [==============================] - 7s 112us/sample - loss: 0.3919 - acc: 0.8606 - val_loss: 0.4929 - val_acc: 0.8371\n",
      "Epoch 297/300\n",
      "60000/60000 [==============================] - 7s 112us/sample - loss: 0.3946 - acc: 0.8594 - val_loss: 0.4925 - val_acc: 0.8376\n",
      "Epoch 298/300\n",
      "60000/60000 [==============================] - 7s 111us/sample - loss: 0.3946 - acc: 0.8606 - val_loss: 0.4858 - val_acc: 0.8376\n",
      "Epoch 299/300\n",
      "60000/60000 [==============================] - 7s 113us/sample - loss: 0.3935 - acc: 0.8589 - val_loss: 0.5163 - val_acc: 0.8365\n",
      "Epoch 300/300\n",
      "60000/60000 [==============================] - 7s 112us/sample - loss: 0.3933 - acc: 0.8598 - val_loss: 0.5052 - val_acc: 0.8395\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f4570511d90>"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trainX, trainY, validation_data=(testX, testY), epochs=300, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Py-KwkmjOIVU"
   },
   "source": [
    "### Customize the learning rate to 0.001 in sgd optimizer and run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2d3yJ1eCx1Do"
   },
   "outputs": [],
   "source": [
    "#Initialize Sequential model\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "#Reshape data from 2D to 1D -> 28x28 to 784\n",
    "model.add(tf.keras.layers.Reshape((784,),input_shape=(28,28,)))\n",
    "\n",
    "#Normalize the data\n",
    "model.add(tf.keras.layers.BatchNormalization(input_shape=(4,)))\n",
    "\n",
    "#Add Dense Layer which provides 10 Outputs after applying softmax\n",
    "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "\n",
    "sgd_optimizer = tf.keras.optimizers.SGD(lr=0.001)\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'],optimizer=sgd_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4548750,
     "status": "ok",
     "timestamp": 1569738877205,
     "user": {
      "displayName": "Raghav Praneeth Amaravadi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAMhpxsJE7d0g9jOsWi8V_E3rjg4Ut9Yz-RLZWsQw=s64",
      "userId": "05628056859539857657"
     },
     "user_tz": -330
    },
    "id": "pJUqA5T4OIVc",
    "outputId": "23f7827b-76a1-4c22-9881-97eb0079e425"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/300\n",
      "60000/60000 [==============================] - 7s 115us/sample - loss: 0.9425 - acc: 0.6785 - val_loss: 0.6967 - val_acc: 0.7647\n",
      "Epoch 2/300\n",
      "60000/60000 [==============================] - 7s 112us/sample - loss: 0.6492 - acc: 0.7787 - val_loss: 0.6214 - val_acc: 0.7907\n",
      "Epoch 3/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.5929 - acc: 0.7966 - val_loss: 0.5859 - val_acc: 0.8020\n",
      "Epoch 4/300\n",
      "60000/60000 [==============================] - 7s 111us/sample - loss: 0.5617 - acc: 0.8063 - val_loss: 0.5604 - val_acc: 0.8091\n",
      "Epoch 5/300\n",
      "60000/60000 [==============================] - 7s 113us/sample - loss: 0.5416 - acc: 0.8141 - val_loss: 0.5470 - val_acc: 0.8122\n",
      "Epoch 6/300\n",
      "60000/60000 [==============================] - 7s 115us/sample - loss: 0.5280 - acc: 0.8195 - val_loss: 0.5363 - val_acc: 0.8153\n",
      "Epoch 7/300\n",
      "60000/60000 [==============================] - 7s 118us/sample - loss: 0.5193 - acc: 0.8218 - val_loss: 0.5236 - val_acc: 0.8183\n",
      "Epoch 8/300\n",
      "60000/60000 [==============================] - 7s 121us/sample - loss: 0.5083 - acc: 0.8252 - val_loss: 0.5196 - val_acc: 0.8226\n",
      "Epoch 9/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.5020 - acc: 0.8275 - val_loss: 0.5207 - val_acc: 0.8223\n",
      "Epoch 10/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.4950 - acc: 0.8299 - val_loss: 0.5064 - val_acc: 0.8240\n",
      "Epoch 11/300\n",
      "60000/60000 [==============================] - 7s 113us/sample - loss: 0.4918 - acc: 0.8311 - val_loss: 0.5057 - val_acc: 0.8259\n",
      "Epoch 12/300\n",
      "60000/60000 [==============================] - 7s 112us/sample - loss: 0.4860 - acc: 0.8327 - val_loss: 0.5011 - val_acc: 0.8258\n",
      "Epoch 13/300\n",
      "60000/60000 [==============================] - 7s 112us/sample - loss: 0.4826 - acc: 0.8343 - val_loss: 0.4953 - val_acc: 0.8279\n",
      "Epoch 14/300\n",
      "60000/60000 [==============================] - 7s 113us/sample - loss: 0.4763 - acc: 0.8363 - val_loss: 0.4930 - val_acc: 0.8283\n",
      "Epoch 15/300\n",
      "60000/60000 [==============================] - 7s 112us/sample - loss: 0.4760 - acc: 0.8385 - val_loss: 0.4960 - val_acc: 0.8274\n",
      "Epoch 16/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.4727 - acc: 0.8370 - val_loss: 0.4947 - val_acc: 0.8291\n",
      "Epoch 17/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.4696 - acc: 0.8375 - val_loss: 0.4947 - val_acc: 0.8316\n",
      "Epoch 18/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.4681 - acc: 0.8394 - val_loss: 0.4850 - val_acc: 0.8305\n",
      "Epoch 19/300\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 0.4675 - acc: 0.8389 - val_loss: 0.4902 - val_acc: 0.8314\n",
      "Epoch 20/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.4638 - acc: 0.8412 - val_loss: 0.4823 - val_acc: 0.8327\n",
      "Epoch 21/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.4627 - acc: 0.8407 - val_loss: 0.4804 - val_acc: 0.8333\n",
      "Epoch 22/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.4604 - acc: 0.8404 - val_loss: 0.4892 - val_acc: 0.8341\n",
      "Epoch 23/300\n",
      "60000/60000 [==============================] - 7s 114us/sample - loss: 0.4580 - acc: 0.8422 - val_loss: 0.4780 - val_acc: 0.8341\n",
      "Epoch 24/300\n",
      "60000/60000 [==============================] - 7s 112us/sample - loss: 0.4574 - acc: 0.8442 - val_loss: 0.4785 - val_acc: 0.8345\n",
      "Epoch 25/300\n",
      "60000/60000 [==============================] - 7s 113us/sample - loss: 0.4564 - acc: 0.8428 - val_loss: 0.4763 - val_acc: 0.8354\n",
      "Epoch 26/300\n",
      "60000/60000 [==============================] - 7s 113us/sample - loss: 0.4546 - acc: 0.8437 - val_loss: 0.4774 - val_acc: 0.8350\n",
      "Epoch 27/300\n",
      "60000/60000 [==============================] - 7s 112us/sample - loss: 0.4531 - acc: 0.8443 - val_loss: 0.4774 - val_acc: 0.8355\n",
      "Epoch 28/300\n",
      "60000/60000 [==============================] - 7s 116us/sample - loss: 0.4507 - acc: 0.8454 - val_loss: 0.4746 - val_acc: 0.8359\n",
      "Epoch 29/300\n",
      "60000/60000 [==============================] - 7s 111us/sample - loss: 0.4469 - acc: 0.8471 - val_loss: 0.4780 - val_acc: 0.8356\n",
      "Epoch 30/300\n",
      "60000/60000 [==============================] - 7s 112us/sample - loss: 0.4492 - acc: 0.8447 - val_loss: 0.4749 - val_acc: 0.8359\n",
      "Epoch 31/300\n",
      "60000/60000 [==============================] - 7s 111us/sample - loss: 0.4463 - acc: 0.8440 - val_loss: 0.4811 - val_acc: 0.8364\n",
      "Epoch 32/300\n",
      "60000/60000 [==============================] - 7s 112us/sample - loss: 0.4452 - acc: 0.8470 - val_loss: 0.4709 - val_acc: 0.8375\n",
      "Epoch 33/300\n",
      "60000/60000 [==============================] - 7s 112us/sample - loss: 0.4430 - acc: 0.8463 - val_loss: 0.4720 - val_acc: 0.8384\n",
      "Epoch 34/300\n",
      "60000/60000 [==============================] - 7s 113us/sample - loss: 0.4432 - acc: 0.8482 - val_loss: 0.4717 - val_acc: 0.8379\n",
      "Epoch 35/300\n",
      "60000/60000 [==============================] - 7s 111us/sample - loss: 0.4441 - acc: 0.8479 - val_loss: 0.4720 - val_acc: 0.8378\n",
      "Epoch 36/300\n",
      "60000/60000 [==============================] - 7s 112us/sample - loss: 0.4435 - acc: 0.8465 - val_loss: 0.4694 - val_acc: 0.8387\n",
      "Epoch 37/300\n",
      "60000/60000 [==============================] - 7s 112us/sample - loss: 0.4408 - acc: 0.8487 - val_loss: 0.4662 - val_acc: 0.8374\n",
      "Epoch 38/300\n",
      "60000/60000 [==============================] - 7s 113us/sample - loss: 0.4398 - acc: 0.8489 - val_loss: 0.4790 - val_acc: 0.8374\n",
      "Epoch 39/300\n",
      "60000/60000 [==============================] - 7s 112us/sample - loss: 0.4407 - acc: 0.8474 - val_loss: 0.4724 - val_acc: 0.8382\n",
      "Epoch 40/300\n",
      "60000/60000 [==============================] - 7s 112us/sample - loss: 0.4379 - acc: 0.8496 - val_loss: 0.4704 - val_acc: 0.8377\n",
      "Epoch 41/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.4374 - acc: 0.8496 - val_loss: 0.4651 - val_acc: 0.8374\n",
      "Epoch 42/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.4398 - acc: 0.8495 - val_loss: 0.4635 - val_acc: 0.8391\n",
      "Epoch 43/300\n",
      "60000/60000 [==============================] - 7s 112us/sample - loss: 0.4358 - acc: 0.8506 - val_loss: 0.4625 - val_acc: 0.8381\n",
      "Epoch 44/300\n",
      "60000/60000 [==============================] - 7s 113us/sample - loss: 0.4368 - acc: 0.8495 - val_loss: 0.4708 - val_acc: 0.8386\n",
      "Epoch 45/300\n",
      "60000/60000 [==============================] - 7s 113us/sample - loss: 0.4359 - acc: 0.8484 - val_loss: 0.4618 - val_acc: 0.8382\n",
      "Epoch 46/300\n",
      "60000/60000 [==============================] - 7s 112us/sample - loss: 0.4323 - acc: 0.8502 - val_loss: 0.4641 - val_acc: 0.8395\n",
      "Epoch 47/300\n",
      "60000/60000 [==============================] - 7s 113us/sample - loss: 0.4335 - acc: 0.8499 - val_loss: 0.4681 - val_acc: 0.8387\n",
      "Epoch 48/300\n",
      "60000/60000 [==============================] - 7s 112us/sample - loss: 0.4353 - acc: 0.8495 - val_loss: 0.4619 - val_acc: 0.8389\n",
      "Epoch 49/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.4324 - acc: 0.8513 - val_loss: 0.4640 - val_acc: 0.8391\n",
      "Epoch 50/300\n",
      "60000/60000 [==============================] - 7s 112us/sample - loss: 0.4347 - acc: 0.8500 - val_loss: 0.4694 - val_acc: 0.8387\n",
      "Epoch 51/300\n",
      "60000/60000 [==============================] - 7s 112us/sample - loss: 0.4322 - acc: 0.8515 - val_loss: 0.4661 - val_acc: 0.8397\n",
      "Epoch 52/300\n",
      "60000/60000 [==============================] - 7s 111us/sample - loss: 0.4324 - acc: 0.8518 - val_loss: 0.4624 - val_acc: 0.8406\n",
      "Epoch 53/300\n",
      "60000/60000 [==============================] - 7s 115us/sample - loss: 0.4298 - acc: 0.8519 - val_loss: 0.4643 - val_acc: 0.8390\n",
      "Epoch 54/300\n",
      "60000/60000 [==============================] - 7s 120us/sample - loss: 0.4321 - acc: 0.8501 - val_loss: 0.4598 - val_acc: 0.8400\n",
      "Epoch 55/300\n",
      "60000/60000 [==============================] - 7s 114us/sample - loss: 0.4295 - acc: 0.8508 - val_loss: 0.4701 - val_acc: 0.8394\n",
      "Epoch 56/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.4302 - acc: 0.8515 - val_loss: 0.4641 - val_acc: 0.8406\n",
      "Epoch 57/300\n",
      "60000/60000 [==============================] - 7s 112us/sample - loss: 0.4288 - acc: 0.8529 - val_loss: 0.4667 - val_acc: 0.8398\n",
      "Epoch 58/300\n",
      "60000/60000 [==============================] - 7s 113us/sample - loss: 0.4297 - acc: 0.8517 - val_loss: 0.4592 - val_acc: 0.8400\n",
      "Epoch 59/300\n",
      "60000/60000 [==============================] - 7s 113us/sample - loss: 0.4286 - acc: 0.8521 - val_loss: 0.4607 - val_acc: 0.8400\n",
      "Epoch 60/300\n",
      "60000/60000 [==============================] - 7s 111us/sample - loss: 0.4277 - acc: 0.8519 - val_loss: 0.4580 - val_acc: 0.8401\n",
      "Epoch 61/300\n",
      "60000/60000 [==============================] - 7s 111us/sample - loss: 0.4245 - acc: 0.8533 - val_loss: 0.4586 - val_acc: 0.8401\n",
      "Epoch 62/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.4264 - acc: 0.8514 - val_loss: 0.4636 - val_acc: 0.8395\n",
      "Epoch 63/300\n",
      "60000/60000 [==============================] - 7s 111us/sample - loss: 0.4260 - acc: 0.8529 - val_loss: 0.4586 - val_acc: 0.8409\n",
      "Epoch 64/300\n",
      "60000/60000 [==============================] - 7s 113us/sample - loss: 0.4261 - acc: 0.8530 - val_loss: 0.4616 - val_acc: 0.8406\n",
      "Epoch 65/300\n",
      "60000/60000 [==============================] - 7s 112us/sample - loss: 0.4244 - acc: 0.8539 - val_loss: 0.4661 - val_acc: 0.8395\n",
      "Epoch 66/300\n",
      "60000/60000 [==============================] - 7s 113us/sample - loss: 0.4258 - acc: 0.8536 - val_loss: 0.4571 - val_acc: 0.8428\n",
      "Epoch 67/300\n",
      "60000/60000 [==============================] - 7s 112us/sample - loss: 0.4226 - acc: 0.8548 - val_loss: 0.4777 - val_acc: 0.8401\n",
      "Epoch 68/300\n",
      "60000/60000 [==============================] - 7s 113us/sample - loss: 0.4237 - acc: 0.8525 - val_loss: 0.4540 - val_acc: 0.8403\n",
      "Epoch 69/300\n",
      "60000/60000 [==============================] - 7s 111us/sample - loss: 0.4247 - acc: 0.8530 - val_loss: 0.4609 - val_acc: 0.8400\n",
      "Epoch 70/300\n",
      "60000/60000 [==============================] - 7s 113us/sample - loss: 0.4236 - acc: 0.8529 - val_loss: 0.4717 - val_acc: 0.8399\n",
      "Epoch 71/300\n",
      "60000/60000 [==============================] - 7s 114us/sample - loss: 0.4251 - acc: 0.8537 - val_loss: 0.4630 - val_acc: 0.8409\n",
      "Epoch 72/300\n",
      "60000/60000 [==============================] - 7s 113us/sample - loss: 0.4237 - acc: 0.8526 - val_loss: 0.4582 - val_acc: 0.8408\n",
      "Epoch 73/300\n",
      "60000/60000 [==============================] - 7s 117us/sample - loss: 0.4248 - acc: 0.8530 - val_loss: 0.4594 - val_acc: 0.8409\n",
      "Epoch 74/300\n",
      "60000/60000 [==============================] - 7s 116us/sample - loss: 0.4221 - acc: 0.8536 - val_loss: 0.4600 - val_acc: 0.8410\n",
      "Epoch 75/300\n",
      "60000/60000 [==============================] - 7s 111us/sample - loss: 0.4235 - acc: 0.8532 - val_loss: 0.4542 - val_acc: 0.8419\n",
      "Epoch 76/300\n",
      "60000/60000 [==============================] - 7s 111us/sample - loss: 0.4215 - acc: 0.8542 - val_loss: 0.4662 - val_acc: 0.8418\n",
      "Epoch 77/300\n",
      "60000/60000 [==============================] - 7s 112us/sample - loss: 0.4211 - acc: 0.8546 - val_loss: 0.4566 - val_acc: 0.8422\n",
      "Epoch 78/300\n",
      "60000/60000 [==============================] - 7s 113us/sample - loss: 0.4209 - acc: 0.8545 - val_loss: 0.4536 - val_acc: 0.8413\n",
      "Epoch 79/300\n",
      "60000/60000 [==============================] - 7s 113us/sample - loss: 0.4208 - acc: 0.8555 - val_loss: 0.4607 - val_acc: 0.8415\n",
      "Epoch 80/300\n",
      "60000/60000 [==============================] - 7s 114us/sample - loss: 0.4239 - acc: 0.8538 - val_loss: 0.4610 - val_acc: 0.8419\n",
      "Epoch 81/300\n",
      "60000/60000 [==============================] - 7s 112us/sample - loss: 0.4222 - acc: 0.8551 - val_loss: 0.4570 - val_acc: 0.8415\n",
      "Epoch 82/300\n",
      "60000/60000 [==============================] - 7s 111us/sample - loss: 0.4201 - acc: 0.8546 - val_loss: 0.4601 - val_acc: 0.8410\n",
      "Epoch 83/300\n",
      "60000/60000 [==============================] - 7s 112us/sample - loss: 0.4206 - acc: 0.8543 - val_loss: 0.4659 - val_acc: 0.8412\n",
      "Epoch 84/300\n",
      "60000/60000 [==============================] - 7s 112us/sample - loss: 0.4197 - acc: 0.8548 - val_loss: 0.4564 - val_acc: 0.8420\n",
      "Epoch 85/300\n",
      "60000/60000 [==============================] - 7s 113us/sample - loss: 0.4189 - acc: 0.8543 - val_loss: 0.4762 - val_acc: 0.8407\n",
      "Epoch 86/300\n",
      "60000/60000 [==============================] - 7s 112us/sample - loss: 0.4192 - acc: 0.8541 - val_loss: 0.4679 - val_acc: 0.8408\n",
      "Epoch 87/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.4191 - acc: 0.8552 - val_loss: 0.4526 - val_acc: 0.8414\n",
      "Epoch 88/300\n",
      "60000/60000 [==============================] - 7s 111us/sample - loss: 0.4175 - acc: 0.8556 - val_loss: 0.4656 - val_acc: 0.8411\n",
      "Epoch 89/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.4194 - acc: 0.8546 - val_loss: 0.4650 - val_acc: 0.8421\n",
      "Epoch 90/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.4160 - acc: 0.8561 - val_loss: 0.4629 - val_acc: 0.8411\n",
      "Epoch 91/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.4178 - acc: 0.8558 - val_loss: 0.4638 - val_acc: 0.8407\n",
      "Epoch 92/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.4175 - acc: 0.8565 - val_loss: 0.4542 - val_acc: 0.8415\n",
      "Epoch 93/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.4181 - acc: 0.8552 - val_loss: 0.4594 - val_acc: 0.8423\n",
      "Epoch 94/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.4170 - acc: 0.8553 - val_loss: 0.4677 - val_acc: 0.8419\n",
      "Epoch 95/300\n",
      "60000/60000 [==============================] - 7s 111us/sample - loss: 0.4178 - acc: 0.8543 - val_loss: 0.4574 - val_acc: 0.8424\n",
      "Epoch 96/300\n",
      "60000/60000 [==============================] - 7s 111us/sample - loss: 0.4175 - acc: 0.8553 - val_loss: 0.4518 - val_acc: 0.8431\n",
      "Epoch 97/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.4166 - acc: 0.8553 - val_loss: 0.4683 - val_acc: 0.8418\n",
      "Epoch 98/300\n",
      "60000/60000 [==============================] - 7s 111us/sample - loss: 0.4186 - acc: 0.8547 - val_loss: 0.4613 - val_acc: 0.8407\n",
      "Epoch 99/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.4157 - acc: 0.8564 - val_loss: 0.4552 - val_acc: 0.8414\n",
      "Epoch 100/300\n",
      "60000/60000 [==============================] - 7s 117us/sample - loss: 0.4162 - acc: 0.8551 - val_loss: 0.4593 - val_acc: 0.8423\n",
      "Epoch 101/300\n",
      "60000/60000 [==============================] - 7s 113us/sample - loss: 0.4145 - acc: 0.8566 - val_loss: 0.4611 - val_acc: 0.8418\n",
      "Epoch 102/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.4159 - acc: 0.8563 - val_loss: 0.4570 - val_acc: 0.8419\n",
      "Epoch 103/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.4160 - acc: 0.8562 - val_loss: 0.4611 - val_acc: 0.8414\n",
      "Epoch 104/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.4162 - acc: 0.8555 - val_loss: 0.4576 - val_acc: 0.8411\n",
      "Epoch 105/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.4146 - acc: 0.8553 - val_loss: 0.4559 - val_acc: 0.8423\n",
      "Epoch 106/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.4132 - acc: 0.8559 - val_loss: 0.4662 - val_acc: 0.8417\n",
      "Epoch 107/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.4140 - acc: 0.8562 - val_loss: 0.4588 - val_acc: 0.8424\n",
      "Epoch 108/300\n",
      "60000/60000 [==============================] - 7s 111us/sample - loss: 0.4118 - acc: 0.8570 - val_loss: 0.4647 - val_acc: 0.8421\n",
      "Epoch 109/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.4137 - acc: 0.8562 - val_loss: 0.4601 - val_acc: 0.8434\n",
      "Epoch 110/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.4145 - acc: 0.8553 - val_loss: 0.4655 - val_acc: 0.8431\n",
      "Epoch 111/300\n",
      "60000/60000 [==============================] - 7s 108us/sample - loss: 0.4121 - acc: 0.8558 - val_loss: 0.4743 - val_acc: 0.8417\n",
      "Epoch 112/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.4132 - acc: 0.8572 - val_loss: 0.4596 - val_acc: 0.8435\n",
      "Epoch 113/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.4135 - acc: 0.8562 - val_loss: 0.4595 - val_acc: 0.8433\n",
      "Epoch 114/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.4138 - acc: 0.8573 - val_loss: 0.4570 - val_acc: 0.8430\n",
      "Epoch 115/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.4116 - acc: 0.8557 - val_loss: 0.4626 - val_acc: 0.8408\n",
      "Epoch 116/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.4138 - acc: 0.8556 - val_loss: 0.4552 - val_acc: 0.8421\n",
      "Epoch 117/300\n",
      "60000/60000 [==============================] - 7s 108us/sample - loss: 0.4123 - acc: 0.8575 - val_loss: 0.4693 - val_acc: 0.8430\n",
      "Epoch 118/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.4127 - acc: 0.8562 - val_loss: 0.4667 - val_acc: 0.8415\n",
      "Epoch 119/300\n",
      "60000/60000 [==============================] - 7s 113us/sample - loss: 0.4117 - acc: 0.8569 - val_loss: 0.4604 - val_acc: 0.8416\n",
      "Epoch 120/300\n",
      "60000/60000 [==============================] - 7s 111us/sample - loss: 0.4112 - acc: 0.8570 - val_loss: 0.4650 - val_acc: 0.8444\n",
      "Epoch 121/300\n",
      "60000/60000 [==============================] - 6s 107us/sample - loss: 0.4136 - acc: 0.8565 - val_loss: 0.4564 - val_acc: 0.8436\n",
      "Epoch 122/300\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 0.4114 - acc: 0.8563 - val_loss: 0.4547 - val_acc: 0.8430\n",
      "Epoch 123/300\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 0.4113 - acc: 0.8565 - val_loss: 0.4604 - val_acc: 0.8434\n",
      "Epoch 124/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.4084 - acc: 0.8578 - val_loss: 0.4605 - val_acc: 0.8440\n",
      "Epoch 125/300\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 0.4108 - acc: 0.8565 - val_loss: 0.4560 - val_acc: 0.8429\n",
      "Epoch 126/300\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 0.4112 - acc: 0.8573 - val_loss: 0.4532 - val_acc: 0.8431\n",
      "Epoch 127/300\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 0.4114 - acc: 0.8563 - val_loss: 0.4522 - val_acc: 0.8418\n",
      "Epoch 128/300\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 0.4116 - acc: 0.8573 - val_loss: 0.4578 - val_acc: 0.8443\n",
      "Epoch 129/300\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 0.4118 - acc: 0.8564 - val_loss: 0.4535 - val_acc: 0.8441\n",
      "Epoch 130/300\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 0.4116 - acc: 0.8578 - val_loss: 0.4526 - val_acc: 0.8429\n",
      "Epoch 131/300\n",
      "60000/60000 [==============================] - 6s 107us/sample - loss: 0.4126 - acc: 0.8570 - val_loss: 0.4615 - val_acc: 0.8435\n",
      "Epoch 132/300\n",
      "60000/60000 [==============================] - 6s 107us/sample - loss: 0.4096 - acc: 0.8586 - val_loss: 0.4613 - val_acc: 0.8424\n",
      "Epoch 133/300\n",
      "60000/60000 [==============================] - 7s 108us/sample - loss: 0.4098 - acc: 0.8577 - val_loss: 0.4650 - val_acc: 0.8434\n",
      "Epoch 134/300\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 0.4101 - acc: 0.8582 - val_loss: 0.4593 - val_acc: 0.8443\n",
      "Epoch 135/300\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 0.4112 - acc: 0.8578 - val_loss: 0.4556 - val_acc: 0.8441\n",
      "Epoch 136/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.4103 - acc: 0.8572 - val_loss: 0.4595 - val_acc: 0.8429\n",
      "Epoch 137/300\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 0.4100 - acc: 0.8571 - val_loss: 0.4731 - val_acc: 0.8429\n",
      "Epoch 138/300\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 0.4085 - acc: 0.8573 - val_loss: 0.4591 - val_acc: 0.8432\n",
      "Epoch 139/300\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 0.4097 - acc: 0.8573 - val_loss: 0.4618 - val_acc: 0.8425\n",
      "Epoch 140/300\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 0.4091 - acc: 0.8572 - val_loss: 0.4570 - val_acc: 0.8434\n",
      "Epoch 141/300\n",
      "60000/60000 [==============================] - 6s 107us/sample - loss: 0.4091 - acc: 0.8572 - val_loss: 0.4561 - val_acc: 0.8432\n",
      "Epoch 142/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.4072 - acc: 0.8577 - val_loss: 0.4637 - val_acc: 0.8436\n",
      "Epoch 143/300\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 0.4088 - acc: 0.8577 - val_loss: 0.4682 - val_acc: 0.8418\n",
      "Epoch 144/300\n",
      "60000/60000 [==============================] - 6s 107us/sample - loss: 0.4088 - acc: 0.8588 - val_loss: 0.4588 - val_acc: 0.8419\n",
      "Epoch 145/300\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 0.4089 - acc: 0.8575 - val_loss: 0.4581 - val_acc: 0.8422\n",
      "Epoch 146/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.4094 - acc: 0.8575 - val_loss: 0.4650 - val_acc: 0.8443\n",
      "Epoch 147/300\n",
      "60000/60000 [==============================] - 7s 115us/sample - loss: 0.4082 - acc: 0.8576 - val_loss: 0.4589 - val_acc: 0.8441\n",
      "Epoch 148/300\n",
      "60000/60000 [==============================] - 7s 116us/sample - loss: 0.4084 - acc: 0.8583 - val_loss: 0.4579 - val_acc: 0.8432\n",
      "Epoch 149/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.4091 - acc: 0.8581 - val_loss: 0.4593 - val_acc: 0.8427\n",
      "Epoch 150/300\n",
      "60000/60000 [==============================] - 6s 107us/sample - loss: 0.4090 - acc: 0.8579 - val_loss: 0.4644 - val_acc: 0.8424\n",
      "Epoch 151/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.4080 - acc: 0.8580 - val_loss: 0.4568 - val_acc: 0.8439\n",
      "Epoch 152/300\n",
      "60000/60000 [==============================] - 6s 107us/sample - loss: 0.4075 - acc: 0.8587 - val_loss: 0.4503 - val_acc: 0.8435\n",
      "Epoch 153/300\n",
      "60000/60000 [==============================] - 6s 107us/sample - loss: 0.4079 - acc: 0.8586 - val_loss: 0.4720 - val_acc: 0.8423\n",
      "Epoch 154/300\n",
      "60000/60000 [==============================] - 6s 107us/sample - loss: 0.4075 - acc: 0.8589 - val_loss: 0.4606 - val_acc: 0.8432\n",
      "Epoch 155/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.4070 - acc: 0.8586 - val_loss: 0.4573 - val_acc: 0.8436\n",
      "Epoch 156/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.4108 - acc: 0.8562 - val_loss: 0.4681 - val_acc: 0.8440\n",
      "Epoch 157/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.4086 - acc: 0.8578 - val_loss: 0.4542 - val_acc: 0.8428\n",
      "Epoch 158/300\n",
      "60000/60000 [==============================] - 6s 106us/sample - loss: 0.4066 - acc: 0.8589 - val_loss: 0.4615 - val_acc: 0.8429\n",
      "Epoch 159/300\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 0.4050 - acc: 0.8596 - val_loss: 0.4564 - val_acc: 0.8436\n",
      "Epoch 160/300\n",
      "60000/60000 [==============================] - 6s 107us/sample - loss: 0.4065 - acc: 0.8589 - val_loss: 0.4525 - val_acc: 0.8439\n",
      "Epoch 161/300\n",
      "60000/60000 [==============================] - 6s 107us/sample - loss: 0.4038 - acc: 0.8594 - val_loss: 0.4649 - val_acc: 0.8432\n",
      "Epoch 162/300\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 0.4071 - acc: 0.8574 - val_loss: 0.4566 - val_acc: 0.8441\n",
      "Epoch 163/300\n",
      "60000/60000 [==============================] - 6s 107us/sample - loss: 0.4081 - acc: 0.8564 - val_loss: 0.4498 - val_acc: 0.8435\n",
      "Epoch 164/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.4047 - acc: 0.8586 - val_loss: 0.4539 - val_acc: 0.8438\n",
      "Epoch 165/300\n",
      "60000/60000 [==============================] - 7s 111us/sample - loss: 0.4071 - acc: 0.8588 - val_loss: 0.4625 - val_acc: 0.8444\n",
      "Epoch 166/300\n",
      "60000/60000 [==============================] - 7s 112us/sample - loss: 0.4082 - acc: 0.8567 - val_loss: 0.4547 - val_acc: 0.8440\n",
      "Epoch 167/300\n",
      "60000/60000 [==============================] - 7s 111us/sample - loss: 0.4060 - acc: 0.8589 - val_loss: 0.4594 - val_acc: 0.8430\n",
      "Epoch 168/300\n",
      "60000/60000 [==============================] - 7s 108us/sample - loss: 0.4054 - acc: 0.8585 - val_loss: 0.4513 - val_acc: 0.8436\n",
      "Epoch 169/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.4050 - acc: 0.8580 - val_loss: 0.4559 - val_acc: 0.8432\n",
      "Epoch 170/300\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 0.4065 - acc: 0.8584 - val_loss: 0.4574 - val_acc: 0.8449\n",
      "Epoch 171/300\n",
      "60000/60000 [==============================] - 6s 107us/sample - loss: 0.4061 - acc: 0.8587 - val_loss: 0.4642 - val_acc: 0.8441\n",
      "Epoch 172/300\n",
      "60000/60000 [==============================] - 6s 106us/sample - loss: 0.4060 - acc: 0.8576 - val_loss: 0.4594 - val_acc: 0.8435\n",
      "Epoch 173/300\n",
      "60000/60000 [==============================] - 6s 107us/sample - loss: 0.4059 - acc: 0.8592 - val_loss: 0.4876 - val_acc: 0.8434\n",
      "Epoch 174/300\n",
      "60000/60000 [==============================] - 7s 108us/sample - loss: 0.4029 - acc: 0.8592 - val_loss: 0.4571 - val_acc: 0.8434\n",
      "Epoch 175/300\n",
      "60000/60000 [==============================] - 7s 108us/sample - loss: 0.4050 - acc: 0.8597 - val_loss: 0.4685 - val_acc: 0.8429\n",
      "Epoch 176/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.4063 - acc: 0.8587 - val_loss: 0.4535 - val_acc: 0.8447\n",
      "Epoch 177/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.4056 - acc: 0.8589 - val_loss: 0.4521 - val_acc: 0.8425\n",
      "Epoch 178/300\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 0.4064 - acc: 0.8580 - val_loss: 0.4589 - val_acc: 0.8437\n",
      "Epoch 179/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.4061 - acc: 0.8583 - val_loss: 0.4592 - val_acc: 0.8449\n",
      "Epoch 180/300\n",
      "60000/60000 [==============================] - 6s 107us/sample - loss: 0.4041 - acc: 0.8593 - val_loss: 0.5022 - val_acc: 0.8418\n",
      "Epoch 181/300\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 0.4050 - acc: 0.8582 - val_loss: 0.4588 - val_acc: 0.8433\n",
      "Epoch 182/300\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 0.4062 - acc: 0.8579 - val_loss: 0.4527 - val_acc: 0.8446\n",
      "Epoch 183/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.4065 - acc: 0.8586 - val_loss: 0.4531 - val_acc: 0.8448\n",
      "Epoch 184/300\n",
      "60000/60000 [==============================] - 7s 111us/sample - loss: 0.4031 - acc: 0.8590 - val_loss: 0.4654 - val_acc: 0.8442\n",
      "Epoch 185/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.4041 - acc: 0.8595 - val_loss: 0.4604 - val_acc: 0.8442\n",
      "Epoch 186/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.4026 - acc: 0.8591 - val_loss: 0.4718 - val_acc: 0.8428\n",
      "Epoch 187/300\n",
      "60000/60000 [==============================] - 7s 111us/sample - loss: 0.4037 - acc: 0.8603 - val_loss: 0.4570 - val_acc: 0.8436\n",
      "Epoch 188/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.4052 - acc: 0.8592 - val_loss: 0.4516 - val_acc: 0.8443\n",
      "Epoch 189/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.4028 - acc: 0.8601 - val_loss: 0.4505 - val_acc: 0.8433\n",
      "Epoch 190/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.4052 - acc: 0.8580 - val_loss: 0.4566 - val_acc: 0.8431\n",
      "Epoch 191/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.4036 - acc: 0.8592 - val_loss: 0.4551 - val_acc: 0.8439\n",
      "Epoch 192/300\n",
      "60000/60000 [==============================] - 7s 111us/sample - loss: 0.4043 - acc: 0.8587 - val_loss: 0.4594 - val_acc: 0.8432\n",
      "Epoch 193/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.4031 - acc: 0.8591 - val_loss: 0.4575 - val_acc: 0.8439\n",
      "Epoch 194/300\n",
      "60000/60000 [==============================] - 7s 112us/sample - loss: 0.4035 - acc: 0.8595 - val_loss: 0.4539 - val_acc: 0.8436\n",
      "Epoch 195/300\n",
      "60000/60000 [==============================] - 7s 118us/sample - loss: 0.4034 - acc: 0.8594 - val_loss: 0.4640 - val_acc: 0.8436\n",
      "Epoch 196/300\n",
      "60000/60000 [==============================] - 7s 113us/sample - loss: 0.4028 - acc: 0.8609 - val_loss: 0.4578 - val_acc: 0.8449\n",
      "Epoch 197/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.4036 - acc: 0.8595 - val_loss: 0.4682 - val_acc: 0.8438\n",
      "Epoch 198/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.4044 - acc: 0.8589 - val_loss: 0.4741 - val_acc: 0.8424\n",
      "Epoch 199/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.4042 - acc: 0.8587 - val_loss: 0.4711 - val_acc: 0.8449\n",
      "Epoch 200/300\n",
      "60000/60000 [==============================] - 7s 111us/sample - loss: 0.4036 - acc: 0.8587 - val_loss: 0.4536 - val_acc: 0.8451\n",
      "Epoch 201/300\n",
      "60000/60000 [==============================] - 7s 111us/sample - loss: 0.4036 - acc: 0.8587 - val_loss: 0.4509 - val_acc: 0.8457\n",
      "Epoch 202/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.4031 - acc: 0.8593 - val_loss: 0.4630 - val_acc: 0.8452\n",
      "Epoch 203/300\n",
      "60000/60000 [==============================] - 7s 111us/sample - loss: 0.4018 - acc: 0.8593 - val_loss: 0.4585 - val_acc: 0.8450\n",
      "Epoch 204/300\n",
      "60000/60000 [==============================] - 7s 111us/sample - loss: 0.4019 - acc: 0.8598 - val_loss: 0.4545 - val_acc: 0.8436\n",
      "Epoch 205/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.4012 - acc: 0.8608 - val_loss: 0.4595 - val_acc: 0.8440\n",
      "Epoch 206/300\n",
      "60000/60000 [==============================] - 7s 111us/sample - loss: 0.4044 - acc: 0.8577 - val_loss: 0.4556 - val_acc: 0.8443\n",
      "Epoch 207/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.4037 - acc: 0.8593 - val_loss: 0.4566 - val_acc: 0.8446\n",
      "Epoch 208/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.4041 - acc: 0.8596 - val_loss: 0.4590 - val_acc: 0.8438\n",
      "Epoch 209/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.4018 - acc: 0.8596 - val_loss: 0.4540 - val_acc: 0.8438\n",
      "Epoch 210/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.4014 - acc: 0.8605 - val_loss: 0.4636 - val_acc: 0.8447\n",
      "Epoch 211/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.4018 - acc: 0.8596 - val_loss: 0.4547 - val_acc: 0.8454\n",
      "Epoch 212/300\n",
      "60000/60000 [==============================] - 7s 112us/sample - loss: 0.4014 - acc: 0.8609 - val_loss: 0.4527 - val_acc: 0.8443\n",
      "Epoch 213/300\n",
      "60000/60000 [==============================] - 7s 115us/sample - loss: 0.4030 - acc: 0.8599 - val_loss: 0.4596 - val_acc: 0.8443\n",
      "Epoch 214/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.4023 - acc: 0.8597 - val_loss: 0.4554 - val_acc: 0.8446\n",
      "Epoch 215/300\n",
      "60000/60000 [==============================] - 7s 111us/sample - loss: 0.4016 - acc: 0.8594 - val_loss: 0.4589 - val_acc: 0.8450\n",
      "Epoch 216/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.4021 - acc: 0.8601 - val_loss: 0.4610 - val_acc: 0.8426\n",
      "Epoch 217/300\n",
      "60000/60000 [==============================] - 7s 111us/sample - loss: 0.4009 - acc: 0.8609 - val_loss: 0.4629 - val_acc: 0.8436\n",
      "Epoch 218/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.4020 - acc: 0.8593 - val_loss: 0.4530 - val_acc: 0.8443\n",
      "Epoch 219/300\n",
      "60000/60000 [==============================] - 6s 107us/sample - loss: 0.4016 - acc: 0.8591 - val_loss: 0.4612 - val_acc: 0.8432\n",
      "Epoch 220/300\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 0.4026 - acc: 0.8586 - val_loss: 0.4751 - val_acc: 0.8433\n",
      "Epoch 221/300\n",
      "60000/60000 [==============================] - 6s 107us/sample - loss: 0.4021 - acc: 0.8600 - val_loss: 0.4603 - val_acc: 0.8447\n",
      "Epoch 222/300\n",
      "60000/60000 [==============================] - 7s 108us/sample - loss: 0.4009 - acc: 0.8601 - val_loss: 0.4519 - val_acc: 0.8441\n",
      "Epoch 223/300\n",
      "60000/60000 [==============================] - 6s 107us/sample - loss: 0.4020 - acc: 0.8596 - val_loss: 0.4567 - val_acc: 0.8441\n",
      "Epoch 224/300\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 0.4019 - acc: 0.8599 - val_loss: 0.4678 - val_acc: 0.8422\n",
      "Epoch 225/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.4007 - acc: 0.8599 - val_loss: 0.4580 - val_acc: 0.8443\n",
      "Epoch 226/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.4014 - acc: 0.8594 - val_loss: 0.4616 - val_acc: 0.8437\n",
      "Epoch 227/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.3994 - acc: 0.8597 - val_loss: 0.4567 - val_acc: 0.8430\n",
      "Epoch 228/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.4012 - acc: 0.8593 - val_loss: 0.4552 - val_acc: 0.8435\n",
      "Epoch 229/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.4003 - acc: 0.8596 - val_loss: 0.4577 - val_acc: 0.8455\n",
      "Epoch 230/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.4018 - acc: 0.8604 - val_loss: 0.4557 - val_acc: 0.8441\n",
      "Epoch 231/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.4007 - acc: 0.8603 - val_loss: 0.4746 - val_acc: 0.8436\n",
      "Epoch 232/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.3994 - acc: 0.8592 - val_loss: 0.4676 - val_acc: 0.8435\n",
      "Epoch 233/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.4023 - acc: 0.8599 - val_loss: 0.4631 - val_acc: 0.8442\n",
      "Epoch 234/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.4001 - acc: 0.8609 - val_loss: 0.4548 - val_acc: 0.8449\n",
      "Epoch 235/300\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 0.4013 - acc: 0.8599 - val_loss: 0.4659 - val_acc: 0.8431\n",
      "Epoch 236/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.4026 - acc: 0.8586 - val_loss: 0.4691 - val_acc: 0.8445\n",
      "Epoch 237/300\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 0.4009 - acc: 0.8599 - val_loss: 0.4610 - val_acc: 0.8442\n",
      "Epoch 238/300\n",
      "60000/60000 [==============================] - 7s 111us/sample - loss: 0.4008 - acc: 0.8591 - val_loss: 0.4549 - val_acc: 0.8436\n",
      "Epoch 239/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.4009 - acc: 0.8601 - val_loss: 0.4614 - val_acc: 0.8443\n",
      "Epoch 240/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.3984 - acc: 0.8587 - val_loss: 0.4629 - val_acc: 0.8428\n",
      "Epoch 241/300\n",
      "60000/60000 [==============================] - 7s 112us/sample - loss: 0.4006 - acc: 0.8599 - val_loss: 0.4503 - val_acc: 0.8438\n",
      "Epoch 242/300\n",
      "60000/60000 [==============================] - 7s 119us/sample - loss: 0.4017 - acc: 0.8600 - val_loss: 0.4543 - val_acc: 0.8413\n",
      "Epoch 243/300\n",
      "60000/60000 [==============================] - 7s 113us/sample - loss: 0.4007 - acc: 0.8612 - val_loss: 0.4731 - val_acc: 0.8442\n",
      "Epoch 244/300\n",
      "60000/60000 [==============================] - 7s 111us/sample - loss: 0.3986 - acc: 0.8607 - val_loss: 0.4627 - val_acc: 0.8427\n",
      "Epoch 245/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.3992 - acc: 0.8608 - val_loss: 0.4613 - val_acc: 0.8442\n",
      "Epoch 246/300\n",
      "60000/60000 [==============================] - 7s 111us/sample - loss: 0.3997 - acc: 0.8587 - val_loss: 0.4557 - val_acc: 0.8449\n",
      "Epoch 247/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.4001 - acc: 0.8608 - val_loss: 0.4649 - val_acc: 0.8445\n",
      "Epoch 248/300\n",
      "60000/60000 [==============================] - 7s 108us/sample - loss: 0.4013 - acc: 0.8601 - val_loss: 0.4783 - val_acc: 0.8452\n",
      "Epoch 249/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.3996 - acc: 0.8611 - val_loss: 0.4645 - val_acc: 0.8456\n",
      "Epoch 250/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.4001 - acc: 0.8606 - val_loss: 0.4600 - val_acc: 0.8430\n",
      "Epoch 251/300\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 0.3983 - acc: 0.8608 - val_loss: 0.4515 - val_acc: 0.8445\n",
      "Epoch 252/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.4016 - acc: 0.8583 - val_loss: 0.4686 - val_acc: 0.8439\n",
      "Epoch 253/300\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 0.3984 - acc: 0.8603 - val_loss: 0.4712 - val_acc: 0.8442\n",
      "Epoch 254/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.3993 - acc: 0.8616 - val_loss: 0.4531 - val_acc: 0.8434\n",
      "Epoch 255/300\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 0.4009 - acc: 0.8604 - val_loss: 0.4592 - val_acc: 0.8434\n",
      "Epoch 256/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.3983 - acc: 0.8612 - val_loss: 0.4524 - val_acc: 0.8445\n",
      "Epoch 257/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.3989 - acc: 0.8599 - val_loss: 0.4507 - val_acc: 0.8439\n",
      "Epoch 258/300\n",
      "60000/60000 [==============================] - 7s 108us/sample - loss: 0.3993 - acc: 0.8606 - val_loss: 0.4582 - val_acc: 0.8438\n",
      "Epoch 259/300\n",
      "60000/60000 [==============================] - 7s 113us/sample - loss: 0.4007 - acc: 0.8588 - val_loss: 0.4588 - val_acc: 0.8438\n",
      "Epoch 260/300\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 0.4000 - acc: 0.8595 - val_loss: 0.4518 - val_acc: 0.8444\n",
      "Epoch 261/300\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 0.3997 - acc: 0.8591 - val_loss: 0.4529 - val_acc: 0.8446\n",
      "Epoch 262/300\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 0.3989 - acc: 0.8619 - val_loss: 0.4662 - val_acc: 0.8446\n",
      "Epoch 263/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.3984 - acc: 0.8605 - val_loss: 0.4698 - val_acc: 0.8437\n",
      "Epoch 264/300\n",
      "60000/60000 [==============================] - 6s 107us/sample - loss: 0.3969 - acc: 0.8607 - val_loss: 0.4550 - val_acc: 0.8438\n",
      "Epoch 265/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.3978 - acc: 0.8604 - val_loss: 0.4620 - val_acc: 0.8436\n",
      "Epoch 266/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.3978 - acc: 0.8614 - val_loss: 0.4697 - val_acc: 0.8434\n",
      "Epoch 267/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.3974 - acc: 0.8625 - val_loss: 0.4543 - val_acc: 0.8455\n",
      "Epoch 268/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.3992 - acc: 0.8599 - val_loss: 0.4629 - val_acc: 0.8440\n",
      "Epoch 269/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.3988 - acc: 0.8600 - val_loss: 0.4684 - val_acc: 0.8437\n",
      "Epoch 270/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.3980 - acc: 0.8613 - val_loss: 0.4579 - val_acc: 0.8436\n",
      "Epoch 271/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.3993 - acc: 0.8608 - val_loss: 0.4604 - val_acc: 0.8432\n",
      "Epoch 272/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.3993 - acc: 0.8595 - val_loss: 0.4641 - val_acc: 0.8448\n",
      "Epoch 273/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.3975 - acc: 0.8604 - val_loss: 0.4629 - val_acc: 0.8435\n",
      "Epoch 274/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.3991 - acc: 0.8609 - val_loss: 0.4682 - val_acc: 0.8426\n",
      "Epoch 275/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.4004 - acc: 0.8601 - val_loss: 0.4587 - val_acc: 0.8443\n",
      "Epoch 276/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.3967 - acc: 0.8607 - val_loss: 0.4581 - val_acc: 0.8441\n",
      "Epoch 277/300\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 0.3992 - acc: 0.8600 - val_loss: 0.4623 - val_acc: 0.8438\n",
      "Epoch 278/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.3964 - acc: 0.8610 - val_loss: 0.4541 - val_acc: 0.8449\n",
      "Epoch 279/300\n",
      "60000/60000 [==============================] - 7s 108us/sample - loss: 0.4012 - acc: 0.8599 - val_loss: 0.4672 - val_acc: 0.8425\n",
      "Epoch 280/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.3990 - acc: 0.8600 - val_loss: 0.4663 - val_acc: 0.8436\n",
      "Epoch 281/300\n",
      "60000/60000 [==============================] - 7s 111us/sample - loss: 0.3982 - acc: 0.8608 - val_loss: 0.4609 - val_acc: 0.8444\n",
      "Epoch 282/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.3978 - acc: 0.8615 - val_loss: 0.4698 - val_acc: 0.8439\n",
      "Epoch 283/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.3978 - acc: 0.8607 - val_loss: 0.4607 - val_acc: 0.8439\n",
      "Epoch 284/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.3988 - acc: 0.8600 - val_loss: 0.4621 - val_acc: 0.8443\n",
      "Epoch 285/300\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 0.4006 - acc: 0.8607 - val_loss: 0.4613 - val_acc: 0.8433\n",
      "Epoch 286/300\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 0.3989 - acc: 0.8607 - val_loss: 0.4674 - val_acc: 0.8439\n",
      "Epoch 287/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.3972 - acc: 0.8615 - val_loss: 0.4627 - val_acc: 0.8432\n",
      "Epoch 288/300\n",
      "60000/60000 [==============================] - 7s 108us/sample - loss: 0.3966 - acc: 0.8609 - val_loss: 0.4788 - val_acc: 0.8426\n",
      "Epoch 289/300\n",
      "60000/60000 [==============================] - 7s 116us/sample - loss: 0.3956 - acc: 0.8615 - val_loss: 0.4753 - val_acc: 0.8443\n",
      "Epoch 290/300\n",
      "60000/60000 [==============================] - 7s 113us/sample - loss: 0.3963 - acc: 0.8615 - val_loss: 0.4725 - val_acc: 0.8436\n",
      "Epoch 291/300\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 0.3983 - acc: 0.8594 - val_loss: 0.4809 - val_acc: 0.8434\n",
      "Epoch 292/300\n",
      "60000/60000 [==============================] - 7s 111us/sample - loss: 0.3991 - acc: 0.8593 - val_loss: 0.4591 - val_acc: 0.8441\n",
      "Epoch 293/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.3990 - acc: 0.8602 - val_loss: 0.4558 - val_acc: 0.8446\n",
      "Epoch 294/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.3975 - acc: 0.8615 - val_loss: 0.4787 - val_acc: 0.8429\n",
      "Epoch 295/300\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.3982 - acc: 0.8612 - val_loss: 0.4667 - val_acc: 0.8440\n",
      "Epoch 296/300\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 0.3965 - acc: 0.8607 - val_loss: 0.4566 - val_acc: 0.8441\n",
      "Epoch 297/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.3959 - acc: 0.8616 - val_loss: 0.4624 - val_acc: 0.8446\n",
      "Epoch 298/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.3986 - acc: 0.8599 - val_loss: 0.4600 - val_acc: 0.8420\n",
      "Epoch 299/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.3959 - acc: 0.8601 - val_loss: 0.4531 - val_acc: 0.8441\n",
      "Epoch 300/300\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.3974 - acc: 0.8613 - val_loss: 0.4672 - val_acc: 0.8445\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f45702e0350>"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trainX, trainY, validation_data=(testX, testY), epochs=300, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yLXUE9jWOIVV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j9CSqKvpOIVk"
   },
   "source": [
    "### Build the Neural Network model with 3 Dense layers with 100,100,10 neurons respectively in each layer. Use cross entropy loss function and singmoid as activation in the hidden layers and softmax as activation function in the output layer. Use sgd optimizer with learning rate 0.03."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GGAad54JOIVm"
   },
   "outputs": [],
   "source": [
    "#Initialize Sequential model\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "#Reshape data from 2D to 1D -> 28x28 to 784\n",
    "model.add(tf.keras.layers.Reshape((784,),input_shape=(28,28,)))\n",
    "\n",
    "#Normalize the data\n",
    "model.add(tf.keras.layers.BatchNormalization(input_shape=(4,)))\n",
    "\n",
    "#Add 1st hidden layer\n",
    "model.add(tf.keras.layers.Dense(100, activation='sigmoid'))\n",
    "\n",
    "#Add 2nd hidden layer\n",
    "model.add(tf.keras.layers.Dense(100, activation='sigmoid'))\n",
    "\n",
    "#Add Dense Layer which provides 10 Outputs after applying softmax\n",
    "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "\n",
    "sgd_optimizer = tf.keras.optimizers.SGD(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'],optimizer=sgd_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6788270,
     "status": "ok",
     "timestamp": 1569741116774,
     "user": {
      "displayName": "Raghav Praneeth Amaravadi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAMhpxsJE7d0g9jOsWi8V_E3rjg4Ut9Yz-RLZWsQw=s64",
      "userId": "05628056859539857657"
     },
     "user_tz": -330
    },
    "id": "MQ7oIymROIVp",
    "outputId": "cf6d4927-a1f2-4aef-b13e-51ba1fe8a1e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/300\n",
      "60000/60000 [==============================] - 8s 126us/sample - loss: 1.5780 - acc: 0.6122 - val_loss: 1.0541 - val_acc: 0.7262\n",
      "Epoch 2/300\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.8712 - acc: 0.7422 - val_loss: 0.7261 - val_acc: 0.7598\n",
      "Epoch 3/300\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.6783 - acc: 0.7698 - val_loss: 0.6204 - val_acc: 0.7817\n",
      "Epoch 4/300\n",
      "60000/60000 [==============================] - 7s 122us/sample - loss: 0.5995 - acc: 0.7906 - val_loss: 0.5627 - val_acc: 0.7984\n",
      "Epoch 5/300\n",
      "60000/60000 [==============================] - 8s 129us/sample - loss: 0.5505 - acc: 0.8055 - val_loss: 0.5274 - val_acc: 0.8114\n",
      "Epoch 6/300\n",
      "60000/60000 [==============================] - 8s 126us/sample - loss: 0.5178 - acc: 0.8172 - val_loss: 0.5028 - val_acc: 0.8170\n",
      "Epoch 7/300\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.4965 - acc: 0.8252 - val_loss: 0.4861 - val_acc: 0.8229\n",
      "Epoch 8/300\n",
      "60000/60000 [==============================] - 7s 122us/sample - loss: 0.4795 - acc: 0.8300 - val_loss: 0.4718 - val_acc: 0.8299\n",
      "Epoch 9/300\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.4668 - acc: 0.8336 - val_loss: 0.4607 - val_acc: 0.8319\n",
      "Epoch 10/300\n",
      "60000/60000 [==============================] - 7s 122us/sample - loss: 0.4544 - acc: 0.8374 - val_loss: 0.4537 - val_acc: 0.8345\n",
      "Epoch 11/300\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.4432 - acc: 0.8420 - val_loss: 0.4461 - val_acc: 0.8360\n",
      "Epoch 12/300\n",
      "60000/60000 [==============================] - 7s 122us/sample - loss: 0.4356 - acc: 0.8443 - val_loss: 0.4375 - val_acc: 0.8391\n",
      "Epoch 13/300\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.4274 - acc: 0.8481 - val_loss: 0.4332 - val_acc: 0.8416\n",
      "Epoch 14/300\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.4214 - acc: 0.8511 - val_loss: 0.4266 - val_acc: 0.8449\n",
      "Epoch 15/300\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.4165 - acc: 0.8524 - val_loss: 0.4226 - val_acc: 0.8462\n",
      "Epoch 16/300\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.4116 - acc: 0.8538 - val_loss: 0.4181 - val_acc: 0.8478\n",
      "Epoch 17/300\n",
      "60000/60000 [==============================] - 7s 122us/sample - loss: 0.4043 - acc: 0.8560 - val_loss: 0.4147 - val_acc: 0.8486\n",
      "Epoch 18/300\n",
      "60000/60000 [==============================] - 7s 122us/sample - loss: 0.3993 - acc: 0.8576 - val_loss: 0.4115 - val_acc: 0.8515\n",
      "Epoch 19/300\n",
      "60000/60000 [==============================] - 7s 122us/sample - loss: 0.3952 - acc: 0.8591 - val_loss: 0.4068 - val_acc: 0.8529\n",
      "Epoch 20/300\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.3895 - acc: 0.8610 - val_loss: 0.4055 - val_acc: 0.8520\n",
      "Epoch 21/300\n",
      "60000/60000 [==============================] - 7s 121us/sample - loss: 0.3882 - acc: 0.8622 - val_loss: 0.4018 - val_acc: 0.8546\n",
      "Epoch 22/300\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.3827 - acc: 0.8625 - val_loss: 0.3982 - val_acc: 0.8553\n",
      "Epoch 23/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.3784 - acc: 0.8658 - val_loss: 0.3954 - val_acc: 0.8565\n",
      "Epoch 24/300\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.3749 - acc: 0.8661 - val_loss: 0.3926 - val_acc: 0.8565\n",
      "Epoch 25/300\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.3728 - acc: 0.8671 - val_loss: 0.3906 - val_acc: 0.8582\n",
      "Epoch 26/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.3677 - acc: 0.8687 - val_loss: 0.3886 - val_acc: 0.8593\n",
      "Epoch 27/300\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.3653 - acc: 0.8696 - val_loss: 0.3851 - val_acc: 0.8599\n",
      "Epoch 28/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.3618 - acc: 0.8709 - val_loss: 0.3824 - val_acc: 0.8605\n",
      "Epoch 29/300\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.3589 - acc: 0.8712 - val_loss: 0.3842 - val_acc: 0.8610\n",
      "Epoch 30/300\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.3559 - acc: 0.8725 - val_loss: 0.3781 - val_acc: 0.8616\n",
      "Epoch 31/300\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.3542 - acc: 0.8743 - val_loss: 0.3787 - val_acc: 0.8628\n",
      "Epoch 32/300\n",
      "60000/60000 [==============================] - 8s 129us/sample - loss: 0.3515 - acc: 0.8743 - val_loss: 0.3744 - val_acc: 0.8643\n",
      "Epoch 33/300\n",
      "60000/60000 [==============================] - 8s 128us/sample - loss: 0.3479 - acc: 0.8753 - val_loss: 0.3740 - val_acc: 0.8619\n",
      "Epoch 34/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.3454 - acc: 0.8776 - val_loss: 0.3711 - val_acc: 0.8646\n",
      "Epoch 35/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.3410 - acc: 0.8788 - val_loss: 0.3687 - val_acc: 0.8659\n",
      "Epoch 36/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.3383 - acc: 0.8787 - val_loss: 0.3672 - val_acc: 0.8652\n",
      "Epoch 37/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.3351 - acc: 0.8801 - val_loss: 0.3677 - val_acc: 0.8675\n",
      "Epoch 38/300\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.3352 - acc: 0.8800 - val_loss: 0.3643 - val_acc: 0.8684\n",
      "Epoch 39/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.3313 - acc: 0.8802 - val_loss: 0.3628 - val_acc: 0.8689\n",
      "Epoch 40/300\n",
      "60000/60000 [==============================] - 8s 125us/sample - loss: 0.3284 - acc: 0.8830 - val_loss: 0.3612 - val_acc: 0.8687\n",
      "Epoch 41/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.3268 - acc: 0.8830 - val_loss: 0.3593 - val_acc: 0.8700\n",
      "Epoch 42/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.3248 - acc: 0.8828 - val_loss: 0.3581 - val_acc: 0.8704\n",
      "Epoch 43/300\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.3231 - acc: 0.8850 - val_loss: 0.3595 - val_acc: 0.8695\n",
      "Epoch 44/300\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.3192 - acc: 0.8852 - val_loss: 0.3557 - val_acc: 0.8718\n",
      "Epoch 45/300\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.3171 - acc: 0.8869 - val_loss: 0.3543 - val_acc: 0.8718\n",
      "Epoch 46/300\n",
      "60000/60000 [==============================] - 8s 130us/sample - loss: 0.3151 - acc: 0.8869 - val_loss: 0.3549 - val_acc: 0.8719\n",
      "Epoch 47/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.3140 - acc: 0.8867 - val_loss: 0.3528 - val_acc: 0.8740\n",
      "Epoch 48/300\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.3106 - acc: 0.8887 - val_loss: 0.3506 - val_acc: 0.8735\n",
      "Epoch 49/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.3091 - acc: 0.8880 - val_loss: 0.3504 - val_acc: 0.8742\n",
      "Epoch 50/300\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.3061 - acc: 0.8911 - val_loss: 0.3497 - val_acc: 0.8737\n",
      "Epoch 51/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.3044 - acc: 0.8910 - val_loss: 0.3464 - val_acc: 0.8750\n",
      "Epoch 52/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.3045 - acc: 0.8909 - val_loss: 0.3484 - val_acc: 0.8733\n",
      "Epoch 53/300\n",
      "60000/60000 [==============================] - 8s 125us/sample - loss: 0.3011 - acc: 0.8915 - val_loss: 0.3475 - val_acc: 0.8751\n",
      "Epoch 54/300\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.2978 - acc: 0.8930 - val_loss: 0.3451 - val_acc: 0.8768\n",
      "Epoch 55/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.2978 - acc: 0.8928 - val_loss: 0.3443 - val_acc: 0.8769\n",
      "Epoch 56/300\n",
      "60000/60000 [==============================] - 7s 125us/sample - loss: 0.2954 - acc: 0.8942 - val_loss: 0.3451 - val_acc: 0.8744\n",
      "Epoch 57/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.2924 - acc: 0.8951 - val_loss: 0.3431 - val_acc: 0.8760\n",
      "Epoch 58/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.2896 - acc: 0.8949 - val_loss: 0.3427 - val_acc: 0.8749\n",
      "Epoch 59/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.2897 - acc: 0.8967 - val_loss: 0.3418 - val_acc: 0.8766\n",
      "Epoch 60/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.2879 - acc: 0.8970 - val_loss: 0.3395 - val_acc: 0.8761\n",
      "Epoch 61/300\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.2853 - acc: 0.8975 - val_loss: 0.3389 - val_acc: 0.8773\n",
      "Epoch 62/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.2840 - acc: 0.8982 - val_loss: 0.3377 - val_acc: 0.8797\n",
      "Epoch 63/300\n",
      "60000/60000 [==============================] - 7s 125us/sample - loss: 0.2822 - acc: 0.8975 - val_loss: 0.3361 - val_acc: 0.8776\n",
      "Epoch 64/300\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.2803 - acc: 0.8987 - val_loss: 0.3397 - val_acc: 0.8763\n",
      "Epoch 65/300\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.2779 - acc: 0.8997 - val_loss: 0.3357 - val_acc: 0.8784\n",
      "Epoch 66/300\n",
      "60000/60000 [==============================] - 7s 125us/sample - loss: 0.2760 - acc: 0.9006 - val_loss: 0.3367 - val_acc: 0.8775\n",
      "Epoch 67/300\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.2755 - acc: 0.9010 - val_loss: 0.3337 - val_acc: 0.8802\n",
      "Epoch 68/300\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.2709 - acc: 0.9033 - val_loss: 0.3340 - val_acc: 0.8794\n",
      "Epoch 69/300\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.2727 - acc: 0.9021 - val_loss: 0.3344 - val_acc: 0.8783\n",
      "Epoch 70/300\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.2683 - acc: 0.9041 - val_loss: 0.3324 - val_acc: 0.8808\n",
      "Epoch 71/300\n",
      "60000/60000 [==============================] - 7s 122us/sample - loss: 0.2696 - acc: 0.9031 - val_loss: 0.3340 - val_acc: 0.8781\n",
      "Epoch 72/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.2659 - acc: 0.9043 - val_loss: 0.3335 - val_acc: 0.8790\n",
      "Epoch 73/300\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.2645 - acc: 0.9053 - val_loss: 0.3309 - val_acc: 0.8783\n",
      "Epoch 74/300\n",
      "60000/60000 [==============================] - 8s 133us/sample - loss: 0.2639 - acc: 0.9060 - val_loss: 0.3315 - val_acc: 0.8786\n",
      "Epoch 75/300\n",
      "60000/60000 [==============================] - 8s 127us/sample - loss: 0.2632 - acc: 0.9057 - val_loss: 0.3314 - val_acc: 0.8789\n",
      "Epoch 76/300\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.2598 - acc: 0.9075 - val_loss: 0.3340 - val_acc: 0.8781\n",
      "Epoch 77/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.2581 - acc: 0.9068 - val_loss: 0.3283 - val_acc: 0.8801\n",
      "Epoch 78/300\n",
      "60000/60000 [==============================] - 7s 122us/sample - loss: 0.2588 - acc: 0.9069 - val_loss: 0.3286 - val_acc: 0.8815\n",
      "Epoch 79/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.2535 - acc: 0.9094 - val_loss: 0.3296 - val_acc: 0.8817\n",
      "Epoch 80/300\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.2543 - acc: 0.9088 - val_loss: 0.3282 - val_acc: 0.8821\n",
      "Epoch 81/300\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.2544 - acc: 0.9090 - val_loss: 0.3280 - val_acc: 0.8816\n",
      "Epoch 82/300\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.2513 - acc: 0.9098 - val_loss: 0.3287 - val_acc: 0.8825\n",
      "Epoch 83/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.2481 - acc: 0.9103 - val_loss: 0.3292 - val_acc: 0.8812\n",
      "Epoch 84/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.2481 - acc: 0.9109 - val_loss: 0.3271 - val_acc: 0.8838\n",
      "Epoch 85/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.2453 - acc: 0.9123 - val_loss: 0.3265 - val_acc: 0.8826\n",
      "Epoch 86/300\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.2459 - acc: 0.9116 - val_loss: 0.3235 - val_acc: 0.8833\n",
      "Epoch 87/300\n",
      "60000/60000 [==============================] - 8s 130us/sample - loss: 0.2433 - acc: 0.9142 - val_loss: 0.3262 - val_acc: 0.8825\n",
      "Epoch 88/300\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.2411 - acc: 0.9140 - val_loss: 0.3272 - val_acc: 0.8822\n",
      "Epoch 89/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.2408 - acc: 0.9137 - val_loss: 0.3267 - val_acc: 0.8831\n",
      "Epoch 90/300\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.2382 - acc: 0.9151 - val_loss: 0.3263 - val_acc: 0.8845\n",
      "Epoch 91/300\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.2395 - acc: 0.9135 - val_loss: 0.3265 - val_acc: 0.8823\n",
      "Epoch 92/300\n",
      "60000/60000 [==============================] - 7s 122us/sample - loss: 0.2365 - acc: 0.9147 - val_loss: 0.3275 - val_acc: 0.8822\n",
      "Epoch 93/300\n",
      "60000/60000 [==============================] - 7s 122us/sample - loss: 0.2332 - acc: 0.9158 - val_loss: 0.3270 - val_acc: 0.8830\n",
      "Epoch 94/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.2354 - acc: 0.9159 - val_loss: 0.3241 - val_acc: 0.8845\n",
      "Epoch 95/300\n",
      "60000/60000 [==============================] - 7s 122us/sample - loss: 0.2312 - acc: 0.9168 - val_loss: 0.3270 - val_acc: 0.8826\n",
      "Epoch 96/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.2311 - acc: 0.9174 - val_loss: 0.3241 - val_acc: 0.8842\n",
      "Epoch 97/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.2289 - acc: 0.9175 - val_loss: 0.3258 - val_acc: 0.8843\n",
      "Epoch 98/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.2289 - acc: 0.9179 - val_loss: 0.3243 - val_acc: 0.8856\n",
      "Epoch 99/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.2269 - acc: 0.9187 - val_loss: 0.3248 - val_acc: 0.8845\n",
      "Epoch 100/300\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.2241 - acc: 0.9200 - val_loss: 0.3310 - val_acc: 0.8819\n",
      "Epoch 101/300\n",
      "60000/60000 [==============================] - 7s 122us/sample - loss: 0.2240 - acc: 0.9200 - val_loss: 0.3257 - val_acc: 0.8825\n",
      "Epoch 102/300\n",
      "60000/60000 [==============================] - 8s 126us/sample - loss: 0.2231 - acc: 0.9207 - val_loss: 0.3230 - val_acc: 0.8854\n",
      "Epoch 103/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.2211 - acc: 0.9206 - val_loss: 0.3322 - val_acc: 0.8811\n",
      "Epoch 104/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.2193 - acc: 0.9210 - val_loss: 0.3251 - val_acc: 0.8840\n",
      "Epoch 105/300\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.2191 - acc: 0.9221 - val_loss: 0.3312 - val_acc: 0.8807\n",
      "Epoch 106/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.2177 - acc: 0.9223 - val_loss: 0.3285 - val_acc: 0.8849\n",
      "Epoch 107/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.2177 - acc: 0.9211 - val_loss: 0.3222 - val_acc: 0.8824\n",
      "Epoch 108/300\n",
      "60000/60000 [==============================] - 7s 125us/sample - loss: 0.2159 - acc: 0.9227 - val_loss: 0.3270 - val_acc: 0.8821\n",
      "Epoch 109/300\n",
      "60000/60000 [==============================] - 8s 125us/sample - loss: 0.2139 - acc: 0.9237 - val_loss: 0.3255 - val_acc: 0.8834\n",
      "Epoch 110/300\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.2121 - acc: 0.9245 - val_loss: 0.3281 - val_acc: 0.8839\n",
      "Epoch 111/300\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.2096 - acc: 0.9256 - val_loss: 0.3240 - val_acc: 0.8845\n",
      "Epoch 112/300\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.2116 - acc: 0.9246 - val_loss: 0.3272 - val_acc: 0.8830\n",
      "Epoch 113/300\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.2089 - acc: 0.9242 - val_loss: 0.3288 - val_acc: 0.8851\n",
      "Epoch 114/300\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.2069 - acc: 0.9255 - val_loss: 0.3251 - val_acc: 0.8843\n",
      "Epoch 115/300\n",
      "60000/60000 [==============================] - 8s 126us/sample - loss: 0.2051 - acc: 0.9267 - val_loss: 0.3233 - val_acc: 0.8849\n",
      "Epoch 116/300\n",
      "60000/60000 [==============================] - 8s 130us/sample - loss: 0.2037 - acc: 0.9280 - val_loss: 0.3265 - val_acc: 0.8819\n",
      "Epoch 117/300\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.2038 - acc: 0.9270 - val_loss: 0.3253 - val_acc: 0.8859\n",
      "Epoch 118/300\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.2028 - acc: 0.9278 - val_loss: 0.3271 - val_acc: 0.8855\n",
      "Epoch 119/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.2039 - acc: 0.9278 - val_loss: 0.3319 - val_acc: 0.8846\n",
      "Epoch 120/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.1993 - acc: 0.9279 - val_loss: 0.3263 - val_acc: 0.8837\n",
      "Epoch 121/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.1975 - acc: 0.9300 - val_loss: 0.3264 - val_acc: 0.8854\n",
      "Epoch 122/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.1975 - acc: 0.9301 - val_loss: 0.3255 - val_acc: 0.8860\n",
      "Epoch 123/300\n",
      "60000/60000 [==============================] - 8s 125us/sample - loss: 0.1965 - acc: 0.9293 - val_loss: 0.3269 - val_acc: 0.8856\n",
      "Epoch 124/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.1949 - acc: 0.9295 - val_loss: 0.3252 - val_acc: 0.8859\n",
      "Epoch 125/300\n",
      "60000/60000 [==============================] - 7s 125us/sample - loss: 0.1954 - acc: 0.9302 - val_loss: 0.3285 - val_acc: 0.8849\n",
      "Epoch 126/300\n",
      "60000/60000 [==============================] - 7s 125us/sample - loss: 0.1926 - acc: 0.9318 - val_loss: 0.3263 - val_acc: 0.8858\n",
      "Epoch 127/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.1897 - acc: 0.9316 - val_loss: 0.3275 - val_acc: 0.8860\n",
      "Epoch 128/300\n",
      "60000/60000 [==============================] - 8s 131us/sample - loss: 0.1938 - acc: 0.9307 - val_loss: 0.3266 - val_acc: 0.8854\n",
      "Epoch 129/300\n",
      "60000/60000 [==============================] - 7s 125us/sample - loss: 0.1903 - acc: 0.9330 - val_loss: 0.3267 - val_acc: 0.8854\n",
      "Epoch 130/300\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.1885 - acc: 0.9320 - val_loss: 0.3307 - val_acc: 0.8875\n",
      "Epoch 131/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.1879 - acc: 0.9333 - val_loss: 0.3277 - val_acc: 0.8877\n",
      "Epoch 132/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.1857 - acc: 0.9326 - val_loss: 0.3288 - val_acc: 0.8863\n",
      "Epoch 133/300\n",
      "60000/60000 [==============================] - 8s 125us/sample - loss: 0.1869 - acc: 0.9335 - val_loss: 0.3299 - val_acc: 0.8866\n",
      "Epoch 134/300\n",
      "60000/60000 [==============================] - 7s 125us/sample - loss: 0.1838 - acc: 0.9354 - val_loss: 0.3324 - val_acc: 0.8862\n",
      "Epoch 135/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.1848 - acc: 0.9339 - val_loss: 0.3281 - val_acc: 0.8863\n",
      "Epoch 136/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.1822 - acc: 0.9339 - val_loss: 0.3316 - val_acc: 0.8869\n",
      "Epoch 137/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.1792 - acc: 0.9359 - val_loss: 0.3286 - val_acc: 0.8877\n",
      "Epoch 138/300\n",
      "60000/60000 [==============================] - 7s 122us/sample - loss: 0.1817 - acc: 0.9353 - val_loss: 0.3343 - val_acc: 0.8868\n",
      "Epoch 139/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.1783 - acc: 0.9363 - val_loss: 0.3318 - val_acc: 0.8864\n",
      "Epoch 140/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.1786 - acc: 0.9370 - val_loss: 0.3313 - val_acc: 0.8846\n",
      "Epoch 141/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.1768 - acc: 0.9372 - val_loss: 0.3301 - val_acc: 0.8871\n",
      "Epoch 142/300\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.1747 - acc: 0.9381 - val_loss: 0.3317 - val_acc: 0.8874\n",
      "Epoch 143/300\n",
      "60000/60000 [==============================] - 7s 122us/sample - loss: 0.1763 - acc: 0.9364 - val_loss: 0.3347 - val_acc: 0.8855\n",
      "Epoch 144/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.1743 - acc: 0.9383 - val_loss: 0.3344 - val_acc: 0.8836\n",
      "Epoch 145/300\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.1754 - acc: 0.9373 - val_loss: 0.3337 - val_acc: 0.8846\n",
      "Epoch 146/300\n",
      "60000/60000 [==============================] - 7s 122us/sample - loss: 0.1725 - acc: 0.9387 - val_loss: 0.3392 - val_acc: 0.8866\n",
      "Epoch 147/300\n",
      "60000/60000 [==============================] - 7s 122us/sample - loss: 0.1713 - acc: 0.9387 - val_loss: 0.3310 - val_acc: 0.8912\n",
      "Epoch 148/300\n",
      "60000/60000 [==============================] - 7s 122us/sample - loss: 0.1714 - acc: 0.9388 - val_loss: 0.3330 - val_acc: 0.8881\n",
      "Epoch 149/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.1696 - acc: 0.9393 - val_loss: 0.3338 - val_acc: 0.8865\n",
      "Epoch 150/300\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.1694 - acc: 0.9398 - val_loss: 0.3356 - val_acc: 0.8859\n",
      "Epoch 151/300\n",
      "60000/60000 [==============================] - 7s 122us/sample - loss: 0.1694 - acc: 0.9388 - val_loss: 0.3372 - val_acc: 0.8883\n",
      "Epoch 152/300\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.1687 - acc: 0.9386 - val_loss: 0.3337 - val_acc: 0.8867\n",
      "Epoch 153/300\n",
      "60000/60000 [==============================] - 7s 122us/sample - loss: 0.1675 - acc: 0.9396 - val_loss: 0.3360 - val_acc: 0.8874\n",
      "Epoch 154/300\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.1652 - acc: 0.9408 - val_loss: 0.3351 - val_acc: 0.8880\n",
      "Epoch 155/300\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.1651 - acc: 0.9414 - val_loss: 0.3365 - val_acc: 0.8865\n",
      "Epoch 156/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.1642 - acc: 0.9419 - val_loss: 0.3370 - val_acc: 0.8854\n",
      "Epoch 157/300\n",
      "60000/60000 [==============================] - 8s 131us/sample - loss: 0.1638 - acc: 0.9415 - val_loss: 0.3354 - val_acc: 0.8864\n",
      "Epoch 158/300\n",
      "60000/60000 [==============================] - 8s 132us/sample - loss: 0.1619 - acc: 0.9419 - val_loss: 0.3417 - val_acc: 0.8890\n",
      "Epoch 159/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.1606 - acc: 0.9429 - val_loss: 0.3373 - val_acc: 0.8884\n",
      "Epoch 160/300\n",
      "60000/60000 [==============================] - 7s 125us/sample - loss: 0.1602 - acc: 0.9423 - val_loss: 0.3373 - val_acc: 0.8887\n",
      "Epoch 161/300\n",
      "60000/60000 [==============================] - 8s 125us/sample - loss: 0.1580 - acc: 0.9442 - val_loss: 0.3454 - val_acc: 0.8887\n",
      "Epoch 162/300\n",
      "60000/60000 [==============================] - 8s 126us/sample - loss: 0.1594 - acc: 0.9437 - val_loss: 0.3401 - val_acc: 0.8877\n",
      "Epoch 163/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.1573 - acc: 0.9444 - val_loss: 0.3372 - val_acc: 0.8909\n",
      "Epoch 164/300\n",
      "60000/60000 [==============================] - 7s 125us/sample - loss: 0.1578 - acc: 0.9439 - val_loss: 0.3404 - val_acc: 0.8882\n",
      "Epoch 165/300\n",
      "60000/60000 [==============================] - 7s 125us/sample - loss: 0.1582 - acc: 0.9437 - val_loss: 0.3400 - val_acc: 0.8870\n",
      "Epoch 166/300\n",
      "60000/60000 [==============================] - 8s 125us/sample - loss: 0.1551 - acc: 0.9444 - val_loss: 0.3447 - val_acc: 0.8885\n",
      "Epoch 167/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.1537 - acc: 0.9456 - val_loss: 0.3506 - val_acc: 0.8891\n",
      "Epoch 168/300\n",
      "60000/60000 [==============================] - 8s 125us/sample - loss: 0.1534 - acc: 0.9449 - val_loss: 0.3386 - val_acc: 0.8877\n",
      "Epoch 169/300\n",
      "60000/60000 [==============================] - 8s 131us/sample - loss: 0.1549 - acc: 0.9460 - val_loss: 0.3420 - val_acc: 0.8871\n",
      "Epoch 170/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.1509 - acc: 0.9461 - val_loss: 0.3430 - val_acc: 0.8894\n",
      "Epoch 171/300\n",
      "60000/60000 [==============================] - 7s 125us/sample - loss: 0.1507 - acc: 0.9464 - val_loss: 0.3448 - val_acc: 0.8886\n",
      "Epoch 172/300\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.1500 - acc: 0.9468 - val_loss: 0.3452 - val_acc: 0.8859\n",
      "Epoch 173/300\n",
      "60000/60000 [==============================] - 7s 125us/sample - loss: 0.1504 - acc: 0.9470 - val_loss: 0.3451 - val_acc: 0.8878\n",
      "Epoch 174/300\n",
      "60000/60000 [==============================] - 7s 125us/sample - loss: 0.1487 - acc: 0.9479 - val_loss: 0.3475 - val_acc: 0.8889\n",
      "Epoch 175/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.1484 - acc: 0.9477 - val_loss: 0.3487 - val_acc: 0.8879\n",
      "Epoch 176/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.1465 - acc: 0.9481 - val_loss: 0.3450 - val_acc: 0.8873\n",
      "Epoch 177/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.1459 - acc: 0.9485 - val_loss: 0.3436 - val_acc: 0.8892\n",
      "Epoch 178/300\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.1448 - acc: 0.9481 - val_loss: 0.3512 - val_acc: 0.8885\n",
      "Epoch 179/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.1453 - acc: 0.9482 - val_loss: 0.3479 - val_acc: 0.8882\n",
      "Epoch 180/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.1434 - acc: 0.9493 - val_loss: 0.3513 - val_acc: 0.8867\n",
      "Epoch 181/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.1425 - acc: 0.9492 - val_loss: 0.3452 - val_acc: 0.8898\n",
      "Epoch 182/300\n",
      "60000/60000 [==============================] - 8s 125us/sample - loss: 0.1449 - acc: 0.9484 - val_loss: 0.3517 - val_acc: 0.8869\n",
      "Epoch 183/300\n",
      "60000/60000 [==============================] - 7s 125us/sample - loss: 0.1411 - acc: 0.9488 - val_loss: 0.3511 - val_acc: 0.8871\n",
      "Epoch 184/300\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.1414 - acc: 0.9496 - val_loss: 0.3497 - val_acc: 0.8883\n",
      "Epoch 185/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.1373 - acc: 0.9509 - val_loss: 0.3509 - val_acc: 0.8886\n",
      "Epoch 186/300\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.1385 - acc: 0.9513 - val_loss: 0.3504 - val_acc: 0.8871\n",
      "Epoch 187/300\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.1406 - acc: 0.9499 - val_loss: 0.3512 - val_acc: 0.8884\n",
      "Epoch 188/300\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.1381 - acc: 0.9510 - val_loss: 0.3506 - val_acc: 0.8882\n",
      "Epoch 189/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.1374 - acc: 0.9512 - val_loss: 0.3533 - val_acc: 0.8882\n",
      "Epoch 190/300\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.1371 - acc: 0.9515 - val_loss: 0.3639 - val_acc: 0.8849\n",
      "Epoch 191/300\n",
      "60000/60000 [==============================] - 7s 122us/sample - loss: 0.1368 - acc: 0.9513 - val_loss: 0.3600 - val_acc: 0.8876\n",
      "Epoch 192/300\n",
      "60000/60000 [==============================] - 7s 122us/sample - loss: 0.1338 - acc: 0.9530 - val_loss: 0.3527 - val_acc: 0.8881\n",
      "Epoch 193/300\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.1343 - acc: 0.9529 - val_loss: 0.3574 - val_acc: 0.8882\n",
      "Epoch 194/300\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.1360 - acc: 0.9522 - val_loss: 0.3516 - val_acc: 0.8876\n",
      "Epoch 195/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.1325 - acc: 0.9527 - val_loss: 0.3530 - val_acc: 0.8881\n",
      "Epoch 196/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.1341 - acc: 0.9520 - val_loss: 0.3625 - val_acc: 0.8878\n",
      "Epoch 197/300\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.1334 - acc: 0.9526 - val_loss: 0.3555 - val_acc: 0.8882\n",
      "Epoch 198/300\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.1314 - acc: 0.9540 - val_loss: 0.3628 - val_acc: 0.8874\n",
      "Epoch 199/300\n",
      "60000/60000 [==============================] - 8s 131us/sample - loss: 0.1301 - acc: 0.9543 - val_loss: 0.3641 - val_acc: 0.8854\n",
      "Epoch 200/300\n",
      "60000/60000 [==============================] - 7s 125us/sample - loss: 0.1319 - acc: 0.9536 - val_loss: 0.3690 - val_acc: 0.8851\n",
      "Epoch 201/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.1317 - acc: 0.9533 - val_loss: 0.3588 - val_acc: 0.8878\n",
      "Epoch 202/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.1294 - acc: 0.9543 - val_loss: 0.3631 - val_acc: 0.8873\n",
      "Epoch 203/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.1280 - acc: 0.9537 - val_loss: 0.3577 - val_acc: 0.8889\n",
      "Epoch 204/300\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.1302 - acc: 0.9540 - val_loss: 0.3605 - val_acc: 0.8849\n",
      "Epoch 205/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.1267 - acc: 0.9550 - val_loss: 0.3596 - val_acc: 0.8881\n",
      "Epoch 206/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.1267 - acc: 0.9544 - val_loss: 0.3605 - val_acc: 0.8887\n",
      "Epoch 207/300\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.1273 - acc: 0.9545 - val_loss: 0.3598 - val_acc: 0.8882\n",
      "Epoch 208/300\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.1251 - acc: 0.9556 - val_loss: 0.3696 - val_acc: 0.8866\n",
      "Epoch 209/300\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.1233 - acc: 0.9560 - val_loss: 0.3722 - val_acc: 0.8877\n",
      "Epoch 210/300\n",
      "60000/60000 [==============================] - 8s 129us/sample - loss: 0.1234 - acc: 0.9567 - val_loss: 0.3641 - val_acc: 0.8876\n",
      "Epoch 211/300\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.1226 - acc: 0.9568 - val_loss: 0.3736 - val_acc: 0.8881\n",
      "Epoch 212/300\n",
      "60000/60000 [==============================] - 7s 122us/sample - loss: 0.1227 - acc: 0.9566 - val_loss: 0.3666 - val_acc: 0.8863\n",
      "Epoch 213/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.1205 - acc: 0.9581 - val_loss: 0.3631 - val_acc: 0.8900\n",
      "Epoch 214/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.1222 - acc: 0.9571 - val_loss: 0.3674 - val_acc: 0.8880\n",
      "Epoch 215/300\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.1206 - acc: 0.9579 - val_loss: 0.3708 - val_acc: 0.8879\n",
      "Epoch 216/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.1210 - acc: 0.9570 - val_loss: 0.3645 - val_acc: 0.8896\n",
      "Epoch 217/300\n",
      "60000/60000 [==============================] - 7s 125us/sample - loss: 0.1215 - acc: 0.9566 - val_loss: 0.3750 - val_acc: 0.8843\n",
      "Epoch 218/300\n",
      "60000/60000 [==============================] - 7s 125us/sample - loss: 0.1183 - acc: 0.9581 - val_loss: 0.3755 - val_acc: 0.8868\n",
      "Epoch 219/300\n",
      "60000/60000 [==============================] - 7s 125us/sample - loss: 0.1183 - acc: 0.9590 - val_loss: 0.3776 - val_acc: 0.8861\n",
      "Epoch 220/300\n",
      "60000/60000 [==============================] - 7s 125us/sample - loss: 0.1183 - acc: 0.9581 - val_loss: 0.3730 - val_acc: 0.8867\n",
      "Epoch 221/300\n",
      "60000/60000 [==============================] - 8s 126us/sample - loss: 0.1166 - acc: 0.9591 - val_loss: 0.3701 - val_acc: 0.8858\n",
      "Epoch 222/300\n",
      "60000/60000 [==============================] - 7s 125us/sample - loss: 0.1146 - acc: 0.9600 - val_loss: 0.3742 - val_acc: 0.8876\n",
      "Epoch 223/300\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.1162 - acc: 0.9593 - val_loss: 0.3743 - val_acc: 0.8879\n",
      "Epoch 224/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.1155 - acc: 0.9588 - val_loss: 0.3775 - val_acc: 0.8850\n",
      "Epoch 225/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.1142 - acc: 0.9597 - val_loss: 0.3772 - val_acc: 0.8880\n",
      "Epoch 226/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.1141 - acc: 0.9603 - val_loss: 0.3852 - val_acc: 0.8844\n",
      "Epoch 227/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.1152 - acc: 0.9590 - val_loss: 0.3805 - val_acc: 0.8872\n",
      "Epoch 228/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.1146 - acc: 0.9594 - val_loss: 0.3890 - val_acc: 0.8835\n",
      "Epoch 229/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.1120 - acc: 0.9608 - val_loss: 0.3755 - val_acc: 0.8883\n",
      "Epoch 230/300\n",
      "60000/60000 [==============================] - 8s 125us/sample - loss: 0.1125 - acc: 0.9611 - val_loss: 0.3893 - val_acc: 0.8864\n",
      "Epoch 231/300\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.1130 - acc: 0.9611 - val_loss: 0.3858 - val_acc: 0.8882\n",
      "Epoch 232/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.1109 - acc: 0.9612 - val_loss: 0.3805 - val_acc: 0.8871\n",
      "Epoch 233/300\n",
      "60000/60000 [==============================] - 7s 125us/sample - loss: 0.1103 - acc: 0.9614 - val_loss: 0.3912 - val_acc: 0.8848\n",
      "Epoch 234/300\n",
      "60000/60000 [==============================] - 7s 125us/sample - loss: 0.1121 - acc: 0.9603 - val_loss: 0.3858 - val_acc: 0.8858\n",
      "Epoch 235/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.1092 - acc: 0.9612 - val_loss: 0.3938 - val_acc: 0.8878\n",
      "Epoch 236/300\n",
      "60000/60000 [==============================] - 7s 125us/sample - loss: 0.1080 - acc: 0.9622 - val_loss: 0.3834 - val_acc: 0.8866\n",
      "Epoch 237/300\n",
      "60000/60000 [==============================] - 8s 126us/sample - loss: 0.1077 - acc: 0.9631 - val_loss: 0.3861 - val_acc: 0.8855\n",
      "Epoch 238/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.1086 - acc: 0.9617 - val_loss: 0.3901 - val_acc: 0.8869\n",
      "Epoch 239/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.1097 - acc: 0.9610 - val_loss: 0.3834 - val_acc: 0.8876\n",
      "Epoch 240/300\n",
      "60000/60000 [==============================] - 8s 129us/sample - loss: 0.1091 - acc: 0.9619 - val_loss: 0.3859 - val_acc: 0.8861\n",
      "Epoch 241/300\n",
      "60000/60000 [==============================] - 8s 133us/sample - loss: 0.1074 - acc: 0.9617 - val_loss: 0.3982 - val_acc: 0.8875\n",
      "Epoch 242/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.1073 - acc: 0.9625 - val_loss: 0.3933 - val_acc: 0.8879\n",
      "Epoch 243/300\n",
      "60000/60000 [==============================] - 8s 127us/sample - loss: 0.1060 - acc: 0.9631 - val_loss: 0.3848 - val_acc: 0.8895\n",
      "Epoch 244/300\n",
      "60000/60000 [==============================] - 8s 125us/sample - loss: 0.1045 - acc: 0.9629 - val_loss: 0.3891 - val_acc: 0.8852\n",
      "Epoch 245/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.1064 - acc: 0.9627 - val_loss: 0.3862 - val_acc: 0.8881\n",
      "Epoch 246/300\n",
      "60000/60000 [==============================] - 7s 125us/sample - loss: 0.1038 - acc: 0.9642 - val_loss: 0.3925 - val_acc: 0.8861\n",
      "Epoch 247/300\n",
      "60000/60000 [==============================] - 8s 125us/sample - loss: 0.1041 - acc: 0.9638 - val_loss: 0.3899 - val_acc: 0.8880\n",
      "Epoch 248/300\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.1048 - acc: 0.9638 - val_loss: 0.3966 - val_acc: 0.8851\n",
      "Epoch 249/300\n",
      "60000/60000 [==============================] - 7s 125us/sample - loss: 0.1019 - acc: 0.9653 - val_loss: 0.3923 - val_acc: 0.8876\n",
      "Epoch 250/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.1030 - acc: 0.9639 - val_loss: 0.4047 - val_acc: 0.8869\n",
      "Epoch 251/300\n",
      "60000/60000 [==============================] - 8s 131us/sample - loss: 0.1040 - acc: 0.9636 - val_loss: 0.3924 - val_acc: 0.8875\n",
      "Epoch 252/300\n",
      "60000/60000 [==============================] - 7s 125us/sample - loss: 0.1026 - acc: 0.9640 - val_loss: 0.3957 - val_acc: 0.8880\n",
      "Epoch 253/300\n",
      "60000/60000 [==============================] - 8s 125us/sample - loss: 0.1021 - acc: 0.9642 - val_loss: 0.3932 - val_acc: 0.8869\n",
      "Epoch 254/300\n",
      "60000/60000 [==============================] - 8s 126us/sample - loss: 0.1015 - acc: 0.9642 - val_loss: 0.4000 - val_acc: 0.8852\n",
      "Epoch 255/300\n",
      "60000/60000 [==============================] - 8s 125us/sample - loss: 0.1003 - acc: 0.9651 - val_loss: 0.3907 - val_acc: 0.8873\n",
      "Epoch 256/300\n",
      "60000/60000 [==============================] - 7s 125us/sample - loss: 0.0994 - acc: 0.9650 - val_loss: 0.3998 - val_acc: 0.8871\n",
      "Epoch 257/300\n",
      "60000/60000 [==============================] - 8s 126us/sample - loss: 0.1012 - acc: 0.9645 - val_loss: 0.3949 - val_acc: 0.8849\n",
      "Epoch 258/300\n",
      "60000/60000 [==============================] - 7s 125us/sample - loss: 0.0975 - acc: 0.9656 - val_loss: 0.3913 - val_acc: 0.8873\n",
      "Epoch 259/300\n",
      "60000/60000 [==============================] - 7s 125us/sample - loss: 0.1004 - acc: 0.9656 - val_loss: 0.3987 - val_acc: 0.8869\n",
      "Epoch 260/300\n",
      "60000/60000 [==============================] - 7s 125us/sample - loss: 0.0998 - acc: 0.9655 - val_loss: 0.4007 - val_acc: 0.8884\n",
      "Epoch 261/300\n",
      "60000/60000 [==============================] - 8s 126us/sample - loss: 0.0974 - acc: 0.9662 - val_loss: 0.4004 - val_acc: 0.8877\n",
      "Epoch 262/300\n",
      "60000/60000 [==============================] - 8s 126us/sample - loss: 0.0961 - acc: 0.9665 - val_loss: 0.4073 - val_acc: 0.8826\n",
      "Epoch 263/300\n",
      "60000/60000 [==============================] - 8s 125us/sample - loss: 0.0973 - acc: 0.9665 - val_loss: 0.4074 - val_acc: 0.8864\n",
      "Epoch 264/300\n",
      "60000/60000 [==============================] - 7s 125us/sample - loss: 0.0947 - acc: 0.9674 - val_loss: 0.4058 - val_acc: 0.8846\n",
      "Epoch 265/300\n",
      "60000/60000 [==============================] - 8s 125us/sample - loss: 0.0992 - acc: 0.9652 - val_loss: 0.4090 - val_acc: 0.8851\n",
      "Epoch 266/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.0961 - acc: 0.9664 - val_loss: 0.4017 - val_acc: 0.8862\n",
      "Epoch 267/300\n",
      "60000/60000 [==============================] - 8s 126us/sample - loss: 0.0974 - acc: 0.9663 - val_loss: 0.4026 - val_acc: 0.8879\n",
      "Epoch 268/300\n",
      "60000/60000 [==============================] - 8s 126us/sample - loss: 0.0965 - acc: 0.9671 - val_loss: 0.4204 - val_acc: 0.8835\n",
      "Epoch 269/300\n",
      "60000/60000 [==============================] - 8s 126us/sample - loss: 0.0948 - acc: 0.9667 - val_loss: 0.4112 - val_acc: 0.8846\n",
      "Epoch 270/300\n",
      "60000/60000 [==============================] - 7s 125us/sample - loss: 0.0941 - acc: 0.9665 - val_loss: 0.4153 - val_acc: 0.8860\n",
      "Epoch 271/300\n",
      "60000/60000 [==============================] - 8s 125us/sample - loss: 0.0950 - acc: 0.9661 - val_loss: 0.4106 - val_acc: 0.8834\n",
      "Epoch 272/300\n",
      "60000/60000 [==============================] - 8s 125us/sample - loss: 0.0951 - acc: 0.9668 - val_loss: 0.4135 - val_acc: 0.8868\n",
      "Epoch 273/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.0955 - acc: 0.9666 - val_loss: 0.4153 - val_acc: 0.8847\n",
      "Epoch 274/300\n",
      "60000/60000 [==============================] - 8s 127us/sample - loss: 0.0939 - acc: 0.9668 - val_loss: 0.4149 - val_acc: 0.8844\n",
      "Epoch 275/300\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.0927 - acc: 0.9676 - val_loss: 0.4098 - val_acc: 0.8858\n",
      "Epoch 276/300\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.0927 - acc: 0.9680 - val_loss: 0.4164 - val_acc: 0.8843\n",
      "Epoch 277/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.0912 - acc: 0.9683 - val_loss: 0.4251 - val_acc: 0.8835\n",
      "Epoch 278/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.0925 - acc: 0.9675 - val_loss: 0.4126 - val_acc: 0.8837\n",
      "Epoch 279/300\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.0897 - acc: 0.9693 - val_loss: 0.4156 - val_acc: 0.8847\n",
      "Epoch 280/300\n",
      "60000/60000 [==============================] - 7s 125us/sample - loss: 0.0928 - acc: 0.9674 - val_loss: 0.4143 - val_acc: 0.8869\n",
      "Epoch 281/300\n",
      "60000/60000 [==============================] - 7s 125us/sample - loss: 0.0912 - acc: 0.9678 - val_loss: 0.4173 - val_acc: 0.8856\n",
      "Epoch 282/300\n",
      "60000/60000 [==============================] - 8s 132us/sample - loss: 0.0908 - acc: 0.9692 - val_loss: 0.4246 - val_acc: 0.8841\n",
      "Epoch 283/300\n",
      "60000/60000 [==============================] - 8s 125us/sample - loss: 0.0893 - acc: 0.9690 - val_loss: 0.4112 - val_acc: 0.8879\n",
      "Epoch 284/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.0897 - acc: 0.9679 - val_loss: 0.4206 - val_acc: 0.8850\n",
      "Epoch 285/300\n",
      "60000/60000 [==============================] - 7s 125us/sample - loss: 0.0885 - acc: 0.9690 - val_loss: 0.4210 - val_acc: 0.8852\n",
      "Epoch 286/300\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.0893 - acc: 0.9684 - val_loss: 0.4179 - val_acc: 0.8874\n",
      "Epoch 287/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.0886 - acc: 0.9688 - val_loss: 0.4224 - val_acc: 0.8868\n",
      "Epoch 288/300\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.0890 - acc: 0.9691 - val_loss: 0.4192 - val_acc: 0.8859\n",
      "Epoch 289/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.0885 - acc: 0.9698 - val_loss: 0.4213 - val_acc: 0.8844\n",
      "Epoch 290/300\n",
      "60000/60000 [==============================] - 7s 125us/sample - loss: 0.0884 - acc: 0.9682 - val_loss: 0.4255 - val_acc: 0.8821\n",
      "Epoch 291/300\n",
      "60000/60000 [==============================] - 8s 125us/sample - loss: 0.0894 - acc: 0.9682 - val_loss: 0.4296 - val_acc: 0.8853\n",
      "Epoch 292/300\n",
      "60000/60000 [==============================] - 8s 130us/sample - loss: 0.0892 - acc: 0.9690 - val_loss: 0.4362 - val_acc: 0.8860\n",
      "Epoch 293/300\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.0887 - acc: 0.9693 - val_loss: 0.4323 - val_acc: 0.8828\n",
      "Epoch 294/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.0880 - acc: 0.9695 - val_loss: 0.4303 - val_acc: 0.8830\n",
      "Epoch 295/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.0872 - acc: 0.9693 - val_loss: 0.4258 - val_acc: 0.8865\n",
      "Epoch 296/300\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.0851 - acc: 0.9704 - val_loss: 0.4316 - val_acc: 0.8814\n",
      "Epoch 297/300\n",
      "60000/60000 [==============================] - 7s 122us/sample - loss: 0.0842 - acc: 0.9708 - val_loss: 0.4333 - val_acc: 0.8836\n",
      "Epoch 298/300\n",
      "60000/60000 [==============================] - 7s 125us/sample - loss: 0.0854 - acc: 0.9697 - val_loss: 0.4250 - val_acc: 0.8849\n",
      "Epoch 299/300\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.0838 - acc: 0.9708 - val_loss: 0.4269 - val_acc: 0.8847\n",
      "Epoch 300/300\n",
      "60000/60000 [==============================] - 7s 125us/sample - loss: 0.0845 - acc: 0.9703 - val_loss: 0.4310 - val_acc: 0.8838\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f457b645990>"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trainX, trainY, validation_data=(testX, testY), epochs=300, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X-O-fFxnOIVt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BiP7IL52OIVw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nr2YsZV0OIV0"
   },
   "source": [
    "## Review model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 329
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1583,
     "status": "ok",
     "timestamp": 1569741653682,
     "user": {
      "displayName": "Raghav Praneeth Amaravadi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAMhpxsJE7d0g9jOsWi8V_E3rjg4Ut9Yz-RLZWsQw=s64",
      "userId": "05628056859539857657"
     },
     "user_tz": -330
    },
    "id": "h4ojW6-oOIV2",
    "outputId": "b7385cbe-aefe-45f8-d47d-1c624a3a6e57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape_3 (Reshape)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 784)               3136      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               78500     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 92,746\n",
      "Trainable params: 91,178\n",
      "Non-trainable params: 1,568\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gfFGmbZLOIV5"
   },
   "source": [
    "### Run the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r_8X5qazc55u"
   },
   "source": [
    "*Model has been run in the above steps.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bIkbMEN5OIV7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "R6_ExternalLab_AIML_V1.ipynb",
   "provenance": [
    {
     "file_id": "1J06UX5lk_XVu6R6OaG8KU_4mBgwfe-xh",
     "timestamp": 1569505552974
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
