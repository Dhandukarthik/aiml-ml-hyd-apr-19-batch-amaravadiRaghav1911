{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "R8_External_Lab_Questions-HYD_AIML_Nov18.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QGIsF1ADyJ58"
      },
      "source": [
        "# Transfer Learning CIFAR10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "E-n6tVFayGBe"
      },
      "source": [
        "* Train a simple convnet on the CIFAR dataset the first 5 output classes [0..4].\n",
        "* Freeze convolutional layers and fine-tune dense layers for the last 5 ouput classes [5..9].\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Cq8ejXHJyGYq"
      },
      "source": [
        "### 1. Import CIFAR10 data and create 2 datasets with one dataset having classes from 0 to 4 and other having classes from 5 to 9 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snRLJHdGy_Gi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1fffb7b5-e680-4cd9-bfb5-06da617d65ef"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "import tarfile\n",
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout, Flatten, Reshape\n",
        "from keras.layers import Convolution2D, MaxPooling2D\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.utils import np_utils"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uWYbxnBayFUP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "51e867d7-d405-4b07-83d4-4f2e6e925b87"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "print(x_train.shape, 'train samples')\n",
        "print(x_test.shape, 'test samples')\n",
        "print(y_train.shape, 'train target')\n",
        "print(y_test.shape, 'test target')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50000, 32, 32, 3) train samples\n",
            "(10000, 32, 32, 3) test samples\n",
            "(50000, 1) train target\n",
            "(10000, 1) test target\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVwNYyHn0rs4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "0092223c-e42c-423a-ffe9-e19888a3e57f"
      },
      "source": [
        "x_train_lt5 = x_train[y_train[:,0] < 5]\n",
        "y_train_lt5 = y_train[y_train[:,0] < 5]\n",
        "x_test_lt5 = x_test[y_test[:,0] < 5]\n",
        "y_test_lt5 = y_test[y_test[:,0] < 5]\n",
        "\n",
        "print(x_train_lt5.shape, 'train_lt5 samples')\n",
        "print(x_test_lt5.shape, 'test_lt5 samples')\n",
        "print(y_train_lt5.shape, 'train_lt5 target')\n",
        "print(y_test_lt5.shape, 'test_lt5 target')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(25000, 32, 32, 3) train_lt5 samples\n",
            "(5000, 32, 32, 3) test_lt5 samples\n",
            "(25000, 1) train_lt5 target\n",
            "(5000, 1) test_lt5 target\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVLO-NfL7vM1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "879640c9-dd72-4976-da2a-16261d1f8dc9"
      },
      "source": [
        "x_train_gt5 = x_train[y_train[:,0] >= 5]\n",
        "y_train_gt5 = y_train[y_train[:,0] >= 5] - 5  # make classes start at 0 for\n",
        "x_test_gt5 = x_test[y_test[:,0] >= 5]         # np_utils.to_categorical\n",
        "y_test_gt5 = y_test[y_test[:,0] >= 5] - 5\n",
        "\n",
        "print(x_train_gt5.shape, 'train_gt5 samples')\n",
        "print(x_test_gt5.shape, 'test_gt5 samples')\n",
        "print(y_train_gt5.shape, 'train_gt5 target')\n",
        "print(y_test_gt5.shape, 'test_gt5 target')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(25000, 32, 32, 3) train_gt5 samples\n",
            "(5000, 32, 32, 3) test_gt5 samples\n",
            "(25000, 1) train_gt5 target\n",
            "(5000, 1) test_gt5 target\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xtCKmQh4yXhT"
      },
      "source": [
        "### 2. Use One-hot encoding to divide y_train and y_test into required no of output classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uN5O2kJ3yYa6",
        "colab": {}
      },
      "source": [
        "y_train_l5 = np_utils.to_categorical(y_train_lt5, 5)\n",
        "y_test_l5 = np_utils.to_categorical(y_test_lt5, 5)\n",
        "\n",
        "y_train_g5 = np_utils.to_categorical(y_train_gt5, 5)\n",
        "y_test_g5 = np_utils.to_categorical(y_test_gt5, 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cuOiKWfeybAl"
      },
      "source": [
        "### 3. Build a sequential neural network model which can classify the classes 0 to 4 of CIFAR10 dataset with at least 80% accuracy on test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0rs9jCRGdk3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epoch = 20\n",
        "batch = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5HzxNbiiyoBD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "outputId": "26110033-5272-4281-811f-b1a5d8ba41d9"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(BatchNormalization(input_shape = (32,32,3)))\n",
        "model.add(Convolution2D(32, (3,3), activation ='relu', input_shape = (32, 32, 3))) \n",
        "model.add(MaxPooling2D(pool_size=2))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Convolution2D(filters=128, kernel_size=4, padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=2))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Convolution2D(filters=64, kernel_size=4, padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=2))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Convolution2D(filters=32, kernel_size=2, padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=2))\n",
        "\n",
        "\n",
        "model.add(Flatten()) \n",
        "\n",
        "# fully connected layer\n",
        "model.add(Dense(units=32,activation = 'relu'))\n",
        "\n",
        "model.add(Dense(units = 64, activation = 'relu'))\n",
        "\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(units = 32, activation = 'relu'))\n",
        "\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(units = 5, activation = 'softmax')) \n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "\n",
        "early_stopping = keras.callbacks.EarlyStopping(monitor='val_acc', patience=7, verbose=1, mode='auto')\n",
        "callback_list = [early_stopping]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aA-nzCbyGap0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 933
        },
        "outputId": "34168136-9fe3-45ad-fc49-75d6b016ec00"
      },
      "source": [
        "model.fit(x_train_lt5, y_train_l5, batch_size=batch, nb_epoch=epoch, validation_data=(x_test_lt5, y_test_l5), callbacks=callback_list)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Train on 25000 samples, validate on 5000 samples\n",
            "Epoch 1/20\n",
            "25000/25000 [==============================] - 124s 5ms/step - loss: 1.3314 - acc: 0.4022 - val_loss: 1.0084 - val_acc: 0.5914\n",
            "Epoch 2/20\n",
            "25000/25000 [==============================] - 123s 5ms/step - loss: 1.0119 - acc: 0.5819 - val_loss: 0.8835 - val_acc: 0.6416\n",
            "Epoch 3/20\n",
            "25000/25000 [==============================] - 122s 5ms/step - loss: 0.8941 - acc: 0.6449 - val_loss: 0.8010 - val_acc: 0.6840\n",
            "Epoch 4/20\n",
            "25000/25000 [==============================] - 122s 5ms/step - loss: 0.8224 - acc: 0.6846 - val_loss: 0.7043 - val_acc: 0.7330\n",
            "Epoch 5/20\n",
            "25000/25000 [==============================] - 123s 5ms/step - loss: 0.7615 - acc: 0.7130 - val_loss: 0.6810 - val_acc: 0.7464\n",
            "Epoch 6/20\n",
            "25000/25000 [==============================] - 123s 5ms/step - loss: 0.7109 - acc: 0.7364 - val_loss: 0.6167 - val_acc: 0.7728\n",
            "Epoch 7/20\n",
            "25000/25000 [==============================] - 123s 5ms/step - loss: 0.6765 - acc: 0.7505 - val_loss: 0.6110 - val_acc: 0.7804\n",
            "Epoch 8/20\n",
            "25000/25000 [==============================] - 122s 5ms/step - loss: 0.6434 - acc: 0.7662 - val_loss: 0.5755 - val_acc: 0.7870\n",
            "Epoch 9/20\n",
            "25000/25000 [==============================] - 123s 5ms/step - loss: 0.6162 - acc: 0.7772 - val_loss: 0.5360 - val_acc: 0.8000\n",
            "Epoch 10/20\n",
            "25000/25000 [==============================] - 123s 5ms/step - loss: 0.5888 - acc: 0.7846 - val_loss: 0.5401 - val_acc: 0.7944\n",
            "Epoch 11/20\n",
            "25000/25000 [==============================] - 123s 5ms/step - loss: 0.5707 - acc: 0.7951 - val_loss: 0.5037 - val_acc: 0.8112\n",
            "Epoch 12/20\n",
            "25000/25000 [==============================] - 123s 5ms/step - loss: 0.5496 - acc: 0.7992 - val_loss: 0.5209 - val_acc: 0.8078\n",
            "Epoch 13/20\n",
            "25000/25000 [==============================] - 123s 5ms/step - loss: 0.5395 - acc: 0.8078 - val_loss: 0.4788 - val_acc: 0.8216\n",
            "Epoch 14/20\n",
            "25000/25000 [==============================] - 123s 5ms/step - loss: 0.5131 - acc: 0.8145 - val_loss: 0.4797 - val_acc: 0.8260\n",
            "Epoch 15/20\n",
            "25000/25000 [==============================] - 123s 5ms/step - loss: 0.5147 - acc: 0.8143 - val_loss: 0.4910 - val_acc: 0.8210\n",
            "Epoch 16/20\n",
            "25000/25000 [==============================] - 124s 5ms/step - loss: 0.4960 - acc: 0.8238 - val_loss: 0.4608 - val_acc: 0.8288\n",
            "Epoch 17/20\n",
            "25000/25000 [==============================] - 123s 5ms/step - loss: 0.4794 - acc: 0.8258 - val_loss: 0.4490 - val_acc: 0.8334\n",
            "Epoch 18/20\n",
            "25000/25000 [==============================] - 123s 5ms/step - loss: 0.4727 - acc: 0.8348 - val_loss: 0.4955 - val_acc: 0.8234\n",
            "Epoch 19/20\n",
            "25000/25000 [==============================] - 123s 5ms/step - loss: 0.4552 - acc: 0.8391 - val_loss: 0.4628 - val_acc: 0.8344\n",
            "Epoch 20/20\n",
            "25000/25000 [==============================] - 123s 5ms/step - loss: 0.4524 - acc: 0.8384 - val_loss: 0.4259 - val_acc: 0.8438\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe483127d30>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Nf26adggnRs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "a9d3ad0b-8935-451e-acf7-d2be7c4e432a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctVoioZPhA-O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('/content/drive/My Drive/Colab Notebooks/nn_cifar10.h5')\n",
        "model.save_weights('/content/drive/My Drive/Colab Notebooks/nn_cifar10_weights.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "woTfNst_ynRG"
      },
      "source": [
        "### 4. In the model which was built above (for classification of classes 0-4 in CIFAR10), make only the dense layers to be trainable and conv layers to be non-trainable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o_VCDB3Byb1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "92725b63-6e32-4584-f351-118f515a527c"
      },
      "source": [
        "for layers in model.layers:\n",
        "    print(layers.name)\n",
        "    if('dense' not in layers.name):\n",
        "        layers.trainable = False\n",
        "        print(layers.name + 'is not trainable\\n')\n",
        "    if('dense' in layers.name):\n",
        "        print(layers.name + ' is trainable\\n')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "batch_normalization_1\n",
            "batch_normalization_1is not trainable\n",
            "\n",
            "conv2d_1\n",
            "conv2d_1is not trainable\n",
            "\n",
            "max_pooling2d_1\n",
            "max_pooling2d_1is not trainable\n",
            "\n",
            "dropout_1\n",
            "dropout_1is not trainable\n",
            "\n",
            "conv2d_2\n",
            "conv2d_2is not trainable\n",
            "\n",
            "max_pooling2d_2\n",
            "max_pooling2d_2is not trainable\n",
            "\n",
            "dropout_2\n",
            "dropout_2is not trainable\n",
            "\n",
            "conv2d_3\n",
            "conv2d_3is not trainable\n",
            "\n",
            "max_pooling2d_3\n",
            "max_pooling2d_3is not trainable\n",
            "\n",
            "dropout_3\n",
            "dropout_3is not trainable\n",
            "\n",
            "conv2d_4\n",
            "conv2d_4is not trainable\n",
            "\n",
            "max_pooling2d_4\n",
            "max_pooling2d_4is not trainable\n",
            "\n",
            "flatten_1\n",
            "flatten_1is not trainable\n",
            "\n",
            "dense_1\n",
            "dense_1 is trainable\n",
            "\n",
            "dense_2\n",
            "dense_2 is trainable\n",
            "\n",
            "dropout_4\n",
            "dropout_4is not trainable\n",
            "\n",
            "dense_3\n",
            "dense_3 is trainable\n",
            "\n",
            "dropout_5\n",
            "dropout_5is not trainable\n",
            "\n",
            "dense_4\n",
            "dense_4 is trainable\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1-uUPqWpyeyX"
      },
      "source": [
        "### 5. Utilize the the model trained on CIFAR 10 (classes 0 to 4) to classify the classes 5 to 9 of CIFAR 10  (Use Transfer Learning) <br>\n",
        "Achieve an accuracy of more than 85% on test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "szHjJgDvyfCt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 845
        },
        "outputId": "62503675-799e-4d5d-87f6-5951447faf8c"
      },
      "source": [
        "model.fit(x_train_gt5, y_train_g5, batch_size=batch, nb_epoch=epoch, validation_data=(x_test_gt5, y_test_g5), callbacks=callback_list)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:493: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 25000 samples, validate on 5000 samples\n",
            "Epoch 1/20\n",
            "25000/25000 [==============================] - 122s 5ms/step - loss: 1.1697 - acc: 0.5412 - val_loss: 0.6751 - val_acc: 0.7688\n",
            "Epoch 2/20\n",
            "25000/25000 [==============================] - 122s 5ms/step - loss: 0.7188 - acc: 0.7483 - val_loss: 0.5724 - val_acc: 0.8012\n",
            "Epoch 3/20\n",
            "25000/25000 [==============================] - 122s 5ms/step - loss: 0.6052 - acc: 0.7918 - val_loss: 0.4788 - val_acc: 0.8336\n",
            "Epoch 4/20\n",
            "25000/25000 [==============================] - 123s 5ms/step - loss: 0.5365 - acc: 0.8138 - val_loss: 0.4217 - val_acc: 0.8490\n",
            "Epoch 5/20\n",
            "25000/25000 [==============================] - 122s 5ms/step - loss: 0.5052 - acc: 0.8260 - val_loss: 0.3808 - val_acc: 0.8646\n",
            "Epoch 6/20\n",
            "25000/25000 [==============================] - 122s 5ms/step - loss: 0.4632 - acc: 0.8396 - val_loss: 0.3933 - val_acc: 0.8590\n",
            "Epoch 7/20\n",
            "25000/25000 [==============================] - 122s 5ms/step - loss: 0.4372 - acc: 0.8502 - val_loss: 0.3314 - val_acc: 0.8860\n",
            "Epoch 8/20\n",
            "25000/25000 [==============================] - 122s 5ms/step - loss: 0.4135 - acc: 0.8548 - val_loss: 0.3377 - val_acc: 0.8828\n",
            "Epoch 9/20\n",
            "25000/25000 [==============================] - 122s 5ms/step - loss: 0.4064 - acc: 0.8592 - val_loss: 0.3297 - val_acc: 0.8834\n",
            "Epoch 10/20\n",
            "25000/25000 [==============================] - 122s 5ms/step - loss: 0.3764 - acc: 0.8698 - val_loss: 0.3158 - val_acc: 0.8856\n",
            "Epoch 11/20\n",
            "25000/25000 [==============================] - 121s 5ms/step - loss: 0.3597 - acc: 0.8757 - val_loss: 0.2946 - val_acc: 0.8960\n",
            "Epoch 12/20\n",
            "25000/25000 [==============================] - 121s 5ms/step - loss: 0.3591 - acc: 0.8795 - val_loss: 0.2891 - val_acc: 0.8952\n",
            "Epoch 13/20\n",
            "25000/25000 [==============================] - 122s 5ms/step - loss: 0.3334 - acc: 0.8836 - val_loss: 0.2794 - val_acc: 0.9020\n",
            "Epoch 14/20\n",
            "25000/25000 [==============================] - 123s 5ms/step - loss: 0.3361 - acc: 0.8832 - val_loss: 0.2740 - val_acc: 0.9068\n",
            "Epoch 15/20\n",
            "25000/25000 [==============================] - 122s 5ms/step - loss: 0.3211 - acc: 0.8917 - val_loss: 0.2877 - val_acc: 0.8968\n",
            "Epoch 16/20\n",
            "25000/25000 [==============================] - 122s 5ms/step - loss: 0.3096 - acc: 0.8928 - val_loss: 0.2744 - val_acc: 0.9036\n",
            "Epoch 17/20\n",
            "25000/25000 [==============================] - 122s 5ms/step - loss: 0.2958 - acc: 0.8979 - val_loss: 0.2758 - val_acc: 0.9018\n",
            "Epoch 18/20\n",
            "25000/25000 [==============================] - 122s 5ms/step - loss: 0.2876 - acc: 0.9026 - val_loss: 0.2611 - val_acc: 0.9092\n",
            "Epoch 19/20\n",
            "25000/25000 [==============================] - 123s 5ms/step - loss: 0.2853 - acc: 0.9024 - val_loss: 0.2566 - val_acc: 0.9114\n",
            "Epoch 20/20\n",
            "25000/25000 [==============================] - 121s 5ms/step - loss: 0.2728 - acc: 0.9072 - val_loss: 0.2743 - val_acc: 0.9030\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe43d2dbd68>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6dIh9oAsM1_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FU-HwvIdH0M-"
      },
      "source": [
        "## Sentiment analysis <br> \n",
        "\n",
        "The objective of the second problem is to perform Sentiment analysis from the tweets data collected from the users targeted at various mobile devices.\n",
        "Based on the tweet posted by a user (text), we will classify if the sentiment of the user targeted at a particular mobile device is positive or not."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nAQDiZHRH0M_"
      },
      "source": [
        "### 6. Read the dataset (tweets.csv) and drop the NA's while reading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3eXGIe-SH0NA",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv('/content/drive/My Drive/Colab Notebooks/tweets.csv', encoding = \"ISO-8859-1\").dropna()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CWeWe1eJH0NF",
        "outputId": "b2f3973b-d38e-4e63-82f0-5aefba17fb5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data.shape"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3291, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7kX-WoJDH0NV",
        "outputId": "5a4d512b-e76b-4349-9142-76d51e66b2e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        }
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_text</th>\n",
              "      <th>emotion_in_tweet_is_directed_at</th>\n",
              "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
              "      <td>iPhone</td>\n",
              "      <td>Negative emotion</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
              "      <td>iPad or iPhone App</td>\n",
              "      <td>Positive emotion</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
              "      <td>iPad</td>\n",
              "      <td>Positive emotion</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
              "      <td>iPad or iPhone App</td>\n",
              "      <td>Negative emotion</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
              "      <td>Google</td>\n",
              "      <td>Positive emotion</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          tweet_text  ... is_there_an_emotion_directed_at_a_brand_or_product\n",
              "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...  ...                                   Negative emotion\n",
              "1  @jessedee Know about @fludapp ? Awesome iPad/i...  ...                                   Positive emotion\n",
              "2  @swonderlin Can not wait for #iPad 2 also. The...  ...                                   Positive emotion\n",
              "3  @sxsw I hope this year's festival isn't as cra...  ...                                   Negative emotion\n",
              "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...  ...                                   Positive emotion\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OGWB3P2WH0NY"
      },
      "source": [
        "### Consider only rows having Positive emotion and Negative emotion and remove other rows from the dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bdgA_8N2H0NY",
        "colab": {}
      },
      "source": [
        "data = data[(data['is_there_an_emotion_directed_at_a_brand_or_product'] == 'Positive emotion') | (data['is_there_an_emotion_directed_at_a_brand_or_product'] == 'Negative emotion')]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_Jlu-reIH0Na",
        "outputId": "a09031c8-296d-4641-c1ca-7ee1cf6beb1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data.shape"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3191, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SotCRvkDH0Nf"
      },
      "source": [
        "### 7. Represent text as numerical data using `CountVectorizer` and get the document term frequency matrix\n",
        "\n",
        "#### Use `vect` as the variable name for initialising CountVectorizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YcbkY4sgH0Ng",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import metrics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KyXtZGr-H0Nl",
        "colab": {}
      },
      "source": [
        "vect = CountVectorizer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5pxd5fSHH0Nt"
      },
      "source": [
        "### 8. Find number of different words in vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "p1DQ2LdNH0Nu",
        "colab": {}
      },
      "source": [
        "words = vect.fit_transform(data.tweet_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWX9qxFPv3Cb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "47fcb8d1-4afb-4567-b765-118d43b8bdc9"
      },
      "source": [
        "print('Number of words: ', words.shape[1])"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of words:  5648\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dwtgjTBeH0Ny"
      },
      "source": [
        "#### Tip: To see all available functions for an Object use dir"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2n_iCcTNH0N0",
        "outputId": "ed5b95be-9ac9-40e5-bc6e-42246b7d8f28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#dir(cv)\n",
        "dir(words)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['__abs__',\n",
              " '__add__',\n",
              " '__array_priority__',\n",
              " '__bool__',\n",
              " '__class__',\n",
              " '__delattr__',\n",
              " '__dict__',\n",
              " '__dir__',\n",
              " '__div__',\n",
              " '__doc__',\n",
              " '__eq__',\n",
              " '__format__',\n",
              " '__ge__',\n",
              " '__getattr__',\n",
              " '__getattribute__',\n",
              " '__getitem__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__iadd__',\n",
              " '__idiv__',\n",
              " '__imul__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__isub__',\n",
              " '__iter__',\n",
              " '__itruediv__',\n",
              " '__le__',\n",
              " '__len__',\n",
              " '__lt__',\n",
              " '__matmul__',\n",
              " '__module__',\n",
              " '__mul__',\n",
              " '__ne__',\n",
              " '__neg__',\n",
              " '__new__',\n",
              " '__nonzero__',\n",
              " '__pow__',\n",
              " '__radd__',\n",
              " '__rdiv__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__repr__',\n",
              " '__rmatmul__',\n",
              " '__rmul__',\n",
              " '__rsub__',\n",
              " '__rtruediv__',\n",
              " '__setattr__',\n",
              " '__setitem__',\n",
              " '__sizeof__',\n",
              " '__str__',\n",
              " '__sub__',\n",
              " '__subclasshook__',\n",
              " '__truediv__',\n",
              " '__weakref__',\n",
              " '_add_dense',\n",
              " '_add_sparse',\n",
              " '_arg_min_or_max',\n",
              " '_arg_min_or_max_axis',\n",
              " '_asindices',\n",
              " '_binopt',\n",
              " '_cs_matrix__get_has_canonical_format',\n",
              " '_cs_matrix__get_sorted',\n",
              " '_cs_matrix__set_has_canonical_format',\n",
              " '_cs_matrix__set_sorted',\n",
              " '_deduped_data',\n",
              " '_divide',\n",
              " '_divide_sparse',\n",
              " '_get_arrayXarray',\n",
              " '_get_arrayXint',\n",
              " '_get_arrayXslice',\n",
              " '_get_columnXarray',\n",
              " '_get_dtype',\n",
              " '_get_intXarray',\n",
              " '_get_intXint',\n",
              " '_get_intXslice',\n",
              " '_get_sliceXarray',\n",
              " '_get_sliceXint',\n",
              " '_get_sliceXslice',\n",
              " '_get_submatrix',\n",
              " '_imag',\n",
              " '_inequality',\n",
              " '_insert_many',\n",
              " '_major_index_fancy',\n",
              " '_major_slice',\n",
              " '_maximum_minimum',\n",
              " '_min_or_max',\n",
              " '_min_or_max_axis',\n",
              " '_minor_index_fancy',\n",
              " '_minor_reduce',\n",
              " '_minor_slice',\n",
              " '_mul_multivector',\n",
              " '_mul_scalar',\n",
              " '_mul_sparse_matrix',\n",
              " '_mul_vector',\n",
              " '_prepare_indices',\n",
              " '_process_toarray_args',\n",
              " '_real',\n",
              " '_rsub_dense',\n",
              " '_scalar_binopt',\n",
              " '_set_arrayXarray',\n",
              " '_set_arrayXarray_sparse',\n",
              " '_set_dtype',\n",
              " '_set_intXint',\n",
              " '_set_many',\n",
              " '_set_self',\n",
              " '_setdiag',\n",
              " '_shape',\n",
              " '_sub_dense',\n",
              " '_sub_sparse',\n",
              " '_swap',\n",
              " '_validate_indices',\n",
              " '_with_data',\n",
              " '_zero_many',\n",
              " 'arcsin',\n",
              " 'arcsinh',\n",
              " 'arctan',\n",
              " 'arctanh',\n",
              " 'argmax',\n",
              " 'argmin',\n",
              " 'asformat',\n",
              " 'asfptype',\n",
              " 'astype',\n",
              " 'ceil',\n",
              " 'check_format',\n",
              " 'conj',\n",
              " 'conjugate',\n",
              " 'copy',\n",
              " 'count_nonzero',\n",
              " 'data',\n",
              " 'deg2rad',\n",
              " 'diagonal',\n",
              " 'dot',\n",
              " 'dtype',\n",
              " 'eliminate_zeros',\n",
              " 'expm1',\n",
              " 'floor',\n",
              " 'format',\n",
              " 'getH',\n",
              " 'get_shape',\n",
              " 'getcol',\n",
              " 'getformat',\n",
              " 'getmaxprint',\n",
              " 'getnnz',\n",
              " 'getrow',\n",
              " 'has_canonical_format',\n",
              " 'has_sorted_indices',\n",
              " 'indices',\n",
              " 'indptr',\n",
              " 'log1p',\n",
              " 'max',\n",
              " 'maximum',\n",
              " 'maxprint',\n",
              " 'mean',\n",
              " 'min',\n",
              " 'minimum',\n",
              " 'multiply',\n",
              " 'ndim',\n",
              " 'nnz',\n",
              " 'nonzero',\n",
              " 'power',\n",
              " 'prune',\n",
              " 'rad2deg',\n",
              " 'reshape',\n",
              " 'resize',\n",
              " 'rint',\n",
              " 'set_shape',\n",
              " 'setdiag',\n",
              " 'shape',\n",
              " 'sign',\n",
              " 'sin',\n",
              " 'sinh',\n",
              " 'sort_indices',\n",
              " 'sorted_indices',\n",
              " 'sqrt',\n",
              " 'sum',\n",
              " 'sum_duplicates',\n",
              " 'tan',\n",
              " 'tanh',\n",
              " 'toarray',\n",
              " 'tobsr',\n",
              " 'tocoo',\n",
              " 'tocsc',\n",
              " 'tocsr',\n",
              " 'todense',\n",
              " 'todia',\n",
              " 'todok',\n",
              " 'tolil',\n",
              " 'transpose',\n",
              " 'trunc']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ShA6D8jKH0N5"
      },
      "source": [
        "### Find out how many Positive and Negative emotions are there.\n",
        "\n",
        "Hint: Use value_counts on that column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "q7LAl5pzH0N6",
        "outputId": "e4da9567-15f5-449a-9408-c1c240877587",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "pd.value_counts(data['is_there_an_emotion_directed_at_a_brand_or_product'])"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Positive emotion    2672\n",
              "Negative emotion     519\n",
              "Name: is_there_an_emotion_directed_at_a_brand_or_product, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IUvgj0FoH0N9"
      },
      "source": [
        "###  Change the labels for Positive and Negative emotions as 1 and 0 respectively and store in a different column in the same dataframe named 'label'\n",
        "\n",
        "Hint: use map on that column and give labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YftKwFv7H0N9",
        "colab": {}
      },
      "source": [
        "data['label'] = data.is_there_an_emotion_directed_at_a_brand_or_product.map({'Positive emotion':1, 'Negative emotion':0})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3YErwYLCH0N_"
      },
      "source": [
        "### 9. Define the feature set (independent variable or X) to be `text` column and `labels` as target (or dependent variable)  and divide into train and test datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcPg1vqpuu7z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = data.tweet_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgesYSsZu9IL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = data['is_there_an_emotion_directed_at_a_brand_or_product']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1fm7dWHtuDv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweet_train,tweet_test,senti_train,senti_test = train_test_split(X, y, test_size = 0.2, random_state = 42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66B1o_0TzfS7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vecto = CountVectorizer()\n",
        "tweet_train_dtm = vecto.fit_transform(tweet_train)\n",
        "tweet_test_dtm = vecto.transform(tweet_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Q5nlCuaaH0OD"
      },
      "source": [
        "## 10. **Predicting the sentiment:**\n",
        "\n",
        "\n",
        "### Use Naive Bayes and Logistic Regression and their accuracy scores for predicting the sentiment of the given text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2AbVYssaH0OE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "8dd983db-2611-4ccb-aa76-2545a4e912a8"
      },
      "source": [
        "# Logistic Regression\n",
        "#Logistic Regression\n",
        "logreg = LogisticRegression()\n",
        "logreg.fit(tweet_train_dtm, senti_train)\n",
        "\n",
        "y_pred_class = logreg.predict(tweet_test_dtm)\n",
        "print('\\nTest Accuracy Logistic Regression: ', metrics.accuracy_score(senti_test, y_pred_class))\n",
        "\n",
        "y_pred_train = logreg.predict(tweet_train_dtm)\n",
        "print('Train Accuracy Logistic Regression: ', metrics.accuracy_score(senti_train, y_pred_train))"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test Accuracy Logistic Regression:  0.863849765258216\n",
            "Train Accuracy Logistic Regression:  0.9804075235109718\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j78A8mqGyhiu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "cdcd9f2d-62a6-4350-f280-cc8cba0b4ac0"
      },
      "source": [
        "nb = MultinomialNB()\n",
        "nb.fit(tweet_train_dtm, senti_train)\n",
        "y_pred_class_nb = nb.predict(tweet_test_dtm)\n",
        "print('Test Accuracy NB: ', metrics.accuracy_score(senti_test, y_pred_class_nb))\n",
        "y_pred_train_nb = nb.predict(tweet_train_dtm)\n",
        "print('Train Accuracy NB: ', metrics.accuracy_score(senti_train, y_pred_train_nb))"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy NB:  0.865414710485133\n",
            "Train Accuracy NB:  0.9521943573667712\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sw-0B33tH0Ox"
      },
      "source": [
        "## 11. Create a function called `tokenize_predict` which can take count vectorizer object as input and prints the accuracy for x (text) and y (labels)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "okCTOs1TH0Oy",
        "colab": {}
      },
      "source": [
        "def tokenize_test(vect):\n",
        "    tweet_train_dtm = vect.fit_transform(tweet_train)\n",
        "    tweet_test_dtm = vect.transform(tweet_test)\n",
        "    print('Features: ', tweet_train_dtm.shape[1])\n",
        "    nb = MultinomialNB()\n",
        "    nb.fit(tweet_train_dtm, senti_train)\n",
        "    y_pred_class = nb.predict(tweet_test_dtm)\n",
        "    print('Accuracy: ', metrics.accuracy_score(senti_test, y_pred_class))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JxZ8jfPEH0O0"
      },
      "source": [
        "### Create a count vectorizer function which includes n_grams = 1,2  and pass it to tokenize_predict function to print the accuracy score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kdCyAN_IH0O0",
        "outputId": "ae22ffcc-ca8c-44f6-d2be-098f56042b0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# include 1-grams and 2-grams\n",
        "vect = CountVectorizer(ngram_range=(1, 2))\n",
        "tokenize_test(vect)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Features:  25638\n",
            "Accuracy:  0.8779342723004695\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "axepytmgH0O4"
      },
      "source": [
        "### 12. Create a count vectorizer function with stopwords = 'english'  and pass it to tokenize_predict function to print the accuracy score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HToGkq7vH0O4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "e2487587-9738-455c-f568-a536d729d6eb"
      },
      "source": [
        "vect = CountVectorizer(stop_words='english')\n",
        "tokenize_test(vect)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Features:  4766\n",
            "Accuracy:  0.8685446009389671\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iOIlJRxoH0O7"
      },
      "source": [
        "### 13. Create a count vectorizer function with stopwords = 'english' and max_features =300  and pass it to tokenize_predict function to print the accuracy score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6fUhff-oH0O8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "6da8959a-2287-4b30-8236-95ecfa358c88"
      },
      "source": [
        "vect = CountVectorizer(stop_words='english', max_features=300)\n",
        "tokenize_test(vect)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Features:  300\n",
            "Accuracy:  0.8215962441314554\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "S2KZNWVkH0PA"
      },
      "source": [
        "### 14. Create a count vectorizer function with n_grams = 1,2  and max_features = 15000  and pass it to tokenize_predict function to print the accuracy score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3v9XD082H0PB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "77f4f523-8cb2-4725-cf3d-b7f58f480bcd"
      },
      "source": [
        "vect = CountVectorizer(stop_words='english', max_features=15000)\n",
        "tokenize_test(vect)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Features:  4766\n",
            "Accuracy:  0.8685446009389671\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "We3JK_SRH0PO"
      },
      "source": [
        "### 15. Create a count vectorizer function with n_grams = 1,2  and include terms that appear at least 2 times (min_df = 2)  and pass it to tokenize_predict function to print the accuracy score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fUHrfDCyH0PP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "74a1f343-47d1-4246-95aa-37c58423538e"
      },
      "source": [
        "vect = CountVectorizer(ngram_range=(1,2), min_df=2)\n",
        "tokenize_test(vect)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Features:  8179\n",
            "Accuracy:  0.8575899843505478\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvTQq-me1kYh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}